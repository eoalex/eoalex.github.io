---
title: 'Flume1.6.0入门:安装、配置及示列'
tags:
  - flume
id: 156
categories:
  - 大数据
date: 2015-11-14 16:18:06
---

# 1.简介

flume是一个分布式、可靠、和高可用的海量日志采集、聚合和传输的系统。支持在日志系统中定制各类数据发送方，用于收集数据;同时，Flume提供对数据进行简单处理，并写到各种数据接受方(比如文本、HDFS、Hbase等)的能力 。
flume的数据流由事件(Event)贯穿始终。事件是flume的基本数据单位，它携带日志数据(字节数组形式)并且携带有头信息，这些Event由Agent外部的Source生成，当Source捕获事件后会进行特定的格式化，然后Source会把事件推入(单个或多个)Channel中。你可以把Channel看作是一个缓冲区，它将保存事件直到Sink处理完该事件。Sink负责持久化日志或者把事件推向另一个Source。
flume的一些核心概念：
Agent 使用JVM 运行flume。每台机器运行一个agent，但是可以在一个agent中包含多个sources和sinks。
Client 生产数据，运行在一个独立的线程。
Source 从Client收集数据，传递给Channel。
Sink 从Channel收集数据，运行在一个独立线程。
Channel 连接 sources 和 sinks ，这个有点像一个队列。
Events 可以是日志记录、 avro 对象等。
基本的数据流模型
[![flume_UserGuide_image00](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/11/flume_UserGuide_image00.png)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/11/flume_UserGuide_image00.png)

# 2.环境

Hadoop 1.2.1 ,配置见下表，Hadoop 1.2.1 的安装参见 [hadoop-1-2-1-完全分布式安装部署](http://blog.yaodataking.com/2015/10/hadoop-1-2-1-完全分布式安装部署/) ，这里略。
<table style="height: 120px" width="300">
<tbody>
<tr>
<td width="197">IP</td>
<td width="197">NAME</td>
<td width="197"></td>
</tr>
<tr>
<td width="197">192.168.199.11</td>
<td width="197">Hadoop11</td>
<td width="197">Namenode,flume</td>
</tr>
<tr>
<td width="197">192.168.199.12</td>
<td width="197">Hadoop12</td>
<td width="197">Datanode</td>
</tr>
<tr>
<td width="197">192.168.199.13</td>
<td width="197">Hadoop13</td>
<td width="197">Datanode</td>
</tr>
</tbody>
</table>

# 3.安装配置

从镜像服务器下载并解压
#wget http://mirrors.aliyun.com/apache/flume/1.6.0/apache-flume-1.6.0-bin.tar.gz
#tar -xzvf apache-flume-1.6.0-bin.tar.gz
[![2015-11-14_15-29-56](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/11/2015-11-14_15-29-56.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/11/2015-11-14_15-29-56.jpg)
#vim /etc/profile
export FLUME_HOME=/home/grid/apache-flume-1.6.0-bin
export FLUME_CONF_DIR=$FLUME_HOME/conf
export PATH=$FLUME_HOME/bin:$PATH
#source /etc/profile
#cd conf
#cp flume-env-template.sh flume-env.sh
#vim flume-env.sh
[![2015-11-14_15-36-35](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/11/2015-11-14_15-36-35.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/11/2015-11-14_15-36-35.jpg)

# 4.测试

#flume-ng version
[![2015-11-14_15-37-42](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/11/2015-11-14_15-37-42.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/11/2015-11-14_15-37-42.jpg)
出现上面的信息表示安装成功了。

# 5.示列

## 5.1 一个简单sample

#cp conf/flume-conf.properties.template conf/flume-sample
#vim conf/flume-sample
a1.sources = r1
a1.sinks = k1
a1.channels = c1
# Describe/configure the source
a1.sources.r1.type = netcat
a1.sources.r1.bind = localhost
a1.sources.r1.port = 44444
# Describe the sink
a1.sinks.k1.type = logger
# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100
# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
//运行，并显示log在终端
#bin/flume-ng agent --conf conf --conf-file conf/flume-sample --name a1 -Dflume.root.logger=INFO,console

再打开一个终端,输入telnet命令
#telnet localhost 44444
然后输入hello word
[![2015-11-14_15-50-57](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/11/2015-11-14_15-50-57.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/11/2015-11-14_15-50-57.jpg)
回到原来的终端，我们看到hello world被flume的agent收集了。
[![2015-11-14_15-51-12](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/11/2015-11-14_15-51-12.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/11/2015-11-14_15-51-12.jpg)

## 5.2收集日志到HDFS示列

#cp conf/flume-conf.properties.template conf/flume-exec
#vim conf/flume-exec
a1.sources = r1
a1.sinks = k1
a1.channels = c1
a1.sources.r1.type = exec
a1.sources.r1.channels = c1
a1.sources.r1.command = tail -F /home/grid/hadoop-1.2.1/logs/hadoop-grid-namenode-hadoop11.log
a1.sinks.k1.type = hdfs
a1.sinks.k1.channel = c1
a1.sinks.k1.hdfs.path = hdfs://hadoop11:9000/outputs
a1.sinks.k1.hdfs.filePrefix = events-
a1.sinks.k1.hdfs.round = true
a1.sinks.k1.hdfs.roundValue = 10
a1.sinks.k1.hdfs.roundUnit = minute
a1.sinks.k1.hdfs.rollSize = 4000000
a1.sinks.k1.hdfs.rollCount = 0
a1.sinks.k1.hdfs.writeFormat = Text
a1.sinks.k1.hdfs.fileType = DataStream
a1.sinks.k1.hdfs.batchSize = 10
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100
//运行，并显示log在终端
#bin/flume-ng agent --conf conf --conf-file conf/flume-exec --name a1 -Dflume.root.logger=INFO,console
[![2015-11-14_15-56-33](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/11/2015-11-14_15-56-33.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/11/2015-11-14_15-56-33.jpg)[![2015-11-14_16-31-29](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/11/2015-11-14_16-31-29.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/11/2015-11-14_16-31-29.jpg)
我们看到flume不断收集hadoop-grid-namenode-hadoop11.log的数据写入HDFS中。
以上只是简单演示了flume的功能，进一步研究可以查看[官方的用户指南](http://flume.apache.org/FlumeUserGuide.html)