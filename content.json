[{"title":"Storm与kafka集成(上)","date":"2017-05-15T11:52:57.000Z","path":"2017/05/15/storm-3/","text":"一、简介我们知道storm的作用主要是进行流式实时计算，对于均匀的数据流storm处理是非常有效的，但是现实生活中大部分场景并不是均匀的数据流，而是时而多时而少的数据流入，这种情况下显然用批量处理是不合适的，如果使用storm做实时计算的话可能因为数据拥堵而导致服务器挂掉，应对这种情况，使用kafka作为消息队列是非常合适的选择，kafka可以将不均匀的数据转换成均匀的消息流，从而和storm比较完善的结合，这样才可以实现稳定的流式计算，storm和kafka结合，实质上无非是把Kafka的数据消费，是由Storm去消费，通过KafkaSpout将数据输送到Storm，然后让Storm安装业务需求对接受的数据做实时处理，最后将处理后的数据输出或者保存到文件、数据库、分布式存储等等。 二、搭建storm和kafka集群我们要搭建storm和kafka集群,这里使用docker镜像。所以首先使用docker file建立镜像。1.storm的docker file FROM openjdk:8-jre-alpine ARG MIRROR=http://mirrors.aliyun.com ARG BIN_VERSION=apache-storm-1.0.3 # Install required packages RUN apk add --no-cache \\ bash \\ python \\ su-exec RUN wget -q -O - ${MIRROR}/apache/storm/${BIN_VERSION}/${BIN_VERSION}.tar.gz | tar -xzf - -C /usr/share \\ &amp;&amp; mv /usr/share/${BIN_VERSION} /usr/share/storm \\ &amp;&amp; rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/* WORKDIR /usr/share/storm # add startup script ADD entrypoint.sh entrypoint.sh ADD cluster.xml log4j2/cluster.xml ADD worker.xml log4j2/worker.xml RUN chmod +x entrypoint.sh # supervisor: worker ports EXPOSE 6700 6701 6702 6703 # logviewer EXPOSE 8000 # DRPC and remote deployment EXPOSE 6627 3772 3773 ENTRYPOINT [&quot;/usr/share/storm/entrypoint.sh&quot;]`&lt;/pre&gt; 2.zookeeper和kafak的docker file 这里略，参见本博客关于[kafka](http://blog.yaodataking.com/2016/10/kafka-4.html)的博文。 3.使用docker-compose 启动集群 &lt;pre&gt;`version: &apos;2.0&apos; services: zookeeper0: image: alex/zookeeper_cluster:3.4.6 container_name: zookeeper0 hostname: zookeeper0 ports: - &quot;2181:2181&quot; - &quot;2888:2888&quot; - &quot;3888:3888&quot; expose: - 2181 - 2888 - 3888 environment: ZOOKEEPER_PORT: 2181 ZOOKEEPER_ID: 0 ZOOKEEPER_SERVERS: server.0=zookeeper0:2888:3888 server.1=zookeeper1:28881:38881 server.2=zookeeper2:28882:38882 zookeeper1: image: alex/zookeeper_cluster:3.4.6 container_name: zookeeper1 hostname: zookeeper1 ports: - &quot;2182:2182&quot; - &quot;28881:28881&quot; - &quot;38881:38881&quot; expose: - 2182 - 2888 - 3888 environment: ZOOKEEPER_PORT: 2182 ZOOKEEPER_ID: 1 ZOOKEEPER_SERVERS: server.0=zookeeper0:2888:3888 server.1=zookeeper1:28881:38881 server.2=zookeeper2:28882:38882 zookeeper2: image: alex/zookeeper_cluster:3.4.6 container_name: zookeeper2 hostname: zookeeper2 ports: - &quot;2183:2183&quot; - &quot;28882:28882&quot; - &quot;38882:38882&quot; expose: - 2183 - 2888 - 3888 environment: ZOOKEEPER_PORT: 2183 ZOOKEEPER_ID: 2 ZOOKEEPER_SERVERS: server.0=zookeeper0:2888:3888 server.1=zookeeper1:28881:38881 server.2=zookeeper2:28882:38882 kafka0: image: alex/kafka_cluster:0.8.2.2 container_name: kafka0 hostname: kafka0 ports: - &quot;9092:9092&quot; environment: ZOOKEEPER_CONNECT: zookeeper0:2181,zookeeper1:2182,zookeeper2:2183 BROKER_ID: 0 BROKER_PORT: 9092 ADVERTISED_HOST_NAME: kafka0 HOST_NAME: kafka0 volumes: - /var/run/docker.sock:/var/run/docker.sock depends_on: - zookeeper0 - zookeeper1 - zookeeper2 expose: - 9092 kafka1: image: alex/kafka_cluster:0.8.2.2 container_name: kafka1 hostname: kafka1 ports: - &quot;9093:9093&quot; environment: ZOOKEEPER_CONNECT: zookeeper0:2181,zookeeper1:2182,zookeeper2:2183 BROKER_ID: 1 BROKER_PORT: 9093 ADVERTISED_HOST_NAME: kafka1 HOST_NAME: kafka1 volumes: - /var/run/docker.sock:/var/run/docker.sock depends_on: - zookeeper0 - zookeeper1 - zookeeper2 expose: - 9093 kafka2: image: alex/kafka_cluster:0.8.2.2 container_name: kafka2 hostname: kafka2 ports: - &quot;9094:9094&quot; environment: ZOOKEEPER_CONNECT: zookeeper0:2181,zookeeper1:2182,zookeeper2:2183 BROKER_ID: 2 BROKER_PORT: 9094 ADVERTISED_HOST_NAME: kafka2 HOST_NAME: kafka2 volumes: - /var/run/docker.sock:/var/run/docker.sock depends_on: - zookeeper0 - zookeeper1 - zookeeper2 expose: - 9094 nimbus: image: alex/storm:1.0.3 container_name: nimbus command: nimbus -c nimbus.host=nimbus environment: - STORM_ZOOKEEPER_SERVERS=zookeeper0,zookeeper1,zookeeper2 hostname: nimbus ports: - &quot;6627:6627&quot; ui: image: alex/storm:1.0.3 container_name: ui command: ui -c nimbus.host=nimbus environment: - STORM_ZOOKEEPER_SERVERS=zookeeper0,zookeeper1,zookeeper2 hostname: ui ports: - &quot;8080:8080&quot; depends_on: - nimbus supervisor1: image: alex/storm:1.0.3 container_name: supervisor1 command: supervisor -c nimbus.host=nimbus -c supervisor.slots.ports=[6700,6701,6702,6703] environment: - STORM_ZOOKEEPER_SERVERS=zookeeper0,zookeeper1,zookeeper2 hostname: supervisor1 ports: - &quot;8000:8000&quot; depends_on: - nimbus","tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://blog.yaodataking.com/tags/Kafka/"},{"name":"Storm","slug":"Storm","permalink":"http://blog.yaodataking.com/tags/Storm/"}]},{"title":"docker ps 命令详解","date":"2017-04-09T06:19:35.000Z","path":"2017/04/09/docker-ps/","text":"对docker容器的管理，docker ps命令必不可少。1.基本命令格式基本命令格式为，docker ps [OPTIONS]，其中OPTIONS有如下选项 --all, -a false 显示所有容器默认只显示正在运行中的 --filter, -f 根据条件过滤 --format 显示所需字段 --last, -n -1 显示最后创建的n个容器 --latest, -l false 显示最后创建的容器 --no-trunc false 输出时不截断字段 --quiet, -q false 只显示容器id --size, -s false 显示文件大小`&lt;/pre&gt; 2.过滤条件使用 docker ps 给了我们查看容器的能力，但是如果容器的数量足够多，显然需要一个过滤条件，目前支持的过滤命令有： &lt;pre&gt;`id (容器的id) label (label=&lt;key&gt; or label=&lt;key&gt;=&lt;value&gt;) name (容器的名字) exited (列出已退出的容器. 需要与 --all选项一起用) status (状态created|restarting|running|removing|paused|exited|dead) ancestor (&lt;image-name&gt;[:&lt;tag&gt;], &lt;image id&gt; or &lt;image@digest&gt;) - 过滤所有含某个镜像或层产生的容器 before (容器的ID或名字) - 过滤给定容器ID或名字之前创建的容器 since (容器的ID或名字) - 过滤给定容器ID或名字之后创建的容器 isolation (default|process|hyperv) (Windows daemon only) volume (volume name或mount point) - 过滤有mount volume的容器 network (network id或name) - 过滤指定network id或名字的容器 health (starting healthy unhealthy none) - 过滤指定health状态的容器`&lt;/pre&gt; 以下是一些具体使用例子 &lt;pre&gt;` $ docker ps --filter &quot;label=color&quot; $ docker ps --filter &quot;label=color=blue&quot; 下列筛选器匹配所有容器的名称包含kafka字符串。 $ docker ps --filter &quot;name=kafka&quot; 容器正常停止的过滤 $ docker ps -a --filter &apos;exited=0&apos; 使用kill命令退出的容器 $ docker ps -a --filter &apos;exited=137&apos; 正在运行中的容器 $ docker ps --filter status=running 所有含ubuntu字符串的镜像产生的容器 $ docker ps --filter ancestor=ubuntu 所有层d0e008c6cf02的镜像产生的容器 $ docker ps --filter ancestor=d0e008c6cf02 过滤容器9c3527ed70ce之前创建的容器 $ docker ps -f before=9c3527ed70ce 过滤容器6e63f6ff38b0之后创建的容器 $ docker ps -f since=6e63f6ff38b0 过滤含有指定卷的容器 $ docker ps --filter volume=remote-volume 过滤含网络net1的容器 $ docker ps --filter network=net1 过滤publish或expose指定端口的容器 $ docker ps --filter publish=80 $ docker ps --filter expose=8000-8080/tcp `&lt;/pre&gt; 3.自定义显示字段 &lt;pre&gt;`--format选项给了我们可以自定义显示字段的能力 Placeholder Description .ID Container ID .Image Image ID .Command Quoted command .CreatedAt Time when the container was created. .RunningFor Elapsed time since the container was started. .Ports Exposed ports. .Status Container status. .Size Container disk size. .Names Container names. .Labels All labels assigned to the container. .Label Value of a specific label for this container. For example &apos;{{.Label \"com.docker.swarm.cpu\"}}&apos; .Mounts Names of the volumes mounted in this container. .Networks Names of the networks attached to this container. `&lt;/pre&gt; 以下具体使用例子 &lt;pre&gt;` 显示容器的id和命令 $ docker ps --format &quot;{{.ID}}: {{.Command}}&quot; a87ecb4f327c: /bin/sh -c #(nop) MA 01946d9d34d8: /bin/sh -c #(nop) MA c1d3b0166030: /bin/sh -c yum -y up 41d50ecd2f57: /bin/sh -c #(nop) MA 显示容器的id和label并带字段名 $ docker ps --format &quot;table {{.ID}}\\t{{.Labels}}&quot; CONTAINER ID LABELS a87ecb4f327c com.docker.swarm.node=ubuntu,com.docker.swarm.storage=ssd 01946d9d34d8 c1d3b0166030 com.docker.swarm.node=debian,com.docker.swarm.cpu=6 41d50ecd2f57 com.docker.swarm.node=fedora,com.docker.swarm.cpu=3,com.docker.swarm.storage=ssd","tags":[{"name":"Docker","slug":"Docker","permalink":"http://blog.yaodataking.com/tags/Docker/"}]},{"title":"机器学习之三：降维技术","date":"2017-03-26T15:10:42.000Z","path":"2017/03/26/machine-learning-3/","text":"1.基本概念机器学习领域中所谓的降维就是指采用某种映射方法，将原高维空间中的数据点映射到低维度的空间中。降维的本质是学习一个映射函数 f : x-&gt;y，其中x是原始数据点的表达，目前最多使用向量表达形式。y是数据点映射后的低维向量表达，通常y的维度小于x的维度（当然提高维度也是可以的）。f可能是显式的或隐式的、线性的或非线性的。 2.降维的作用 降低时间和空间复杂度 节省了提取不必要特征的开销 去掉数据集中夹杂的噪声较简单的模型在小数据集上有更强的鲁棒性当数据能有较少的特征进行解释，我们可以更好的解释数据，使得我们可以提取知识实现数据可视化 3.降维的方法3.1主成分分析PCA（Principal Component Analysis）Pearson于1901年提出，再由Hotelling(1933)加以发展的一种多变量统计方法。通过析取主成分显出最大的个别差异，也用来削减回归分析和聚类分析中变量的数目，可以使用样本协方差矩阵或相关系数矩阵作为出发点进行分析。Kaiser主张(1960)将特征值小于1的成分放弃，只保留特征值大于1的成分，如果能用不超过3-5个成分就能解释变异的80%，就算是成功。基本思想:设法将原先众多具有一定相关性的指标，重新组合为一组新的互相独立的 综合指标，并代替原先的指标。 3.2因子分析降维的一种方法，是主成分分析的推广和发展。是用于分析隐藏在表面现象背后的因子作用的统计模型。试图用最少个数的不可测的公共因子的线性函数与特殊因子之和来描述原来观测的每一分量。 因子分析的主要用途减少分析变量个数通过对变量间相关关系的探测，将原始变量分组，即将相关性高的变量分为一组，用共性因子来代替该变量使问题背后的业务因素的意义更加清晰呈现 与主成分分析的区别主成分分析侧重“变异量”，通过转换原始变量为新的组合变量使到数据的“变异量”最大，从而能把样本个体之间的差异最大化，但得出来的主成分往往从业务场景的角度难以解释。因子分析更重视相关变量的“共变异量”，组合的是相关性较强的原始变量，目的是找到在背后起作用的少量关键因子，因子分析的结果往往更容易用业务知识去加以解释。 3.3线性判别式分析（Linear Discriminant Analysis）线性判别式分析（Linear Discriminant Analysis），简称为LDA。也称为Fisher线性判别（Fisher Linear Discriminant，FLD），是模式识别的经典算法，在1996年由Belhumeur引入模式识别和人工智能领域。基本思想是将高维的模式样本投影到最佳鉴别矢量空间，以达到抽取分类信息和压缩特征空间维数的效果，投影后保证模式样本在新的子空间有最大的类间距离和最小的类内距离，即模式在该空间中有最佳的可分离性。LDA与前面介绍过的PCA都是常用的降维技术。PCA主要是从特征的协方差角度，去找到比较好的投影方式。LDA更多的是考虑了标注，即希望投影后不同类别之间数据点的距离更大，同一类别的数据点更紧凑。 3.4多维尺度分析（Multi Dimensional Scaling）多维尺度分析（Multi Dimensional Scaling）,简称为MDS。MDS的目标是在降维的过程中将数据的dissimilarity(差异性)保持下来，也可以理解降维让高维空间中的距离关系与低维空间中距离关系保持不变。MDS利用成对样本间相似性，目的是利用这个信息去构建合适的低维空间，使得样本在此空间的距离和在高维空间中的样本间的相似性尽可能的保持一致。 3.5局部线性嵌入Locally Linear Embedding（LLE）Locally Linear Embedding（LLE）是一种非线性降维算法，它能够使降维后的数据较好地保持原有流形结构。LLE可以说是流形学习方法最经典的工作之一。很多后续的流形学习、降维方法都与LLE有密切联系。LLE算法认为每一个数据点都可以由其近邻点的线性加权组合构造得到。算法的主要步骤分为三步：(1)寻找每个样本点的k个近邻点；（2）由每个 样本点的近邻点计算出该样本点的局部重建权值矩阵；（3）由该样本点的局部重建权值矩阵和其近邻点计算出该样本点的输出值。","tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://blog.yaodataking.com/tags/机器学习/"}]},{"title":"Storm的基本概念和组件","date":"2017-03-07T11:54:33.000Z","path":"2017/03/07/storm-2/","text":"Storm是什么Storm是开源的、分布式、流式计算系统。 我们知道Hadoop是开源的、分布式 系统，但是，Hadoop它只能处理适合进行批量计算的需求，对于，非批量的计算就不能够满足要求了。于是类似Storm流式计算系统就雨后春笋般的冒出来了，如Yahoo的S4，IBM的StreamBase，Amazon的Kinesis，Spark的Streaming，Google的Millwheel，不消说，Storm是业内最知名的。 Storm的主要特点Storm的主工程师Nathan Marz表示： Storm可以方便地在一个计算机集群中编写与扩展复杂的实时计算，Storm之于实时处理，就好比Hadoop之于批处理。Storm保证每个消息都会得到处理，而且它很快——在一个小集群中，每秒可以处理数以百万计的消息。更棒的是你可以使用任意编程语言来做开发。Storm的主要特点如下：1.简单的编程模型。类似于MapReduce降低了并行批处理复杂性，Storm降低了进行实时处理的复杂性。2.可以使用各种编程语言。你可以在Storm之上使用各种编程语言。默认支持Clojure、Java、Ruby和Python。要增加对其他语言的支持，只需实现一个简单的Storm通信协议即可。3.容错性。Storm会管理工作进程和节点的故障。4.水平扩展。计算是在多个线程、进程和服务器之间并行进行的。5.可靠的消息处理。Storm保证每个消息至少能得到一次完整处理。任务失败时，它会负责从消息源重试消息。6.快速。系统的设计保证了消息能得到快速的处理，使用ØMQ作为其底层消息队列。7.本地模式。Storm有一个“本地模式”，可以在处理过程中完全模拟Storm集群。这让你可以快速进行开发和单元测试。 不过Storm不是一个完整的解决方案。使用Storm时你需要关注以下几点：如果使用的是自己的消息队列，需要加入消息队列做数据的来源和产出的代码需要考虑如何做故障处理：如何记录消息队列处理的进度，应对Storm重启，挂掉的场景需要考虑如何做消息的回退：如果某些消息处理一直失败怎么办？ Storm组件及概念1. Nimbus：雨云，主节点的守护进程，负责为工作节点分发任务(任务写入Zookeeper)。2. Supervisor：从Zookeeper接收Nimbus分配的任务，启动和停止属于自己管理的worker进程。3. Worker：运行具体处理组件逻辑的进程。4. Topology：一个Storm拓扑打包了一个实时处理程序的逻辑。一个Storm拓扑类似一个Hadoop的MapReduce任务(Job)。主要区别是MapReduce任务最终会结束，而拓扑会一直运行（当然直到你杀死它)。一个拓扑是一个通过流分组(stream grouping)把Spout和Bolt连接到一起的拓扑结构。图的每条边代表一个Bolt订阅了其他Spout或者Bolt的输出流。一个拓扑就是一个复杂的多阶段的流计算。5. Spout：龙卷，是一个在Topology中产生源数据流的组件。通常情况下Spout会从外部数据源中读取数据，然后转换为Topology内部的源数据。Spout 是一个主动的角色，其接口中有个nextTuple()函数， Storm框架会不停地调用此函数，用户只要在其中生成源数据即可。6. Bolt：雷电，是一个在Topology中接受数据然后执行处理的组件。Bolt可以执行过滤、函数操作、合并、写数据库等任何操作。Bolt是一个被动的角色，其接口中有个execute(Tuple input)函数,在接受到消息后会调用此函数，用户可以在其中执行自己想要的操作。7. Task：Worker中每一个Spout/Bolt的线程称为一个Task. 在 Storm 0.8之后，task不再与物理线程对应，同一个Spout/Bolt的Task可能会共享一个物理线程，该线程称为Executor。8. Tuple：元组，一次消息传递的基本单元。9. Stream：源源不断传递的Tuple就组成了stream。10.Stream Grouping：即消息的partition方法。流分组策略告诉Topology如何在两个组件之间发送Tuple。 Storm 中提供若干种实用的grouping方式，包括shuffle, fields hash, all, global, none, direct和localOrShuffle等。Storm基本架构Nimbus和Supervisor之间的通信依靠Zookeeper来完成，并且Nimbus进程和Supervisor都是快速失败和无状态的。所有的状态要么在Zookeeper里面，要么在本地磁盘上。这就意味着你可以用Kill -9 来杀死 Nimbus和Supervisor进程，然后在重启它们，它们可以继续工作，就像什么也没发生。这个设计使Storm具有非常高的稳定性。 与Hadoop的区别Hadoop使用磁盘作为中间交换的介质，而Storm的数据是一直在内存中流转的。两者面向的领域也不完全相同，一个是批量处理，基于任务调度的；另外一个是实时处理，基于流。以水为例，Hadoop可以看作是纯净水，一桶桶地搬；而Storm是用水管，预先接好（Topology），然后打开水龙头，水就源源不断地流出来了。 与Spark Streaming的区别虽然这两个框架都提供可扩展性和容错性,它们根本的区别在于他们的处理模型。Storm处理的是每次传入的一个事件，而Spark Streaming是处理某个时间段窗口内的事件流。因此,Storm处理一个事件可以达到秒内的延迟，而Spark Streaming则有几秒钟的延迟。","tags":[{"name":"Storm","slug":"Storm","permalink":"http://blog.yaodataking.com/tags/Storm/"}]},{"title":"机器学习之二：回归分析","date":"2017-02-28T13:11:13.000Z","path":"2017/02/28/machine-learning-2/","text":"1.基本概念我们首先了解一下关于回归的基本概念。因变量（dependent variable）是函数中的专业名词，函数关系式中，某些特定的数会随另一个（或另几个）会变动的数的变动而变动，就称为因变量。如：Y=f(X)。此式表示为：Y随X的变化而变化。Y是因变量，X是自变量。自变量（Independent variable）一词来自数学。在数学中，y=f（x）。在这一方程中自变量是x，因变量是y。将这个方程运用到心理学的研究中，自变量是指研究者主动操纵，而引起因变量发生变化的因素或条件，因此自变量被看作是因变量的原因。自变量有连续变量和类别变量之分。如果实验者操纵的自变量是连续变量，则实验是函数型实验。如实验者操纵的自变量是类别变量，则实验是因素型的。在心理学实验中，一个明显的问题是要有一个有机体作为被试对刺激作反应。显然，这里刺激变量就是自变量。变量间的关系：1)变量间有完全确定的关系：函数关系式2)变量间有一定的关系，无法用函数形式表示出来，为研究这类变量之间的关系就需要通过大量试验或观测获得数据，用统计方法去寻找它们间的关系，这种关系反映了变量间的统计规律，研究这类统计规律的方法之一就是回归分析。 回归分析（regression analysis)是确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法。运用十分广泛，回归分析按照涉及的变量的多少，分为一元回归和多元回归分析；在线性回归(Linear Regression)中，按照因变量的多少，可分为简单回归分析和多重回归分析；按照自变量和因变量之间的关系类型，可分为线性回归分析和非线性回归分析。如果在回归分析中，只包括一个自变量和一个因变量，且二者的关系可用一条直线近似表示，这种回归分析称为一元线性回归分析。如果回归分析中包括两个或两个以上的自变量，且自变量之间存在线性相关，则称为多元线性回归分析。回归分析实际上就是利用样本（已知数据），产生拟合方程，从而（对未知数据）迚行预测。 2. 主要算法目前的回归算法中，只要有以下几种算法。线性回归（Linear Regression）普通最小二乘回归（Ordinary Least Squares Regression，OLSR）逻辑回归（Logistic Regression）逐步回归（Stepwise Regression）岭回归（Ridge Regression）LASSO回归（Least Absolute Shrinkage and Selection Operator）ElasticNet回归优点：直接、快速、知名度高缺点：要求严格的假设 需要处理异常值 2.1线性回归（Linear Regression）线性回归用最适直线(回归线)去建立因变量Y和一个或多个自变量X之间的关系。可以用公式来表示：Y=a+bX+ea为截距，b为回归线的斜率，e是误差项。如何找到那条回归线？我们可以通过最小二乘法把这个问题解决。其实最小二乘法就是线性回归模型的损失函数，只要把损失函数做到最小时得出的参数，才是我们最需要的参数。*2.2普通最小二乘 (OLS) 回归在 OLS 回归中，估计方程可通过确定将样本的数据点与由方程预测的值之间的距离平方和最小化的方程计算得出。应该满足的 OLS 假定仅当满足以下假定时，OLS 回归才会提供最精确的无偏估计值。1) 回归模型的系数为线性系数。最小二乘可通过变换变量（而不是系数）来为曲率建模。您必须指定适当的函数形式才能正确地为任何曲率建模。 在此，对预测变量 X 进行了平方计算以便为曲率建模。2) 残差的均值为零。模型中包含常量将迫使均值等于零。3) 所有预测变量都与残差不相关。4) 残差与残差之间不相关（序列相关）。5) 残差具有恒定方差。6) 任何预测变量都不与其他预测变量完全相关 (r=1)。最好也避免不完全的高度相关（多重共线性）。7) 残差呈正态分布。由于仅当所有这些假定都满足时，OLS 回归才会提供最佳估计值，因此检验这些假定极为重要。 常用方法包括检查残差图、使用失拟检验以及使用方差膨胀因子 (VIF) 检查预测变量之间的相关性。 2.3逻辑回归（Logistic Regression）逻辑回归（logistic analysis)就是一种减小预测范围，将预测值限定为[0,1]间的一种回归模型。Logistic回归的因变量可以是二分类的，也可以是多分类的，但是二分类的更为常用，也更加容易解释。所以实际中最常用的就是二分类的Logistic回归。 2.4逐步回归（Stepwise Regression）在实际问题中, 人们总是希望从对因变量有影响的诸多变量中选择一些变量作为自变量, 应用多元回归分析的方法建立“最优”回归方程以便对因变量进行预报或控制。所谓“最优”回归方程, 主要是指希望在回归方程中包含所有对因变量影响显著的自变量而不包含对影响不显著的自变量的回归方程。逐步回归分析正是根据这种原则提出来的一种回归分析方法。它的主要思路是在考虑的全部自变量中按其对的作用大小, 显著程度大小或者说贡献大小, 由大到小地逐个引入回归方程, 而对那些对作用不显著的变量可能始终不被引人回归方程。另外, 己被引人回归方程的变量在引入新变量后也可能失去重要性, 而需要从回归方程中剔除出去。引人一个变量或者从回归方程中剔除一个变量都称为逐步回归的一步, 每一步都要进行检验, 以保证在引人新变量前回归方程中只含有对影响显著的变量, 而不显著的变量已被剔除。2.5岭回归（Ridge Regression）岭回归是一种专用于共线性数据分析的有偏估计回归方法，实质上是一种改良的最小二乘估计法，通过放弃最小二乘法的无偏性，以损失部分信息、降低精度为代价获得回归系数更为符合实际、更可靠的回归方法，对病态数据的拟合要强于最小二乘法。2.6LASSO回归（Least Absolute Shrinkage and Selection Operator）Lasso也是通过惩罚其回归系数的绝对值。与岭回归不同的是，Lasso回归在惩罚方程中用的是绝对值，而不是平方。这就使得惩罚后的值可能会变成0.2.7ElasticNet回归ElasticNet回归是Lasso回归和岭回归的组合。它会事先训练L1和L2作为惩罚项。当许多变量是相关的时候，Elastic-net是有用的。Lasso一般会随机选择其中一个，而Elastic-net则会选在两个。与Lasso和岭回归的利弊比较，一个实用的优点就是Elastic-Net会继承一些岭回归的稳定性。3.如何选用回归模型面对如此多的回归模型，最重要的是根据自变量因变量的类型、数据的维数和其他数据的重要特征去选择最合适的方法。以下是我们选择正确回归模型时要主要考虑的因素：1.数据探索是建立预测模型不可或缺的部分。它应该是在选择正确模型之前要做的。2.为了比较不同模型的拟合程度，我们可以分析不同的度量，比如统计显著性参数、R方、调整R方、最小信息标准、BIC和误差准则。另一个是Mallow‘s Cp准则。3.交叉验证是验证预测模型最好的方法。你把你的数据集分成两组：一组用于训练，一组用于验证。4.如果你的数据集有许多让你困惑的变量，你就不应该用自动模型选择方法，因为你不想把这些变量放在模型当中。5.不强大的模型往往容易建立，而强大的模型很难建立。6.回归正则方法在高维度和多重共线性的情况下表现的很好。","tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://blog.yaodataking.com/tags/机器学习/"}]},{"title":"《未来简史》读书笔记","date":"2017-02-23T14:52:16.000Z","path":"2017/02/23/homo-tommorrow/","text":"如果说《人类简史》是真正从一个全新的角度把智人的历史作为整体研究的一部巨著，那么《未来简史》则更进一步指出智人未来的发展路线。也许你会为作者所说的神人可以永生而心驰神往，但是别忘了还用两种人-无用的人和没有自主的人，你会是哪种人？他们的关系是怎样的？谁会真正拥有自由意识？哪一个是真正的自我？生物真的只是一堆算法吗？生命真的只是数据处理吗？智能和意识，到底哪个更有价值？未来实际怎样发展，谁也说不准。不管怎么样，我们学习历史的最重要的目的就是要摆脱历史的枷锁。历史学家的作用不是在紧要关头告诉我们下一步的历史一定会往哪个方向走，反而恰恰是告诉我们你可以想象多种不同的可能性，让历史往一个不一样的方向走。不管你认不认同赫拉利在《未来简史》中的观点，然而他的思考是如此可贵！因为当我们思考未来时，往往会受限于当今的意识形态和社会制度，要以新的方式来思考或行动并非易事。赫拉利至少给我们打开了一扇窗。","tags":[]},{"title":"使用Docker快速安装简单Storm集群环境","date":"2017-02-23T02:59:05.000Z","path":"2017/02/23/storm-1/","text":"1.环境准备主机：虚拟机Ubuntu 16.04 内存2G 硬盘20GBDocker镜像：zookeeper版本3.4.6 Storm版本1.0.22.安装2.1 docker filezookeeper的dockerfile FROM java:openjdk-8-jre-alpine ARG MIRROR=http://mirrors.aliyun.com/ ARG VERSION=3.4.6 LABEL name=&quot;zookeeper&quot; version=$VERSION RUN apk update &amp;amp;&amp;amp; apk add ca-certificates &amp;amp;&amp;amp; \\ apk add tzdata &amp;amp;&amp;amp; \\ ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime &amp;amp;&amp;amp; \\ echo &quot;Asia/Shanghai&quot; &amp;gt; /etc/timezone RUN apk add --no-cache wget bash \\ &amp;amp;&amp;amp; mkdir /opt \\ &amp;amp;&amp;amp; wget -q -O - $MIRROR/apache/zookeeper/zookeeper-$VERSION/zookeeper-$VERSION.tar.gz | tar -xzf - -C /opt \\ &amp;amp;&amp;amp; mv /opt/zookeeper-$VERSION /opt/zookeeper \\ &amp;amp;&amp;amp; cp /opt/zookeeper/conf/zoo_sample.cfg /opt/zookeeper/conf/zoo.cfg \\ &amp;amp;&amp;amp; mkdir -p /tmp/zookeeper EXPOSE 2181 WORKDIR /opt/zookeeper VOLUME [&quot;/opt/zookeeper/conf&quot;, &quot;/tmp/zookeeper&quot;] ENTRYPOINT [&quot;/opt/zookeeper/bin/zkServer.sh&quot;] CMD [&quot;start-foreground&quot;]`&lt;/pre&gt; Storm的dockerfile &lt;pre&gt;`FROM openjdk:8-jre-alpine # Install required packages RUN apk add --no-cache \\ bash \\ python \\ su-exec ENV STORM_USER=storm \\ STORM_CONF_DIR=/conf \\ STORM_DATA_DIR=/data \\ STORM_LOG_DIR=/logs # Add a user and make dirs RUN set -x \\ &amp;&amp; adduser -D &quot;$STORM_USER&quot; \\ &amp;&amp; mkdir -p &quot;$STORM_CONF_DIR&quot; &quot;$STORM_DATA_DIR&quot; &quot;$STORM_LOG_DIR&quot; \\ &amp;&amp; chown -R &quot;$STORM_USER:$STORM_USER&quot; &quot;$STORM_CONF_DIR&quot; &quot;$STORM_DATA_DIR&quot; &quot;$STORM_LOG_DIR&quot; ARG GPG_KEY=ACEFE18DD2322E1E84587A148DE03962E80B8FFD ARG DISTRO_NAME=apache-storm-1.0.2 # Download Apache Storm, verify its PGP signature, untar and clean up RUN set -x \\ &amp;&amp; apk add --no-cache --virtual .build-deps \\ gnupg \\ &amp;&amp; wget -q &quot;http://www.apache.org/dist/storm/$DISTRO_NAME/$DISTRO_NAME.tar.gz&quot; \\ &amp;&amp; wget -q &quot;http://www.apache.org/dist/storm/$DISTRO_NAME/$DISTRO_NAME.tar.gz.asc&quot; \\ &amp;&amp; export GNUPGHOME=&quot;$(mktemp -d)&quot; \\ &amp;&amp; gpg --keyserver ha.pool.sks-keyservers.net --recv-key &quot;$GPG_KEY&quot; \\ &amp;&amp; gpg --batch --verify &quot;$DISTRO_NAME.tar.gz.asc&quot; &quot;$DISTRO_NAME.tar.gz&quot; \\ &amp;&amp; tar -xzf &quot;$DISTRO_NAME.tar.gz&quot; \\ &amp;&amp; chown -R &quot;$STORM_USER:$STORM_USER&quot; &quot;$DISTRO_NAME&quot; \\ &amp;&amp; rm -r &quot;$GNUPGHOME&quot; &quot;$DISTRO_NAME.tar.gz&quot; &quot;$DISTRO_NAME.tar.gz.asc&quot; \\ &amp;&amp; apk del .build-deps WORKDIR $DISTRO_NAME ENV PATH $PATH:/$DISTRO_NAME/bin COPY docker-entrypoint.sh / ENTRYPOINT [&quot;/docker-entrypoint.sh&quot;]`&lt;/pre&gt; 2.2 启动 step1 启动zookeeper &lt;pre&gt;`docker run -d --restart always --name zookeeper zookeeper:3.4.6`&lt;/pre&gt; step2 启动Nimbus &lt;pre&gt;`docker run -d --restart always --name nimbus --link zookeeper storm:1.0.2 storm nimbus`&lt;/pre&gt; step3 启动Storm UI &lt;pre&gt;`docker run -d -p 8080:8080 --restart always --name ui --link nimbus storm:1.0.2 storm ui`&lt;/pre&gt; 此时进入127.0.0.1:8080,我们看到nimbus已启动。 [![](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2017/02/2017-02-23_09-26-09.png)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2017/02/2017-02-23_09-26-09.png) step4 启动Supervisor &lt;pre&gt;`docker run -d --restart always --name supervisor1 --link zookeeper --link nimbus storm:1.0.2 storm supervisor`&lt;/pre&gt; [![](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2017/02/2017-02-23_09-29-34.png)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2017/02/2017-02-23_09-29-34.png) 我们看到Supervisor1已正常启动，如果需要我们可以启动多个Supervisor，至此简单Storm集群环境安装完毕。 3.提交Topology 进入nimbus的docker,使用storm的example WordCountTopology提交Topology &lt;pre&gt;`docker exec -it nimbus bash cd examples/storm-starter storm jar storm-starter-topologies-1.0.2.jar org.apache.storm.starter.WordCountTopology first-topology 运行情况详细信息","tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"http://blog.yaodataking.com/tags/zookeeper/"},{"name":"Storm","slug":"Storm","permalink":"http://blog.yaodataking.com/tags/Storm/"}]},{"title":"机器学习之一：什么是机器学习？","date":"2017-01-17T13:24:09.000Z","path":"2017/01/17/machine-learning-1/","text":"1.什么是机器学习？长期以来众说纷纭，Langley（1996）定义机器学习为：“机器学习是一门人工智能的科学，该领域的主要研究对象是人工智能，特别是如何在经验学习中改善具体算法的性能”。Mitchell（1997）在《Machine Learning》中写道：“机器学习是计算机算法的研究，并通过经验提高其自动进行改善”。Alpaydin（2004）提出自己对机器学习的定义：“机器学习是用数据或以往的经验，来优化计算机程序的性能标准”。Drew Conway在《Machine Learning for Hackers》书中定义：“机器学习就是一套工具和方法，凭借这些工具和方法我们可以从观测到的样本中提炼模式、归纳知识。换句话说，在特定情境下，我们可以记录研究对象的行为，从中学习，然后对其行为建模，该模型反过来促进我们对该情境有更深入的理解”。麦好在《机器学习实践指南：案例应用解析》中定义：“机器学习是一门人工智能的科学，该领域的主要研究对象是人工智能，专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能，它是人工智能的核心，是使计算机具有智能的根本途径。机器学习的研究方法通常是根据生理学、认知科学等对人类学习机理的了解，建立人类学习过程的计算模型或认识模型，发展各种学习理论和学习方法，研究通用的学习算法并进行理论上的分析，建立面向任务的具有特定应用的学习系统”。 2.机器学习的发展真正的机器学习研究起步较晚，它的发展过程大体上可分为以下4个时期：第一阶段是在20世纪50年代中叶到20世纪60年代中叶，属于热烈时期。第二阶段是在20世纪60年代中叶至20世纪70年代中叶，被称为机器学习冷静期。第三阶段是从20世纪70年代中叶至20世纪80年代中叶，称为机器学习复兴期。最新的阶段起始于1986年。当时，机器学习综合应用了心理学、生物学和神经生理学以及数学、自动化和计算机科学，并形成了机器学习理论基础，同时还结合各种学习方法取长补短，形成集成学习系统。 3.机器学习比较活跃的领域1）数据分析和数据挖掘数据分析与挖掘技术是机器学习算法和数据存取技术的结合，利用机器学习提供的统计分析、知识发现等手段分析海量数据，同时利用数据存取机制实现数据的高效读写。机器学习在数据分析与挖掘领域中拥有无可取代的地位，2012年Hadoop进军机器学习领域就是一个很好的例子。2）模式识别语音输入，OCR，手写输入，通讯监控，车牌识别，指纹识别，虹膜识别，脸像识别，小波分析3）智慧机器，机器人生产线机器人，人机对话，电脑博弈 4.机器学习常用软件1）MATLAB2）SPSS3）R4）PYTHON 5.具有代表性的算法1）回归预测及相应的降维技术线性回归，Logistic回归，主成分分析，因子分析，岭回归，LASSO最小回归系数分析2）分类器决策树，朴素贝叶斯，贝叶斯信念网绚，支持向量机，提升分类器准确率的Adaboost和随机森林算法3）聚类算法k-means，PCM4）人工神经网络模仿生物神经网络结构和功能的数学模型。","tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://blog.yaodataking.com/tags/机器学习/"}]},{"title":"关于区块链的一点思考","date":"2017-01-17T05:59:31.000Z","path":"2017/01/17/blockchain-1/","text":"回顾2016年，区块链无疑是最火的一个词，各种关于区块链的书籍就像雨后春笋般的冒出来，各种论坛活动都在讨论区块链，特别是科技金融行业。中本聪运用区块链技术本来是为了解决比特币的去中心化信用问题。然而，人们突然发现区块链技术可以在包括金融、贸易、征信、物联网、共享经济等诸多领域解决直接或间接依赖于第三方担保信任机构的问题。麦肯锡的研究表明，区块链技术,是继蒸汽机、电力、信息和互联网科技之后,目前最有潜力触发第五轮颠覆性革命浪潮的核心技术。 1.什么是区块链？区块链是一个公共的分布式总账，任何发生在此区块链网络上的交易会被约定的算法记录到区块链上，且满足以下条件： 存储基于分布式数据库 数据库是区块链的数据载体，区块链是交易的业务逻辑载体 区块链按时间序列化Block，且每个确认块是整个网络数据共识的唯一准则 区块链只对添加有效，对其他操作无效 基于非对称加密的公私钥验证 记账节点要求拜占庭将军问题可解/避免 共识过程(consensus progress)是演化稳定的，即面对一定量的不同节点的矛盾数据不会崩溃 共识过程能够解决double-spending问题 2.什么是比特币？比特币（BitCoin）的概念最初由中本聪在2009年提出，根据中本聪的思路设计发布的开源软件以及建构其上的P2P网络。比特币是一种P2P形式的数字货币。点对点的传输意味着一个去中心化的支付系统。比特币作为区块链的第一个应用，比特币总量固定，至2140年，总量就2100万个。 3.区块链和比特币书籍介绍。关于区块链和比特币的书籍已经有很多，这里仅列出以下几本开源的图书。《高盛区块链报告：区块链 从理论走向实践》《区块链技术指南》《比特币开发者指南》《精通比特币》《数字巨链》 4.区块链技术的挑战从技术角度讲，区块链涉及到的领域比较杂，包括分布式、存储、密码学、心理学、经济学、博弈论、网络协议等。怎么防止交易记录被篡改？怎么证明交易方的身份？怎么保护交易双方的隐私？这是密码学需要解决的问题。分布式一致性问题在很长一段时间内都将是极具学术价值的研究热点，核心的指标将包括容错的节点比例和收敛速度。区块链网络中的块信息需要写到数据库中进行存储，传统的关系数据库及NOSQL数据库能否支持，针对区块链的特点，是否会出现针对性的块数据库（BlockDB）？ 5.比特币会取代法币吗？不会。比特币为了体现稀缺性，限定了总量。但是稀缺性只代表他会有一定的价值，不代表信用。但是不排除将来法币用比特币类似虚拟币锚定。 6.比特币和黄金是什么关系？比特币目前作为一个投资品种，比特币和黄金是共存关系，但是比特币风险大于黄金。 7.个人信用区块链是什么？区块链技术天然适合征信系统，区块链的特性如：数据不可逆、无法篡改，数据由全体参与者共同维护等特性，征信系统其实就是一个公告板。 小结：与其说区块链是一项新技术，不如说是一种新的思想理念。区块链技术突然给人们打开了一扇窗，看到了未来的激动人心的世界。但是正像《区块链：定义未来金融与经济新格局》作者张健所说。 区块链这种协议式的、需要大规模协作和参与的颠覆式技术，其崛起的周期将比大多数人预想的要长，而最终影响的范围和深度也会远远超出大多数人的想象。区块链未来发展的过程不会一帆风顺，可能会经历过热甚至泡沫阶段，也可能会经历低谷。但我相信，区块链作为数字化浪潮下一个阶段的核心技术，最终将会构建出多样化生态的价值互联网，从而深刻改变未来商业社会的结构与我们每个人的生活。","tags":[{"name":"区块链","slug":"区块链","permalink":"http://blog.yaodataking.com/tags/区块链/"}]},{"title":"2016 阅读书单","date":"2016-12-30T06:33:39.000Z","path":"2016/12/30/reading-2016/","text":"《在历史的下降线行走》 张鸣历史有时候前进，有时则会退后，有上升，则有下降。细碎处的故事，空白处的讲述，才能真正反映历史的原貌。本书充斥了这样的故事和讲述。诸如“当牛记者碰到强人的时候”、“戴大头巾状如印度兵的中国士兵”、“懂兵法的和会打仗的”、“对毒与赌的另一种期待”……都是重大历史事件中被正史省略的故事，但正是这些正史瞧不上的鸡零狗碎一样的故事，让你感受到历史的真实，感受到它的血与肉，并带你看到纷扰世界中另一番景致。 《重来：更为简单有效的商业思维》贾森·弗里德 戴维·海涅迈尔·汉森大多数的企业管理的书籍都会告诉你：制定商业计划、分析竞争形势、寻找投资人等等。如果你要找的是那样的书，那么把这本书放回书架吧。《重来：更为简单有效的商业思维》呈现的是一种更好、更简单的经商成功之道。读完这本书，你就会明白为什么计划实际上百害而无一益，为什么你不需要外界投资人，为什么将竞争视而不见反倒会发展得更好。事实是你所需要的比你想象的少得多。你不必成为工作狂，你不必大量招兵买马，你不必把时间浪费在案头工作和会议上，你甚至不必拥有一间办公室。所有这些都仅仅是借口！用直截了当的语言和崇尚简约的方式，《重来》是每一个梦想着拥有自己的事业的人的完美指南。不管是作为中坚力量的企业家、小企业主，还是深陷令人不快的工作中的职场中人、被炒鱿鱼的受害者，抑或是想要“脱贫”的艺术家，都能在这一页页中找到弥足珍贵的指引。 《拿破仑传》 艾密尔·鲁特维克一个人凭借自信与勇气、激情与想象、勤奋与意志，究竟能达到怎样的高度？拿破仑给出了这个问题的答案。 《爱因斯坦：想象颠覆世界》 刘继军颠覆了以前对爱因斯坦的认识。 《特斯拉自传：被遗忘的科学巨匠》 尼古拉·特斯拉尼古拉·特斯拉，世界公论的旷世奇才的心路历程。 《思考，快与慢》 丹尼尔·卡尼曼每个人都要学点概率论，不仅用于生活中点点小事的判断，而且如果能获得大数据的支撑，那么判断和决策的质量将大幅提高。 《不贰：数据化思考》 车品觉答案不重要，思考的角度才重要。可见，要习得一套巧妙的数据化思考方式，三分靠想法，七分靠实践。所以，切勿空谈。每个人都要问一下自己：1.现在你所在的公司，面对的3大问题是什么？2.公司未来的3个月中，要解决的问题是什么？3.在过去的1个月中，你做对了什么，做错了什么？ 《大数据》 涂子沛带你了解数据及数据治理的历史。 《秦谜》 李开元历史的真相到底是什么？作者用严谨的逻辑将蛛丝马迹串联起来，基本还原了历史的真相。然而，历史走到了今天，也许真相已不重要。重要的是，秦始皇灭六国一统天下的历史虽顺应了天意，但严重违背了民意。以史为鉴，当权者顺天重民，调和天意和民意，可谓是须臾而不可忘记。 《新工业革命》 彼得·马什杰里米•里夫金的《第三次工业革命》提示我们正处在以“互联网+新能源”为聚合推动力的第三次工业革命中。彼得·马什的《新工业革命》从制造方式变革的角度出发，给我们展现了新工业革命的特征：科技化、全球化、互联化、绿色化、定制化和利基产业化。随着变革的加速，个性化量产系统逐渐占据主导地位。而制造业的未来在于差异化生产。作者指出，本次新工业革命将是第五次，大约开始于2005年，并将持续至2040年，但由此产生的影响将延续至21世纪末。虽然中国制造业已经做大做强，但也指出了不足及中国制造业出路：集群‘研发创新。展望未来，作者指出，高低成本混合运营的战略表明：在新工业革命时代，成功的路径并不唯一。只要发挥智慧、机智和想象力，条条大路通罗马。 《把时间当作朋友》 纪念版 李笑来这不是一本时间管理的书，因为时间是不可管理的。这是一本叫你如何打开心智，如何运用心智来和时间做朋友的书。越早读到这本书，将会使你越少走弯路。不管成功者还是落魄者都会从中的得到启发。 《中国误会了袁世凯》吕峥可以一读，比较客观的中国近代史，但是说到袁世凯从小立志反清，有点牵强。 《创业维艰》本·霍洛维茨写的很朴实，不管你是不是在创业，不管你是不是CEO，都能从中得到鼓励与启发。 《苏东坡传：中国文人从政的标志性人生》 林语堂很精彩的一本书，正如本书的副标题，苏东坡的人生的确是一波三折，由于政敌的迫害和阻扰，我们现在只能看到文学巨擎苏东坡，至于他的抱负，我们只能站在西湖苏提上想象一下了。 《武则天正传》林语堂这也许是作者的一个尝试，以一个李唐氏后裔的口吻来写，想想也知，对武则天的偏见，恨之入骨跃然纸上。林语堂写的传都带有很强的感情色彩。 《生命八卦》 袁越 2010版很好的科普文集，不仅传播了科学的知识，更是传播了科学的思维方式。 《铁血名将：辛弃疾》一个大词人瞬间还原抗金名将，然而辛弃疾注定是悲剧的，因为这不是一个名将的时代。 《即将到来的场景时代》【美】罗伯特·斯考伯 【美】谢尔·伊斯雷尔可以一读，跟着作者到未来畅想一番，想象大数据、移动设备、社交媒体、传感器和定位系统这五种技术带来的力量。 《非理性繁荣》 第二版 罗伯特·J·希勒不管你是否在金融行业，本书都值得你一读，储蓄，股票，房产，基金，债券等理财产品，已与百姓生活息息相关。整个金融市场的历史就是一部正反馈和反身性的历史。了解了这些，你就会透过丛丛迷雾对当前中国楼市有一个清晰的认识。 《大停滞》 [美]泰勒•考恩1．美国的发展靠什么？靠的就是“低垂的果实”；2．美国进入“大停滞”了吗？为什么会进入“大停滞”？3．大停滞正在结束，“万物互联”带来了新的未来；4．互联网带来的未来图景正是中国的机遇。低垂的果实：大量廉价的土地资源、人口红利和科技进步。 《群山回唱》[美] 卡勒德·胡赛尼时间、空间、人物组成了一幅跨越时空的画卷。 在别人的轨迹里，或许是配角，但在自己的轨迹里，每个人都是自己的主角。 《创造:只给勤奋者的创新书》 (美) 凯文·阿什顿创造不是魔法，而是工作，勤奋者才有机会。可以看看罗胖的解读 。https://book.douban.com/review/7749156/ 《我们为什么总是看错人》 王烁王烁的读书笔记，看一看就好。英文阅读很厉害，值得学习。 《技术简史:从海盗船到黑色直升机》 [美]德伯拉·L·斯帕这是一本2003年出版的书，然而书中揭示的规则至今还在适用，所以英文书名为Ruling the waves，中文名技术简史则更概括了内容。作者讲述了从15世纪葡萄牙人的探险开始的8段技术历史，历经大航海时代，电报时代、无线电时代、广播时代、电视时代、互联网时代、微软时代、网络音乐。每个技术历史都呈现相同的发展阶段，创新阶段，市场化阶段，充满创造性的混乱阶段和制定规则阶段。对我们有何借鉴意义？我们可以借此评估一项新技术所处的发展阶段从而顺势而为，创新阶段搞技术，市场化阶段拼速度，混乱阶段提升整合力，规则化阶段高标准。最重要的一点是，如果你不幸是一名“海盗”，一定要在规则改变之前收手，要不黑色直升机将被送上“断头台”。 《天下的当代性:世界秩序的实践与想象》 赵汀阳“天下者非一人之天下，乃天下之天下也。”从这句话理解来说，天下是属于世界上所有人的世界主权，天下是所有人共享的天下。如果这样理解，那么确实当今世界的天下历史尚未开始。未来何去何从呢？从凯文凯利的《失控》和《必然》我们或许找到答案。 《人类简史：从动物到上帝》 尤瓦尔·赫拉利我们之所以研究历史，不是为了要知道未来，而是要拓展视野，要了解现在的种种绝非“自然”，也并非无可避免。未来的可能性远超过我们的想象。 《智能时代：大数据与智能革命重新定义未来》 吴军从1956年以麦卡锡、明斯基、罗切斯特和香农等人提出人工智能概念以来，至今年2016正好是60年。如今大数据的概念及大数据技术的兴起重塑了人们的思维，使得人工智能的发展进入了一个新的阶段。未来已来。 《罗马人的故事》 盐野七生日本女作家盐野七生历时15年完成的15册巨著，详见读书笔记","tags":[{"name":"阅读书单","slug":"阅读书单","permalink":"http://blog.yaodataking.com/tags/阅读书单/"}]},{"title":"Kafka入门之十二:Kafka的高性能之道","date":"2016-12-30T02:03:24.000Z","path":"2016/12/30/kafka-12/","text":"在 LinkedIn 的 Kafka 的系统上，每天有超过 8000 亿条消息被发送，相当于超过 175 兆兆字节（terabytes）数据，另外，每天还会消耗掉 650 兆兆字节（terabytes）数据的消息，为什么Kafka有这样的能力去处理这么多产生的数据和消耗掉的数据? 下面我们就来分析一下Kafka的高性能之道。1.高效使用磁盘首先kafka的消息是不断追加到文件中的，因此数据只增加不更新。也没有记录级别的数据删除，只会整个segment删除。上述这个特性使kafka可以充分利用磁盘的顺序读写性能，顺序读写不需要硬盘磁头的寻道时间，只需很少的扇区旋转时间，所以速度远快于随机读写。另外kafka重度依赖底层操作系统提供的PageCache功能。当上层有写操作时，操作系统只是将数据写入PageCache，同时标记Page属性为Dirty。当读操作发生时，先从PageCache中查找，如果发生缺页才进行磁盘调度，最终返回需要的数据。2.使用零拷贝在Linux kernel2.2 之后出现了一种叫做”零拷贝(zero-copy)”系统调用机制，就是跳过“用户缓冲区”的拷贝，建立一个磁盘空间和内存的直接映射，数据不再复制到“用户态缓冲区”。传统模式下数据从文件传输到网络需要4次数据拷贝，4次上下文切换和2次系统调用，通过NIO的transferTo/transferFrom调用操作系统的sendfile实现了零拷贝。总共发生2次内核数据拷贝，2次上下文切换和1次系统调用，消除了CPU数据拷贝。这样系统上下文切换减少为2次，可以提升一倍的性能。3.数据批处理和压缩Producer和Consumer均支持批量处理数据，从而减少了网络传输的开销。比如每满100条消息才发送一次，或者每5秒发送一次。另外Producer可将数据压缩后发送给broker，从而减少网络传输代价。目前支持Snappy, Gzip和LZ4压缩。Producer压缩之后，在Consumer端需要解压，虽然增加了CPU的工作，但在对大数据处理上，瓶颈在网络上而不是CPU。4.使用Partition技术通过Partition实现了并行处理和水平扩展，Partition是Kafka(包括Kafka Stream)并行处理的最小单位，不同Partition可处于不同的Broker(节点)，可以充分利用多机资源。同一Broker(节点)上的不同Partition可置于不同的Directory，如果节点上 有多个Disk Drive，可将不同的Drive对应不同的Directory，从而使Kafka充分利用多Disk Drive的磁盘优势。5.使用ISRISR实现了可用性和一致性的动态平衡。ISR可容忍更多的节点失败，ISR如果要容忍f个节点失败，至少只需要f+1个节点。一旦Leader crash后，ISR中的任何replica皆可竞选成为Leader，如果所有replica都crash，可选择让第一个recover的replica或者第一个在ISR中的replica成为leader。6.zerocopy实验这里我们使用NIO的transferTo/transferFrom做文件数据传输性能测试，同时使用read／write方式做文件数据传输测试，并比较二者的差异。 使用FileChannel.transferFrom() 使用FileChannel.transferTo() 使用非直接模式ByteBuffer的read／write 使用直接模式 ByteBuffer的read／write测试文件大小：600+ MB测试的缓冲区大小：4KB机器配置：MacBook Pro i7 2.2GHz Mem 16GB SSD 256 GB 测试结果如下.结论：transferFrom和transferTo 数据传输性能差不多。transferFrom性能稍优使用直接模式和非直接模式 read/write 数据传输性能差不多。直接模式性能稍优transferFrom／To与read/write 性能高一倍以上。 代码如下： package zerocopy; import java.io.IOException; import java.nio.ByteBuffer; import java.nio.channels.FileChannel; import java.nio.file.Files; import java.nio.file.Path; import java.nio.file.Paths; import java.nio.file.StandardOpenOption; import java.util.EnumSet; public class ZeroCopyTest { private final static Path copy_from = Paths.get(\"/tmp/test/from/Security.mp4\"); private final static Path copy_to = Paths.get(\"/tmp/test/to/Security.mp4\"); private static long startTime, elapsedTime; private static int bufferSizeKB = 4; private static int bufferSize = bufferSizeKB * 1024; public static void main(String[] args) throws Exception { transferfrom(); transferTo(); nonDirectBuffer(); directBuffer(); } public static void transferfrom() { try (FileChannel fileChannel_from = (FileChannel.open(copy_from, EnumSet.of(StandardOpenOption.READ))); FileChannel fileChannel_to = (FileChannel.open(copy_to, EnumSet.of(StandardOpenOption.CREATE_NEW, StandardOpenOption.WRITE)))) { startTime = System.currentTimeMillis(); fileChannel_to.transferFrom(fileChannel_from, 0L, (int) fileChannel_from.size()); elapsedTime = System.currentTimeMillis() - startTime; System.out.println(\"transferFrom Time is \" + elapsedTime + \" ms\"); }catch (IOException ex) { System.err.println(ex); } deleteCopied(copy_to); } public static void transferTo() throws Exception{ try (FileChannel fileChannel_from = (FileChannel.open(copy_from, EnumSet.of(StandardOpenOption.READ))); FileChannel fileChannel_to = (FileChannel.open(copy_to, EnumSet.of(StandardOpenOption.CREATE_NEW, StandardOpenOption.WRITE)))) { startTime = System.currentTimeMillis(); fileChannel_from.transferTo(0L, fileChannel_from.size(), fileChannel_to); elapsedTime = System.currentTimeMillis() - startTime; System.out.println(\"transferTo Time is \" + elapsedTime + \" ms\"); }catch (IOException ex) { System.err.println(ex); } deleteCopied(copy_to); } public static void nonDirectBuffer(){ try ( FileChannel fileChannel_from = FileChannel.open(copy_from, EnumSet.of(StandardOpenOption.READ)); FileChannel fileChannel_to = FileChannel.open(copy_to, EnumSet.of(StandardOpenOption.CREATE_NEW, StandardOpenOption.WRITE));){ startTime = System.currentTimeMillis(); ByteBuffer bytebuffer = ByteBuffer.allocate(bufferSize); while ((fileChannel_from.read(bytebuffer)) > 0) { bytebuffer.flip(); fileChannel_to.write(bytebuffer); bytebuffer.clear(); } elapsedTime = System.currentTimeMillis() - startTime; System.out.println(\"nonDirectBuffer read/write Time is \" + elapsedTime + \" ms\"); }catch (IOException ex) { System.err.println(ex); } deleteCopied(copy_to); } public static void directBuffer(){ try ( FileChannel fileChannel_to = FileChannel.open(copy_to, EnumSet.of(StandardOpenOption.CREATE_NEW, StandardOpenOption.WRITE)); FileChannel fileChannel_from = (FileChannel.open(copy_from, EnumSet.of(StandardOpenOption.READ)));) { startTime = System.currentTimeMillis(); ByteBuffer bytebuffer = ByteBuffer.allocateDirect(bufferSize); while ((fileChannel_from.read(bytebuffer)) > 0) { bytebuffer.flip(); fileChannel_to.write(bytebuffer); bytebuffer.clear(); } elapsedTime = System.currentTimeMillis() - startTime; System.out.println(\"directBuffer read/write Time is \" + elapsedTime + \" ms\"); }catch (IOException ex) { System.err.println(ex); } deleteCopied(copy_to); } public static void deleteCopied(Path path){ try { Files.deleteIfExists(path); }catch (IOException ex) { System.err.println(ex); } } } 参考：通过零拷贝实现有效数据传输","tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://blog.yaodataking.com/tags/Kafka/"}]},{"title":"Kafka入门之十一:Kafka的监控","date":"2016-12-23T14:39:23.000Z","path":"2016/12/23/kafka-11/","text":"kafka的数据统计是通过一个叫metrics的工具进行收集的，metrics是一个java类库。metrics以JMX的形式提供了对外查看数据的接口，因此我们首先要在kafka启动的时候指定jmx的端口，然后通过可视化工具jconsole或kafka manager查看。下面我们分别介绍一下。1.JMX配置首先我们看JMX如何配置，在Kafka工具中有个脚本bin/kafka-run-class.sh定义了JMX的启动方法。 # JMX settings if [ -z &quot;$KAFKA_JMX_OPTS&quot; ]; then KAFKA_JMX_OPTS=&quot;-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false &quot; fi # JMX port to use if [ $JMX_PORT ]; then KAFKA_JMX_OPTS=&quot;$KAFKA_JMX_OPTS -Dcom.sun.management.jmxremote.port=$JMX_PORT &quot; fi `&lt;/pre&gt; 因此，我们只要在docker-compose文件中定义KAFKA_JMX_OPTS和JMX_PORT，那么启动docker同时，JMX自动启动。 &lt;pre&gt;` hostname: kafka0 ports: - &quot;19092:19092&quot; - &quot;29092:29092&quot; - &quot;18083:18083&quot; - &quot;12345:12345&quot; environment: ZOOKEEPER_CONNECT: zookeeper0:12181,zookeeper1:12182,zookeeper2:12183/kafka BROKER_ID: 0 LISTENERS: PLAINTEXT://kafka0:19092,SSL://kafka0:29092 ZOOKEEPER_SESSION_TIMEOUT: 3600000 CONNECT_REST_PORT: 18083 KAFKA_PROPERTY_AUTO_CREATE_TOPICS_ENABLE: &quot;false&quot; KAFKA_PROPERTY_SSL_CLIENT_AUTH: required JMX_PORT: 12345 KAFKA_JMX_OPTS: &quot;-Dcom.sun.management.jmxremote=true -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false&quot; `&lt;/pre&gt; 2.JConsole监控 我们知道java 内置了jconsole工具，本博客之前也有介绍过[jconsole](http://blog.yaodataking.com/2016/04/jconsole-remote-mycat.html),因此jconsole的使用并不陌生。我们可以通过docker inspect 查看某个broker的IP地址及jmx端口。然后使用命令进入,如下 &lt;pre&gt;`jconsole 172.19.0.6:12345`&lt;/pre&gt; [![2016-12-23_20-53-38](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/12/2016-12-23_20-53-38.png)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/12/2016-12-23_20-53-38.png) 通过查看Mbean标签下的参数，我们可以获取kafka的一些运行参数。 3.Kafka Manager监控 Kafka Manager是Yahoo构建的一个开源的基于Web的管理工具，可以简化开发者和运维工程师维护Kafka集群的工作。 kakfa manager的[GitHub ](https://github.com/yahoo/kafka-manager)地址。同样的我们创建一个kakfa manager的docker镜像,然后加入docker-compose文件。 &lt;pre&gt;` kafka-manager: build: context: . dockerfile: kafka-manager.Dockerfile image: kafka-manager:1.0 container_name: kafka-manager hostname: kafka-manager ports: - &quot;38080:38080&quot; environment: ZK_HOSTS: zookeeper0:12181 PORT: 38080 expose: - 38080 访问38080端口并加入一个Kafka集群。检查broker，我们还可以管理以下功能： 管理几个不同的集群； 检查集群的状态(topics, brokers, 副本的分布, 分区的分布)； 创建topics Preferred副本选举 重新分配分区 ps,kakfa监控还有一些工具比如Kafka Web Console，KafkaOffsetMonitor。感兴趣的朋友可以去测试。","tags":[{"name":"JConsole","slug":"JConsole","permalink":"http://blog.yaodataking.com/tags/JConsole/"},{"name":"Kafka Manager","slug":"Kafka-Manager","permalink":"http://blog.yaodataking.com/tags/Kafka-Manager/"}]},{"title":"Kafka入门之十:Kafka的SSL加密和认证","date":"2016-12-16T12:53:44.000Z","path":"2016/12/16/kafka-10/","text":"SSL(Secure Sockets Layer 安全套接层),及其继任者传输层安全（Transport Layer Security，TLS）是为网络通信提供安全及数据完整性的一种安全协议。TLS与SSL在传输层对网络连接进行加密。在SSL中使用密钥交换算法交换密钥；使用密钥对数据进行加密；使用散列算法对数据的完整性进行验证，使用数字证书证明自己的身份。下面我们就Kafka中如何实现及步骤介绍。 生成服务器keystore (密钥和证书) 生成客户端keystore (密钥和证书) 创建CA证书 将CA证书导入服务器truststore 将CA证书导入客户端truststore 从服务器keystore导出证书 用CA证书给服务器证书签名 将CA证书导入服务器keystore 将CA证书导入客户端keystore 将已签名服务器证书导入服务器keystore以下设置我们以kakfa服务器kafka0为例。1. 设置证书1.1 生成服务区Keystore keytool -keystore kafka0.keystore.jks -alias kafka0 -validity 365 -storepass test1234 -keypass test1234 -genkey`&lt;/pre&gt; keystore: 密钥仓库存储证书文件。 validity: 证书的有效时间，单位天 1.2 生成客户端Keystore &lt;pre&gt;`keytool -keystore client.keystore.jks -alias kafka0 -validity 365 -storepass test1234 -keypass test1234 -genkey`&lt;/pre&gt; 1.3 创建CA证书 &lt;pre&gt;`openssl req -new -x509 -keyout ca.key -out ca.crt -days 365 -passout pass:test1234`&lt;/pre&gt; 1.4 将CA证书导入服务器truststore &lt;pre&gt;&lt;/code&gt;keytool -v -keystore kafka0.truststore.jks -alias CARoot -import -file ca.crt -storepass test1234&lt;/code&gt;&lt;/pre&gt; 1.5 将CA证书导入客户端truststore &lt;pre&gt;`keytool -v -keystore client.truststore.jks -alias CARoot -import -file ca.crt -storepass test1234`&lt;/pre&gt; 1.6 从服务器keystore导出证书 &lt;pre&gt;`keytool -keystore kafka0.keystore.jks -alias kafka0 -certreq -file kafka0.crt -storepass test1234`&lt;/pre&gt; 1.7 用CA证书给服务器证书签名 &lt;pre&gt;`openssl x509 -req -CA ca.crt -CAkey ca.key -in kafka0.crt -out kafka0-signed.crt -days 365 -CAcreateserial -passin pass:test1234`&lt;/pre&gt; 1.8 将CA证书导入服务器keystore &lt;pre&gt;`keytool -keystore kafka0.keystore.jks -alias CARoot -import -file ca.crt -storepass test1234`&lt;/pre&gt; 1.9 将CA证书导入客户端keystore &lt;pre&gt;`keytool -keystore client.keystore.jks -alias CARoot -import -file ca.crt -storepass test1234`&lt;/pre&gt; 1.10 将已签名服务器证书导入服务器keystore &lt;pre&gt;`keytool -keystore kafka0.keystore.jks -alias kafka0 -import -file kafka0-signed.crt -storepass test1234`&lt;/pre&gt; 2.配置kafka broker 配置server.perproties &lt;pre&gt;`listeners=PLAINTEXT://kafka0:9092,SSL://kafka0:9093 ssl.client.auth=required ssl.keystore.location=/opt/kafka/kafka0.keystore.jks ssl.keystore.password=test1234 ssl.key.password=test1234 ssl.truststore.location=/opt/kafka0.truststore.jks ssl.truststore.password=test1234`&lt;/pre&gt; “required”=&gt;客户端身份验证是必需的，“requested”=&gt;客户端身份验证请求，客户端没有证书仍然可以连接。 3.配置kafka客户端 配置client.perproties &lt;pre&gt;`security.protocol=SSL ssl.keystore.location=/opt/kafka/client.keystore.jks ssl.keystore.password=test1234 ssl.key.password=test1234 ssl.truststore.location=/opt/kafka/client.truststore.jks ssl.truststore.password=test1234`&lt;/pre&gt; 4.验证ssl是否生效 &lt;pre&gt;`openssl s_client -debug -connect kafka0:9093 -tls1 如果证书没有出现或者有任何其他错误信息，那么你的keystore设置不正确。","tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://blog.yaodataking.com/tags/Kafka/"}]},{"title":"Kafka入门之九:Kafka Streams","date":"2016-12-09T15:12:43.000Z","path":"2016/12/09/kafka-9/","text":"1. 概述Kafka Streams是一个客户端程序库，用于处理和分析存储在Kafka中的数据，并将得到的数据写入kafka或发送到外部系统。Kafka Stream中有几个重要的流处理概念：Event time和Process Time、窗口函数、应用状态管理。Kafka Stream的门槛非常低：比如单机进行一些小数据量的功能验证而不需要在其他机器上启动一些服务（比如在Storm运行Topology需要启动Nimbus和Supervisor，当然也支持Local Mode），Kafka Stream的并发模型可以对单应用多实例进行负载均衡。2. 主要特点 轻量的嵌入到Java应用中 除了Kafka Stream Client lib以外无外部依赖 支持本地状态故障转移，以实现非常高效的有状态操作，如join和window函数 低延迟消息处理，支持基于event-time的window操作 提供必要的流处理原语、hige-level Stream DSL和low-level Processor API3. 开发者指南3.1 核心概念3.1.1 Stream Processing Topology stream是Kafka Stream最重要的抽象，它代表了一个无限持续的数据集。stream是有序的、可重放消息、对不可变数据集支持故障转移 一个stream processing application由一到多个processor topologies组成，其中每个processor topology是一张图，由多个streams（edges）连接着多个stream processor（node） 一个stream processor是processor topology中的一个节点，它代表一个在stream中的处理步骤：从上游processors接受数据、进行一些处理、最后发送一到多条数据到下游processorsKafka Stream提供两种开发stream processing topology的API high-level Stream DSL：提供通用的数据操作，如map和fileter lower-level Processor API：提供定义和连接自定义processor，同时跟state store（下文会介绍）交互 3.1.2 Time在流处理中时间是一个比较重要的概念，比如说在窗口（windows）处理中，时间就代表两个处理边界 Event time：一条消息最初产生/创建的时间点 Processing time：消息准备被应用处理的时间点，如kafka消费某条消息的时间，processing time的单位可以是毫秒、小时或天。Processing time晚于Event time. Ingestion Time 消息存入Topic/Partition时的时间Kafka Stream 使用TimestampExtractor 接口为每个消息分配一个timestamp，具体的实现可以是从消息中的某个时间字段获取timestamp以提供event-time的语义或者返回处理时的时钟时间，从而将processing-time的语义留给开发者的处理程序。开发者甚至可以强制使用其他不同的时间概念来进行定义event-time和processing time。3.1.3 States In-memory State Store Persistent State StoreKafka Stream使用state stores提供基于stream的数据存储和数据查询，Kafka Stream内嵌了多个state store，可以通过API访问到，这些state store的实现可以是持久化的KV存储引擎、内存HashMap或者其他数据结构。Kafka Stream提供了local state store的故障转移和自动发现。3.1.4 Windows Hopping time windows Tumbling time windows:Hopping time windows的特例 Sliding windows:只用于Join操作，可由JoinWindow类指定3.2 KStream vs KTable3.2.1 概念Kafka Stream定义了两种基本抽象：KStream 和 KTable，区别来自于key-value对值如何被解释，在一个流中(KStream)，每个key-value是一个独立的信息片断，比如，用户购买流是：alice-&gt;黄油，bob-&gt;面包，alice-&gt;奶酪面包，我们知道alice既买了黄油，又买了奶酪面包。另一方面，对于一个表table( KTable)，是代表一个变化日志，如果表包含两对同样key的key-value值，后者会覆盖前面的记录，因为key值一样的，比如用户地址表：alice -&gt; 纽约, bob -&gt; 旧金山, alice -&gt; 芝加哥，意味着Alice从纽约迁移到芝加哥，而不是同时居住在两个地方。这两个概念之间有一个二元性，一个流能被看成表，而一个表也可以看成流。 KStream:KStream为数据流，每条消息代表一条不可变的新记录 KTable:KTable为change log流，每条消息代表一个更新，几条key相同的消息会将该key的值更新为最后一条消息的值3.2.2 Example对于KStream和KTable中插入两条消息 (“key1”, 1), (“key1”, 2)对KStream作sum，结果为(“key1”,3)对KTable作sum，结果为(“key1”,2)3.2.3 几种JoinKStream和KStream的Join，适用于Window Join，结果为KStream。KStream和KTable的Join，KTable的变化只影响KStream中新数据，新结果的输出由KStream驱动，输出为KStream。KTable和KTable的Join，类似于RDBMS的Join，结果为KTable。3.3 Stream API3.3.1 Low-level processor API通过实现Processor接口并实现process和punctuate方法，每条消息都会调用process方法，punctuate方法会周期性的被调用。使用TopologyBuilder拼装processor。3.3.2 High-level DSL API使用Stream DSL创建processor topology，开发者可以使用KStreamBuilder类，继承自TopologyBuilder。4.一个示例在PurchaseAnalysis.java的基础上，算出不同地区（用户地址），不同性别的订单数及商品总数和总金额。输出结果schema如下地区（用户地区，如SH），性别，订单总数，商品总数，总金额示例输出SH, male, 3, 4, 188888.88BJ, femail, 5, 8, 288888.88首先我们要定义OrderAddressGenderSum类， private String userAddress; private String gender; private int orderCount; private int itemSum; private double orderAmount;增加fromOrderUserItem及add方法。 public static OrderAddressGenderSum fromOrderUserItem(OrderUserItem orderUserItem) { OrderAddressGenderSum orderAddressGenderSum = new OrderAddressGenderSum(); if(orderUserItem == null) { return orderAddressGenderSum; } orderAddressGenderSum.userAddress = orderUserItem.userAddress; orderAddressGenderSum.gender = orderUserItem.gender; orderAddressGenderSum.orderCount = 1; orderAddressGenderSum.itemSum = orderUserItem.quantity; orderAddressGenderSum.orderAmount = orderUserItem.quantity * orderUserItem.itemPrice; return orderAddressGenderSum; } public static OrderAddressGenderSum add(OrderAddressGenderSum v1, OrderAddressGenderSum v2) { OrderAddressGenderSum orderAddressGenderSum = new OrderAddressGenderSum(); orderAddressGenderSum.userAddress = v1.userAddress; orderAddressGenderSum.gender = v1.gender; orderAddressGenderSum.orderCount = v1.orderCount + v2.orderCount; orderAddressGenderSum.itemSum = v1.itemSum + v2.itemSum; orderAddressGenderSum.orderAmount = v1.orderAmount + v2.orderAmount; return orderAddressGenderSum; }定义一个新的streamBuilder，Order使用KStream流，User，Item使用KTable,使用join关联。主要代码如下。 KStreamBuilder streamBuilder = new KStreamBuilder(); KStream orderStream = streamBuilder.stream(Serdes.String(), SerdesFactory.serdFrom(Order.class), “orders”); KTable userTable = streamBuilder.table(Serdes.String(), SerdesFactory.serdFrom(User.class), “users”, “users-state-store”); KTable itemTable = streamBuilder.table(Serdes.String(), SerdesFactory.serdFrom(Item.class), “items”, “items-state-store”); KTable kTable = orderStream .leftJoin(userTable, (Order order, User user) -&gt; OrderUser.fromOrderUser(order, user), Serdes.String(), SerdesFactory.serdFrom(Order.class)) .filter((String userName, OrderUser orderUser) -&gt; orderUser.userAddress != null) .map((String userName, OrderUser orderUser) -&gt; new KeyValue(orderUser.itemName, orderUser)) .through(Serdes.String(), SerdesFactory.serdFrom(OrderUser.class), (String key, OrderUser orderUser, int numPartitions) -&gt; (orderUser.getItemName().hashCode() &amp; 0x7FFFFFFF) % numPartitions, “orderuser-repartition-by-item”) .leftJoin(itemTable, (OrderUser orderUser, Item item) -&gt;OrderUserItem.fromOrderUser(orderUser, item), Serdes.String(),SerdesFactory.serdFrom(OrderUser.class)) .map((String item, OrderUserItem orderUserItem) -&gt; KeyValue.pair(orderUserItem.userAddress + orderUserItem.gender,OrderAddressGenderSum.fromOrderUserItem(orderUserItem))) .groupByKey(Serdes.String(), SerdesFactory.serdFrom(OrderAddressGenderSum.class)) .reduce((OrderAddressGenderSum v1, OrderAddressGenderSum v2) -&gt; OrderAddressGenderSum.add(v1, v2),”gender-amount-state-store”); kTable.foreach((key, orderAddressGenderSum) -&gt; System.out.printf(“%s\\n”, orderAddressGenderSum.toString())); kTable .toStream() .map((String key, OrderAddressGenderSum orderAddressGenderSum) -&gt; new KeyValue(key,orderAddressGenderSum.printSelf())) .to(“gender-amount”); KafkaStreams kafkaStreams = new KafkaStreams(streamBuilder, props); kafkaStreams.cleanUp(); kafkaStreams.start(); System.in.read(); kafkaStreams.close(); kafkaStreams.cleanUp();最终结果如下。注：此示例基于harben的GitHub上的Purchase Analysis示例修改。","tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://blog.yaodataking.com/tags/Kafka/"}]},{"title":"Kafka入门之八:Kafka的新API","date":"2016-12-03T05:10:57.000Z","path":"2016/12/03/kafka-8/","text":"前面几节我们讲的Kafka都是基于0.8.2.2的版本，截止到今天，kafka实际上已经更新到0.10.1.0，那么API都有哪些变化呢？1 Producer API在Kafka 0.8.2, Producer已经被重新设计，所以这次变化较少。1.1增加JAVA接口的发送回调（原来只支持SCALA接口）异步发送消息到一个主题，然后调用提供的callback，发送确认结果。 producer.send(record, new Callback() { @Override public void onCompletion(RecordMetadata metadata, Exception exception) { System.out.printf(\"Send record partition:%d, offset:%d, keysize:%d, valuesize:%d %n\", metadata.partition(), metadata.offset(), metadata.serializedKeySize(), metadata.serializedValueSize()); } }); 1.2重构Partitioner接口原来0.8.2.2的接口是这样的，只有两个参数 public int partition(Object key, int numPartitions) 现在0.10.1.0的接口是这样的，有六个参数。 public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) **2 Consumer API** 在最新的版本中，Consumer API不再区分High Level还是Low Level。 2.1 重构Consumer包 把kafka.consumer和kafka.javaapi统一到kafka.clients.consumer，使包更加统一。 2.2 新增Subscribe和Assign接口 Subscribe实际实现了原High Level功能，Assign实现了原Low Level功能。 2.2.1 Subscribe Subscribe通过ConsumerRebalanceListener来监听和动态分配。通过subscribe(List, ConsumerRebalanceListener)来订阅主题列表，或者通过subscribe(Pattern, ConsumerRebalanceListener)来订阅匹配特定模式的主题。 所以，如果一个主题有4个分区，并且一个消费者组有2个进程，那么每个进程将从2个分区来进行消费，如果一个进程故障，分区将重新分派到同组的其他的进程。如果有新的进程加入该组，分区将现有消费者移动到新的进程。具体来说，如果2个进程订阅了一个主题，指定不同的组，他们将获取这个主题所有的消息，如果他们指定相同的组，那么它们将每个获取大约一半的消息。 2.2.2 Assign 如果我们使用Assign接口，那么将不会使用消费者组,也将禁用动态分区分配.下面的代码演示了直接消费parttion 0和1的消息，不管有多少个进程，消费的消息都是一样的。 consumer.assign(Arrays.asList(new TopicPartition(topic, 0), new TopicPartition(topic, 1))); while (true) { ConsumerRecords records = consumer.poll(100); records.forEach(record -> { System.out.printf(\"client : %s , topic: %s , partition: %d , offset = %d, key = %s, value = %s%n\", clientid, record.topic(), record.partition(), record.offset(), record.key(), record.value()); }); } 2.3 commit功能 除了保持原来自动commit和手动commit的功能外，kafka增加了两个功能。 1）支持同步和异步的commit并支持commit回调 这是0.8.2.2的手动commit。 consumerConnector.commitOffsets(); 在0.10.1.0中手动同步commit。 consumer.commitSync(); 在0.10.1.0中手动异步commit并回调。 consumer.commitAsync(new OffsetCommitCallback() { @Override public void onComplete(Map offsets, Exception exception) { } }); 2）支持手动commit特定的partition的offset consumer.subscribe(Arrays.asList(topic)); AtomicLong atomicLong = new AtomicLong(); while (true) { ConsumerRecords records = consumer.poll(100); records.partitions().forEach(topicPartition -> { List partitionRecords = records.records(topicPartition); partitionRecords.forEach(record -> { System.out.printf(\"client : %s , topic: %s , partition: %d , offset = %d, key = %s, value = %s%n\", clientid, record.topic(), record.partition(), record.offset(), record.key(), record.value()); }); long lastOffset = partitionRecords.get(partitionRecords.size() - 1).offset(); consumer.commitSync(Collections.singletonMap(topicPartition, new OffsetAndMetadata(lastOffset + 1))); }); } { System.out.printf(\"client : %s , topic: %s , partition: %d , offset = %d, key = %s, value = %s%n\", clientid, record.topic(), record.partition(), record.offset(), record.key(), record.value()); }); } 2.6 多线程处理模型Kafka的Consumer的接口为非线程安全的。多线程共用IO，Consumer线程需要自己做好线程同步。如果想立即终止consumer，唯一办法是用调用接口：wakeup()，使处理线程产生WakeupException。参见官方文档 public class KafkaConsumerRunner implements Runnable { private final AtomicBoolean closed = new AtomicBoolean(false); private final KafkaConsumer consumer; public void run() { try { consumer.subscribe(Arrays.asList(&quot;topic&quot;)); while (!closed.get()) { ConsumerRecords records = consumer.poll(10000); // Handle new records } } catch (WakeupException e) { // Ignore exception if closing if (!closed.get()) throw e; } finally { consumer.close(); } } // Shutdown hook which can be called from a separate thread public void shutdown() { closed.set(true); consumer.wakeup(); } }如果用以下的方式开启多个线程是禁止的。 Thread[] consumerThreads = new Thread[2]; for (int i = 0; i &lt; consumerThreads.length; ++i) { consumerThreads[i] = new Thread(runnable); consumerThreads[i].start(); }","tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://blog.yaodataking.com/tags/Kafka/"}]},{"title":"软件架构师的12项修炼","date":"2016-12-01T14:49:23.000Z","path":"2016/12/01/soft-skill/","text":"最近重读了Dave Hendricksen的《软件架构师的12项修炼》，感觉在提高自己的软技能方面还是有所用的，特别是沟通，协商，领导力等关系技能上，其实这些技能对于每个人都是适用的。作者把这些技能看作一个金字塔，如下图。以下是简单的技能说明。关系技能：文雅的举止（在任何环境下都能与人文雅相处的能力）沟通（与人有效交互的能力）协商（将事情办成的能力）领导力（通过施加影响力将事情办成的能力）政治（“政治场合”与人交互的能力）个人技能：透明化（使自我、团队关系和项目透明化的能力）激情（激发和保护激情的能力）语境切换（将注意力迅速切换到新语境并保持专注的能力）商务技能：商务知识（读懂商务语言的能力和了解产品与顾客的能力）创新（如何通过学习和思考来创新的能力）实用主义（抓住关键问题的能力）认知（认知目标、战略及合作伙伴的能力） 对于每个具体的技能，作者详细列出了具体的步骤，也给出了实现途径中的一些忠告。对某些软技能欠缺的朋友不妨参考一下。附：豆瓣对此书的介绍和评价的链接。","tags":[]},{"title":"《罗马人的故事15:罗马世界的终曲》读书笔记","date":"2016-11-30T13:24:10.000Z","path":"2016/11/30/rome-story-15/","text":"尤里安之后，罗马皇帝基本上都是君权神授了。虽然皇帝的位置坐稳了，但作为皇帝的“三大职责”，保障边境安全，维护国内政治秩序，完善基础设施建设，完全交给别人了。斯提利科，被称为最后的罗马人，以一人之躯肩负起帝国的重任，最终落到“记录抹杀刑”的结局，实在是令人唏嘘。罗马统治下的和平早已不再发挥作用，八百年未曾陷落的、长久以来被赞颂为“世界之都”的罗马城于公元410年遭到了浩劫，幸存的人下决心离弃罗马，帝国的国境早已千疮百孔。公元476年，随着少年皇帝罗慕路斯·奥古斯都的退位，西罗马帝国灭亡了。没有蛮族进攻和激烈的战斗，没有火焰，没有惨叫，无声无息，无人注意到她的消失。小西庇阿在毁灭迦太基城后失声痛哭，当旁人问及原因时，他回答道：“这曾经是一个伟大的民族，拥有着辽阔的领地、统治着海洋，在最危急的时刻比那些庞大的帝国表现了更刚毅、勇敢的精神，但仍避免不了灭亡。想想过去的亚述帝国、波斯帝国、马其顿帝国还有那个高傲的特洛伊，又有哪个能避免这样的结局。我真害怕在将来有人会对我的祖国做出同样的事。” 小西庇阿的预言注定成真了。记得有人总结过，我觉得很有道理。 如果把视野拉长到三年以上，你就能隐约感受到经济的周期波动。 如果把视野拉长到十年到三十年，你就能看到人口年龄结构的变化、技术进步的影响、社会风气的演变、经济发展阶段的跃升。 如果把视野拉长到五十年到一百年，你能看到国家的兴衰、世界政治经济格局的调整、战争与和平的更迭。 如果把视野拉长到数百年到数千年，你能看到文化的形成和沉淀、宗教的兴起和衰落。 如果把视野拉到数万年、数十万年乃至数百万年，你就能看到进化的脉络、气候的轮回。罗马帝国为什么衰亡？众说不一。说基督教的原因我认为是不确切的，基督教应该是罗马帝国衰落的果，不是因。说蛮族的入侵也是不确切的，蛮族入侵的威胁一直是存在的，但是力量此消彼涨，一旦罗马军力衰落，蛮族的入侵就源源不断。我们回顾罗马历史知道，罗马的疆域主要是在罗马共和国扩张时期形成的，转成帝制的主要原因就是容易提高效率统一力量保卫疆土。然而帝国的弊端也是显而易见的，罗马历史上就产生好多臭名昭著的皇帝。因此从罗马帝国的兴盛开始，我们就应该知道衰亡也是必然的，正所谓，成也萧何，败也萧何。","tags":[{"name":"罗马","slug":"罗马","permalink":"http://blog.yaodataking.com/tags/罗马/"}]},{"title":"Kafka入门之七:Kafka的offset管理","date":"2016-11-26T05:01:50.000Z","path":"2016/11/26/kafka-7/","text":"1.Low Level Consumer(Simple Consumer)的offset管理上一节我们讲到了kafka的High Level Consumer的消息消费是自动根据offset的顺序消费的。但有时候用户希望比Consumer Group更好的控制数据的消费，比如&bull;同一条消息读多次，方便Replay。&bull;只消费某个Topic的部分Partition。&bull;管理事务，从而确保每条消息被处理一次。kafka提供了kafka.javaapi.consumer.SimpleConsumer这个API,但是相比High Level Consumer，Low Level Consumer要求用户做大量的额外工作，如&bull;在应用程序中跟踪处理offset，并决定下一条消费哪条消息。&bull;获知每个Partition的Leader。&bull;处理Leader的变化。&bull;处理多Consumer的协作。下面一段代码，演示了每次从特定Partition的特定offset开始fetch特定大小的消息，如果这些值固定，返回的消息是固定的。 import java.nio.ByteBuffer; import kafka.api.FetchRequest; import kafka.api.FetchRequestBuilder; import kafka.javaapi.FetchResponse; import kafka.javaapi.consumer.SimpleConsumer; import kafka.javaapi.message.ByteBufferMessageSet; import kafka.message.MessageAndOffset; public class DemoLowLevelConsumer { public static void main(String[] args) throws Exception { final String topic = &quot;topic1&quot;; String clientID = &quot;DemoLowLevelConsumer1&quot;; SimpleConsumer simpleConsumer = new SimpleConsumer(&quot;kafka0&quot;, 9092, 100000, 64 * 1000000, clientID); FetchRequest req = new FetchRequestBuilder().clientId(clientID) .addFetch(topic, 0, 0L, 50000).addFetch(topic, 1, 0L, 50000).addFetch(topic, 2, 0L, 50000).build(); FetchResponse fetchResponse = simpleConsumer.fetch(req); ByteBufferMessageSet messageSet = (ByteBufferMessageSet) fetchResponse.messageSet(topic, 0); for (MessageAndOffset messageAndOffset : messageSet) { ByteBuffer payload = messageAndOffset.message().payload(); long offset = messageAndOffset.offset(); byte[] bytes = new byte[payload.limit()]; payload.get(bytes); System.out.println(&quot;Offset:&quot; + offset + &quot;, Payload:&quot; + new String(bytes, &quot;UTF-8&quot;)); } } }`&lt;/pre&gt; **2.High Level Consumer的offset管理** 对于High Level Consumer来说，offset是自动管理的，我们只需要在参数里设置自动commit还是手工commit。 &lt;pre&gt;`auto.commit.enable=true auto.commit.interval.ms=60 * 1000 `&lt;/pre&gt; 如果auto.commit.enable=false，那么我们就要在程序中手工指定何时执行下面这条语句。 &lt;pre&gt;`ConsumerConnector.commitOffsets();`&lt;/pre&gt; **3.Offset的存储** kafka通过参数设置可以指定offset存储的位置，在zookeeper里还是在kafka上。当然要把dual.commit.enabled设置为true。下面的设置表示offset存储在Kafka上。 &lt;pre&gt;`offsets.storage=kafka dual.commit.enabled=true`&lt;/pre&gt; **4.Offse的范围查询** kafka提供了一个工具可以查询指定topic的offset范围。如查询topic1的offset最小值和最大值。 最小值 &lt;pre&gt;`bin/kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list kafka0:9092 -topic topic1 --time -2`&lt;/pre&gt; 最大值 &lt;pre&gt;`bin/kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list kafka0:9092 -topic topic1 --time -1`&lt;/pre&gt; 当然通过这个工具,我们还可获取指定timestamp的offset，具体用法参见此命令help。 **5.日志压缩** 5.1原理 为什么日志压缩放在这里讲呢？因为kafka的日志压缩跟offset有关，启动压缩机制后，kafka只保留每一Key的最大的offset（也就是最新值)，而把旧的值在压缩过程中删除掉。如下图。 [![2016-11-26_12-15-07](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/11/2016-11-26_12-15-07.png)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/11/2016-11-26_12-15-07.png) 5.2配置 为实现日志压缩，我们必须修改Kafka配置文件server.properties，把log.cleaner.enable设置为true，然后重启kafka。 5.3实验演示 5.3.1目标 创建一个topic，将其log.cleanup.policy设置为compact，等clean（compact）过后使用consumer消费该topic，打印出每条消息的partition key offset，观察其offset是否连续. 5.3.2步骤 a)创建topic1，注意这个topic里加了两个配置cleanup.policy=compact及segment.bytes=512，这将会对这个topic启动日志压缩,并且分段文件达到512字节就轮转。 &lt;pre&gt;`bin/kafka-topics.sh --zookeeper zookeeper0:2181/kafka --topic topic1 --create --config cleanup.policy=compact --config segment.bytes=512 --replication-factor 1 --partitions 3 b)发送消息，我们连续10次发送key为0，1，2的消息。（确保超过512字节）c)消费消息，停止消费，再连续3次发送，再消费消息。看下图查看kafka上的log目录，看到生成了以第一个offset的不同log，旧的log在删除过程中。查看log文件的内容，看到压缩过的log只保留了最后一个offset。5.3.3实验小结&bull;一直保持消费Log head的consumer可按顺序消费所有消息，并且offset连续。&bull;任何从offset 0开始的读操作至少可读到每个key对应的最后一条消息。&bull;每条消息的offset保持不变，offset是消息的永久标志符。&bull;消费本身的顺序不会被改变。","tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://blog.yaodataking.com/tags/Kafka/"}]},{"title":"Oracle的函数及位图索引","date":"2016-11-19T06:29:19.000Z","path":"2016/11/19/oracle-index-2/","text":"1.函数索引1）函数索引的原理：使用基于函数的索引可建立函数返回值的索引。服务器根据索引表达式执行搜索时，不必为每个关键字值调用函数，该函数索引相当于表的一个虚拟列，虽然不真正存于表中，但存于索引结构中。这个函数可以是一个内置 SQL 函数、外部提供的 PL/SQL 函数、或用户编写函数。2）函数索引的好处：a.利用函数索引可以只对限定的键值创建索引，节约空间，提高效率。b.利用函数索引可以避免where子句中使用函数时的全表扫描，提高查询效率。c.使不走索引的SQL使用索引d.减少递归调用3）函数索引对性能的影响当然这会对插入数据有一定影响，因为需要通过函数计算一下，然后生成索引。如果插入数据量不大，而查询数据量比较大。那么为了优化查询速度，稍微降低点插入速度是可以承担的。 2.位图索引1）什么是位图索引位图索引是指用位图表示的索引，oracle对于选择度底的列的每个键值建立一个位图，位图中的每一位可能对应多个列，位图中位等于1表示特定的行含有此位图表示的键值。2）位图索引的优势：a.减少即席查询的相应时间b.和其它类型索引比较，真正节约了索引数据空间c.即使在非常差的硬件上，也可能会有戏剧化的性能提升d.高效的并行DML和LOAD操作。e.生成索引的时候更高效，首先是不排序，其次是占用的空间少（索引空间）。f.可以通过位图索引直接计数。3）位图索引不适用场景：a.列的基数比较多，不适合位图索引，因为它会占用更多的存储空间。b.索引列DML频繁的列，不适合位图索引，容易造成死锁。 3.反向键索引1）什么是反向键索引反向键索引也是一种B树索引，但是它与一般的B树索引相比又有一个很奇特的地方。反向键索引将索引键值的每一个字节做一个翻转变换，举一个例子：数字123456在反向键索引中的存储形式便是654321。好处是消除了热块竞争。坏处是范围查询根本无法使用!2）反向键索引的应用场景反向索引主要是建立在那些以序列号生成的列上，可以将本来是连在一起的index entry分散到不同的leaf block中去，当索引是从序列中取的时候，如果是一般的b-tree 索引，在大量的插入后会导致块的分裂以及树的倾斜，使用reverse key index可以使索引段条目被更均匀的分布，所以，reverse index主要是缓解右向增长的索引右侧叶子节点的争用,因此常用于解决被索引引起的热块问题。 4.全文索引1)什么是全文索引全文索引就是通过将文字按照某种语言进行词汇拆分，重新将数据组合存储，来达到快速检索的目的。全文索引不是按照键值存储的，而是按照分词重组数据，常用于模糊查询Where name like ‘%leonarding%’效率比全表扫描高很多。2）全文索引的典型应用场景a.普通查询，创建OracleText中Context类型的索引，生成大量的关键词，用于加快类似于普通的like ‘%xx%’操作速度，或者查询一些比较大的文档。可以使用contains函数进行数据检索。缺点：比较依赖于关键词和文档格式。有时可能不太准确。b.目录信息Ctxcat(context catelog) 索引为事务维护，也就是说更改数据的时候会更新索引，不需要像context类型的全文索引一样每次更改数据后执行CTX_DDL.SYNC_INDEX刷新操作。c.文档分类，创建规则表，根据关键词，对文档进行分类操作。比较适合对比较大的文档进行分类的操作。索引类型：ctxsys.ctxrule用于将文档分类的应用","tags":[{"name":"Oracle","slug":"Oracle","permalink":"http://blog.yaodataking.com/tags/Oracle/"}]},{"title":"《罗马人的故事14:基督的胜利》读书笔记","date":"2016-11-14T03:00:17.000Z","path":"2016/11/14/rome-story-14/","text":"公元356年，24岁的副帝尤里安开始了他在高卢的战斗生活。令人意想不到的是，作为一名哲学门徒，尤里安犹如凯撒转世，不仅成功瓦解了莱茵河沿岸蛮族的入侵，而且赢得了罗马军队的拥护，罗马人的精神似乎又回来了。历史也给了尤里安一次绝佳的机会，5年后，皇帝君士坦提乌斯突然因病逝世，尤里安顺理成章地成为了罗马帝国唯一的最高统治者。然而尤里安终究不是凯撒，对基督的不正确处理导致他无法被当时社会接受，尤里安的死应该也是一次暗杀。我在想如果是凯撒，他会怎么面对基督的崛起呢？大概也很难吧，先接受再利用应该是正确的政治态度吧。总的来说，如果不能引领及顺应这个时代，个人的成就无法脱离他所处的时代。让我们用尤里安的遗言作为罗马帝国的谢幕吧。 告别人生的时刻好像到了。我一直希望能回报养育我的大自然，现在我做到了，备感欣慰。哲学说，生为苦，死为解脱，因此是快乐。哲学还告诉我，死是神明赐给在现世建下功德之人的最后的奖励。对于迄今为止的所为，我没有任何后悔。我从未有过谋杀及其他卑劣之举，这让我感到欣慰。无论是与世隔绝的时期，还是日后集大权于一身，我始终忠实于自己，没有背叛自己的信念。我尽力顺应神明之期待而活，施善政兴利安民。遇战争，事前必深思熟虑，不得已才为之。尽管如此，结果未必尽如人意。像人间诸事，结果良善是神明之援助，欠佳则归咎于人之过失。我坚信帝国存在之意义，在于保证人民的安全与繁荣，并为之付出努力。我可以问心无愧地断言，为政后我所推行的一切政策，皆是为了实现这个目的。 说到这里，尤里安长长地吸了一口气，然后继续说道： 不能再多说了，我精力不济，感到死亡即将降临。唯有一事须作最后的交代。关于继任皇帝的人选，我不作提名。我的抉择可能考虑不够周延，或不够明智。若无法获得军队的支持，我的推荐会危及他的性命。人选委由诸位决定。我仅祝福罗马帝国的百姓，能在贤明的君主的统治下，过上安全幸福的生活。","tags":[{"name":"罗马","slug":"罗马","permalink":"http://blog.yaodataking.com/tags/罗马/"}]},{"title":"Kafka入门之六:Kafka的Consumer","date":"2016-11-13T01:46:44.000Z","path":"2016/11/13/kafka-6/","text":"1. 几个概念1.1 High Level Consumer 很多时候，客户程序只是希望从Kafka读取数据，不太关心消息offset的处理。同时也希望提供一些语义，例如同一条消息只被某一个Consumer消费（单播）或被所有Consumer消费（广播）。因此，Kafka Hight Level Consumer提供了一个从Kafka消费数据的高层抽象，从而屏蔽掉其中的细节并提供丰富的语义。1.2 Consumer Group High Level Consumer将从某个Partition读取的最后一条消息的offset存于Zookeeper中(Kafka从0.8.2版本开始同时支持将offset存于Zookeeper中与将offset存于专用的Kafka Topic中)。这个offset基于客户程序提供给Kafka的名字来保存，这个名字被称为Consumer Group。Consumer Group是整个Kafka集群全局的，而非某个Topic的。每一个High Level Consumer实例都属于一个Consumer Group，若不指定则属于默认的Group。1.3 Consumer Rebalance(基于Hight Level Consumer API) Kafka保证同一Consumer Group中只有一个Consumer会消费某条消息，实际上，Kafka保证的是稳定状态下每一个Consumer实例只会消费某一个或多个特定Partition的数据，而某个Partition的数据只会被某一个特定的Consumer实例所消费。也就是说Kafka对消息的分配是以Partition为单位分配的，而非以每一条消息作为分配单元。这样设计的劣势是无法保证同一个Consumer Group里的Consumer均匀消费数据，优势是每个Consumer不用都跟大量的Broker通信，减少通信开销，同时也降低了分配难度，实现也更简单。另外，因为同一个Partition里的数据是有序的，这种设计可以保证每个Partition里的数据可以被有序消费。 如果某Consumer Group中Consumer（每个Consumer只创建1个MessageStream）数量少于Partition数量，则至少有一个Consumer会消费多个Partition的数据，如果Consumer的数量与Partition数量相同，则正好一个Consumer消费一个Partition的数据。而如果Consumer的数量多于Partition的数量时，会有部分Consumer无法消费该Topic下任何一条消息。2. 特点2.1 Kafka Hight Level Consumer API的特点：1) 如果Consumer Group中的consumer线程数量比partition多，那么有的线程将永远不会收到消息。因为kafka的设计是在一个partition上是不允许并发的，所以consumer数不要大于partition数。2) 如果Consumer Group中的consumer线程数量比partition少，那么有的线程将会收到多个消息。并且不保证数据间的顺序性，kafka只保证在一个partition上数据是有序的。3) 增减consumer，broker，partition会导致rebalance，所以rebalance后consumer对应的partition会发生变化。4) High-level接口中获取不到数据的时候是会block的.2.2 关于Consumer Group（基于Hight Level Consumer API）的特点：1) 以Consumer Group为单位订阅topic，每个consumer一起去消费一个topic；2) Consumer Group 通过zookeeper来消费kafka集群中的消息(这个过程由zookeeper进行管理)相对于Low Level API自己管理offset，Hight Level API把offset的管理交给了zookeeper，但是Hight Level API并不是消费一次就在zookeeper中更新一次，而是每间隔一个(默认1000ms)时间更新一次offset，可能在重启消费者时拿到重复的消息。此外，当分区leader发生变更时也可能拿到重复的消息。因此在关闭消费者时最好等待一定时间（10s）然后再shutdown。3) Consumer Group 设计的目的之一也是为了应用多线程同时去消费一个topic中的数据。3.实验3.1 Kafka Consumer Group的特性创建一个Topic (名为topic1)，再创建一个属于group2的Consumer实例，并创建三个属于group1的Consumer实例，然后通过Producer向topic1发送Key分别为6，7，8，9，10，11六条消息。结果发现属于group2的Consumer收到了所有的这三条消息，同时group1中的3个Consumer分别收到了这六条消息，如下图所示。3.2 Consumer Rebalance实验1) 如果topic1有0，1，2共三个Partition，当group1只有一个Consumer(名为Consumer1)时，该 Consumer可消费这3个Partition的所有数据。2) 增加一个Consumer(Consumer2)后，其中一个Consumer（Consumer1）可消费2个Partition的数据（Partition 0和Partition 1），另外一个Consumer(Consumer2)可消费另外一个Partition（Partition 2）的数据。3) 再增加一个Consumer(Consumer3)后，每个Consumer可消费一个Partition的数据。Consumer1消费partition0，Consumer2消费partition1，Consumer3消费partition2。4) 再增加一个Consumer(Consumer4)后，其中3个Consumer可分别消费一个Partition的数据，另外一个Consumer(Consumer4)不能消费topic1的任何数据。5) 此时关闭Consumer1，其余3个Consumer可分别消费一个Partition的数据。6) 接着关闭Consumer2，Consumer3可消费2个Partition，Consumer4可消费1个Partition。7) 再关闭Consumer3，仅存的Consumer4可同时消费topic1的3个Partition。 小结，Consumer Rebalance的算法如下：1)将目标Topic下的所有Partirtion排序，存于$P_T$2)对某Consumer Group下所有Consumer排序，存$于C_G$，第$i$个Consumer记为$C_i$3)$N=size(P_T)/size(C_G)$，向上取整4)解除$C_i$对原来分配的Partition的消费权（i从0开始）5)将第$iN$到$（i+1）N-1$个Partition分配给$C_i$","tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://blog.yaodataking.com/tags/Kafka/"}]},{"title":"Kafka入门之五:Kafka的Leader Election","date":"2016-11-12T15:21:16.000Z","path":"2016/11/12/kafka-5/","text":"1.基于Zookeeper的Leader Election1.1抢注Leader节点——非公平模式1) 创建Leader父节点，如 /chroot，并将其设置为persist节点2) 各客户端通过在/chroot下创建Leader节点，如/chroot/leader，来竞争Leader。该节点应被设置为ephemeral3) 若某创建Leader节点成功，则该客户端成功竞选为Leader4) 若创建Leader节点失败，则竞选Leader失败，在/chroot/leader节点上注册exist的watch，一旦该节点被删除则获得通知5) Leader可通过删除Leader节点来放弃Leader6) 如果Leader宕机，由于Leader节点被设置为ephemeral，Leader节点会自行删除。而其它节点由于在Leader节点上注册了watch，故可得到通知，参与下一轮竞选，从而保证总有客户端以Leader角色工作。1.2先到先得，后者监视前者——公平模式1) 创建Leader父节点，如 /chroot，并将其设置为persist节点2) 各客户端通过在/chroot下创建Leader节点，如/chroot/leader，来竞争Leader。该节点应被设置为 ephemeral_sequential3) 客户端通过getChildren方法获取/chroot/下所有子节点，如果其注册的节点的id在所有子节点中最小，则当前客户端竞选Leader成功4) 否则，在前面一个节点上注册watch，一旦前者被删除，则它得到通知，返回step3(并不能直接认为自己成为新的Leader，因为可能前面的节点只是宕机了)5) Leader节点可通过自行删除自己创建的节点以放弃Leader1.3Leader Election在Curator中的实现1.3.1 LeaderLatch竞选为Leader后，不可自行放弃领导权只能通过close方法放弃领导权强烈建议增加ConnectionStateListener，当连接SUSPENDED或者LOST时视为丢失领导权，可通过await方法等待成功获取领导权，并可加入timeout可通过hasLeadership方法判断是否为Leader可通过getLeader方法获取当前Leader可通过getParticipants方法获取当前竞选Leader的参与方1.3.2 LeaderSelector竞选Leader成功后回调takeLeadership方法可在takeLeadership方法中实现业务逻辑一旦takeLeadership方法返回，即视为放弃领导权可通过autoRequeue方法循环获取领导权可通过hasLeadership方法判断是否为Leader可通过getLeader方法获取当前Leader可通过getParticipants方法获取当前竞选Leader的参与方2 Kafka的Leader Election2.1 “各自为政”的Leader Election每个Partition的多个Replica同时竞争Leader优点:实现简单缺点:Herd Effect，Zookeeper负载过重，Latency较大2.2 基于Controller的Leader Election整个集群中选举出一个Broker作为Controller，Controller为所有Topic的所有Partition指定Leader及Follower优点：极大缓解Herd Effect问题，减轻Zookeeper负载Controller与Leader及Follower间通过RPC通信，高效且实时缺点：引入Controller增加了复杂度需要考虑Controller的Failover","tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"http://blog.yaodataking.com/tags/zookeeper/"},{"name":"Kafka","slug":"Kafka","permalink":"http://blog.yaodataking.com/tags/Kafka/"}]},{"title":"Mac 10.12下搭建Eclipse的maven开发环境","date":"2016-11-06T01:58:00.000Z","path":"2016/11/06/mac-10-12-eclipse-maven/","text":"1.Java安装从Oralce官网下载http://download.oracle.com/otn-pub/java/jdk/8u112-b16/jdk-8u112-macosx-x64.dmg下载完毕直接双击dmg文件安装，安装完毕，运行java -version查看 Downloads alex$ java -version java version &quot;1.8.0_112&quot; Java(TM) SE Runtime Environment (build 1.8.0_112-b16) Java HotSpot(TM) 64-Bit Server VM (build 25.112-b16, mixed mode)`&lt;/pre&gt; 2.Eclipse安装 以下镜像地址下载。 http://mirrors.neusoft.edu.cn/eclipse/technology/epp/downloads/release/neon/1a/eclipse-java-neon-1a-macosx-cocoa-x86_64.tar.gz 直接解压至应用程序目录。 &lt;pre&gt;`tar -xzvf eclipse-java-neon-1a-macosx-cocoa-x86_64.tar.gz /Applications`&lt;/pre&gt; 3.Maven安装 下载地址 http://mirror.bit.edu.cn/apache/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz 解压 &lt;pre&gt;`tar -xzvf apache-maven-3.3.9-bin.tar /opt`&lt;/pre&gt; 添加以下命令行至/etc/bashrc &lt;pre&gt;`export MAVEN_HOME=/opt/apache-maven-3.3.9 export PATH=${PATH}:${MAVEN_HOME}/bin`&lt;/pre&gt; 执行source命令生效 &lt;pre&gt;`source /etc/bashrc`&lt;/pre&gt; 验证maven是否安装成功。 &lt;pre&gt;`$ mvn -v Apache Maven 3.3.9 (bb52d8502b132ec0a5a3f4c09453c07478323dc5; 2015-11-11T00:41:47+08:00) Maven home: /opt/apache-maven-3.3.9 Java version: 1.8.0_112, vendor: Oracle Corporation Java home: /Library/Java/JavaVirtualMachines/jdk1.8.0_112.jdk/Contents/Home/jre Default locale: zh_CN, platform encoding: UTF-8 OS name: &quot;mac os x&quot;, version: &quot;10.12&quot;, arch: &quot;x86_64&quot;, family: &quot;mac&quot; 4.配置打开Eclipse，选择Help-&gt;Install New SoftWareAdd new repohttp://m2eclipse.sonatype.org/sites/m2e,点击OK，等待安装完成. 5.验证重启eclipse， Help –&gt; About Eclipse –&gt; Installation Details在Installed Software标签中检查刚才选择的模块是否在这个列表中检查eclipse是否已经支持创建Maven项目：File –&gt; New –&gt; Other ，找到Maven一项，如果展开一切正常，说明m2eclipse已经正确安装了。","tags":[{"name":"eclipse","slug":"eclipse","permalink":"http://blog.yaodataking.com/tags/eclipse/"},{"name":"maven","slug":"maven","permalink":"http://blog.yaodataking.com/tags/maven/"}]},{"title":"SSIS提取InfoPath XML数据示例","date":"2016-11-01T01:06:54.000Z","path":"2016/11/01/infopath-xml-ssis/","text":"引言最近碰到一个看似简单的项目，要提取InfoPath 的表单的数据。关于InfoPath 表单还是有点认识的，表单是以XML格式存储的，模板文件后缀名xsn，应该很容易从模板文件提取schema结构读取XML数据。虽然InfoPath作为一个产品将在2023年永远的退出历史，但是作为表单工具对于办公室信息工作人员来说还是比较受欢迎的。因此一些企业积累了不少的表单数据需要转成其他应用可读的数据。这个项目就是要把表单数据通过SSIS这个ETL工具导入数据库以供后续分析。关于InfoPath,互动百科是这样描述的。 InfoPath是微软Office 2003家族中引入的成员，最终的正式版本为InfoPath2013，该版本支持在线填写表单。 InfoPath是企业级搜集信息和制作表单的工具，将很多的界面控件集成在该工具中，为企业开发表单搜集系统提供了极大的方便。 InfoPath文件的后缀名是.xml，可见InfoPath是基于XML技术的。 作为一个数据存储中间层技术，InfoPath提供大量常用控件，如：Date Picker、文本框、可选节、重复节等，同时提供很多表格页面设计工具。 开发人员可以为每个控件设置相应的数据有效性规则或数学公式。 如果InfoPath仅能做到上述功能，那么我们是可以用Excel做的表单代替InfoPath的，最重要的功能，就是InfoPath提供和数据库和Web服务之间的连接。 2014年1月31日，微软office官方博客宣布，InfoPath2013为最后的桌面客户端版本，InfoPath桌面软件和服务器产品的Lifecycle支持都会到2023年4月。 InfoPath的文件组成InfoPath通常有一个以xsn为后缀名的模板文件，一个正常的InfoPath模板文件包含以下文件manifest.xsf——InfoPath的定义文件，定义整个InfoPath的组成结构，里面会指定该InfoPath的其它文件，并且必须是InfoPath中的第一个文件。myschema.xsd——InfoPath的架构文件，定义该InfoPath里面的包含的域和域的类型sampledata.xml——包含InfoPath的域template.xml——新建表单的时候进行实例化InfoPathview1.xsl——存储InfoPath中视图的文件一个典型的InfoPath XML如下所示， &amp;lt;?xml version=&quot;1.0&quot;?&amp;gt;&amp;lt;?mso-infoPathSolution productVersion=&quot;14.0.0&quot; PIVersion=&quot;1.0.0.0&quot; href=&quot;http://intranet/PO/template.xsn&quot; name=&quot;urn:schemas-microsoft-com:office:infopath:PO:-myXSD-2009-03-06T03-16-15&quot; solutionVersion=&quot;1.0.0.1083&quot; ?&amp;gt;&amp;lt;?mso-application progid=&quot;InfoPath.Document&quot; versionProgid=&quot;InfoPath.Document.2&quot;?&amp;gt;&amp;lt;?mso-infoPath-file-attachment-present?&amp;gt;&amp;lt;my:myFields xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:dfs=&quot;http://schemas.microsoft.com/office/infopath/2003/dataFormSolution&quot; xmlns:s0=&quot;http://microsoft.com/webservices/SharePointPortalServer/UserProfileService&quot; xmlns:s1=&quot;http://microsoft.com/wsdl/types/&quot; xmlns:mime=&quot;http://schemas.xmlsoap.org/wsdl/mime/&quot; xmlns:tm=&quot;http://microsoft.com/wsdl/mime/textMatching/&quot; xmlns:soapenc=&quot;http://schemas.xmlsoap.org/soap/encoding/&quot; xmlns:soap=&quot;http://schemas.xmlsoap.org/wsdl/soap/&quot; xmlns:http=&quot;http://schemas.xmlsoap.org/wsdl/http/&quot; xmlns:ns1=&quot;http://schemas.xmlsoap.org/wsdl/&quot; xmlns:my=&quot;http://schemas.microsoft.com/office/infopath/2003/myXSD/2009-03-06T03:16:15&quot; xmlns:xd=&quot;http://schemas.microsoft.com/office/infopath/2003&quot; xml:lang=&quot;en-us&quot;&amp;gt; &amp;lt;my:PONo&amp;gt;201609223&amp;lt;/my:PONo&amp;gt; &amp;lt;my:POVendor&amp;gt;1&amp;lt;/my:POVendor&amp;gt; &amp;lt;my:PRItemD&amp;gt; &amp;lt;my:ItemNo&amp;gt;11223344&amp;lt;/my:ItemNo&amp;gt; &amp;lt;my:ItemDesc&amp;gt;11223344 desc&amp;lt;/my:ItemDesc&amp;gt; &amp;lt;my:ItemQty&amp;gt;1&amp;lt;/my:ItemQty&amp;gt; &amp;lt;my:ItemPrice&amp;gt;1203.2&amp;lt;/my:ItemPrice&amp;gt; &amp;lt;my:ItemAmt&amp;gt;1203.2&amp;lt;/my:ItemAmt&amp;gt; &amp;lt;/my:PRItemD&amp;gt; &amp;lt;/my:myFields&amp;gt;`&lt;/pre&gt; 从以上我们看到InfoPath除了前面的定义外，InfoPath的数据确实是XML格式的。 **问题的产生** 但是当我们使用SSIS读取这个InfoPath表单时却出现了The XML contains multiple namespaces的错误。 [![2016-10-25_16-47-37](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/10/2016-10-25_16-47-37.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/10/2016-10-25_16-47-37.jpg) 这个问题怎么产生的呢？如何解决这个错误呢？ 分析InfoPath的XML文件，我们看到很多xmlns开头的namespaces的定义，对于存储的数据来说，我们其实只要保留xmlns:xsi这个namespace就可以了。 因此我们需要用XSLT解析InfoPath的XML文件并去掉文件中的InfoPath额外信息 &lt;pre&gt;`&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot; ?&amp;gt; &amp;lt;xsl:stylesheet version=&quot;1.0&quot; xmlns:xsl=&quot;http://www.w3.org/1999/XSL/Transform&quot;&amp;gt; &amp;lt;xsl:output method=&quot;xml&quot; indent=&quot;yes&quot; /&amp;gt; &amp;lt;xsl:template match=&quot;/|comment()|processing-instruction()&quot;&amp;gt; &amp;lt;xsl:copy&amp;gt; &amp;lt;xsl:apply-templates /&amp;gt; &amp;lt;/xsl:copy&amp;gt; &amp;lt;/xsl:template&amp;gt; &amp;lt;xsl:template match=&quot;*&quot;&amp;gt; &amp;lt;xsl:element name=&quot;{local-name()}&quot;&amp;gt; &amp;lt;xsl:apply-templates select=&quot;@*|node()&quot; /&amp;gt; &amp;lt;/xsl:element&amp;gt; &amp;lt;/xsl:template&amp;gt; &amp;lt;xsl:template match=&quot;@*&quot;&amp;gt; &amp;lt;xsl:attribute name=&quot;{local-name()}&quot;&amp;gt; &amp;lt;xsl:value-of select=&quot;.&quot; /&amp;gt; &amp;lt;/xsl:attribute&amp;gt; &amp;lt;/xsl:template&amp;gt; &amp;lt;/xsl:stylesheet&amp;gt;`&lt;/pre&gt; 生成新的xml数据如下 &lt;pre&gt;`&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&amp;gt;&amp;lt;?mso-infoPathSolution productVersion=&quot;14.0.0&quot; PIVersion=&quot;1.0.0.0&quot; href=&quot;http://intranet/PO/template.xsn&quot; name=&quot;urn:schemas-microsoft-com:office:infopath:PO:-myXSD-2009-03-06T03-16-15&quot; solutionVersion=&quot;1.0.0.1083&quot; ?&amp;gt;&amp;lt;?mso-application progid=&quot;InfoPath.Document&quot; versionProgid=&quot;InfoPath.Document.2&quot;?&amp;gt;&amp;lt;?mso-infoPath-file-attachment-present?&amp;gt; &amp;lt;myFields lang=&quot;en-us&quot;&amp;gt; &amp;lt;PONo&amp;gt;201609223&amp;lt;/PONo&amp;gt; &amp;lt;POVendor&amp;gt;1&amp;lt;/POVendor&amp;gt; &amp;lt;PRItemD&amp;gt; &amp;lt;ItemNo&amp;gt;11223344&amp;lt;/ItemNo&amp;gt; &amp;lt;ItemDesc&amp;gt;11223344 desc&amp;lt;/ItemDesc&amp;gt; &amp;lt;ItemQty&amp;gt;1&amp;lt;/ItemQty&amp;gt; &amp;lt;ItemPrice&amp;gt;1203.2&amp;lt;/ItemPrice&amp;gt; &amp;lt;ItemAmt&amp;gt;1203.2&amp;lt;/ItemAmt&amp;gt; &amp;lt;/PRItemD&amp;gt; &amp;lt;/myFields&amp;gt;`&lt;/pre&gt; 但是这样还是不能提取数据,我们看到SSIS 没有读取PRNO，PRVENDOR栏位信息，这是因为这些信息没有包含在一个节点里。 [![2016-10-26_16-41-55](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/10/2016-10-26_16-41-55.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/10/2016-10-26_16-41-55.jpg) 这就需要我们再次使用XSLT格式化XML文件。 &lt;pre&gt;`&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot; ?&amp;gt; &amp;lt;xsl:stylesheet version=&quot;1.0&quot; xmlns:xsl=&quot;http://www.w3.org/1999/XSL/Transform&quot;&amp;gt; &amp;lt;xsl:output method=&quot;xml&quot; indent=&quot;yes&quot; /&amp;gt; &amp;lt;xsl:template match=&quot;/myFields&quot;&amp;gt; &amp;lt;PR&amp;gt; &amp;lt;PRHead&amp;gt; &amp;lt;PONo&amp;gt;&amp;lt;xsl:value-of select=&quot;PONo&quot;/&amp;gt;&amp;lt;/PONo&amp;gt; &amp;lt;POVendor&amp;gt;&amp;lt;xsl:value-of select=&quot;POVendor&quot;/&amp;gt;&amp;lt;/POVendor&amp;gt; &amp;lt;/PRHead&amp;gt; &amp;lt;xsl:for-each select=&quot;PRItemD&quot;&amp;gt; &amp;lt;PRItemD&amp;gt; &amp;lt;ItemNo&amp;gt;&amp;lt;xsl:value-of select=&quot;ItemNo&quot;/&amp;gt;&amp;lt;/ItemNo&amp;gt; &amp;lt;ItemDesc&amp;gt;&amp;lt;xsl:value-of select=&quot;ItemDesc&quot;/&amp;gt;&amp;lt;/ItemDesc&amp;gt; &amp;lt;ItemQty&amp;gt;&amp;lt;xsl:value-of select=&quot;ItemQty&quot;/&amp;gt;&amp;lt;/ItemQty&amp;gt; &amp;lt;ItemPrice&amp;gt;&amp;lt;xsl:value-of select=&quot;ItemPrice&quot;/&amp;gt;&amp;lt;/ItemPrice&amp;gt; &amp;lt;ItemAmt&amp;gt;&amp;lt;xsl:value-of select=&quot;ItemAmt&quot;/&amp;gt;&amp;lt;/ItemAmt&amp;gt; &amp;lt;/PRItemD&amp;gt; &amp;lt;/xsl:for-each&amp;gt; &amp;lt;/PR&amp;gt; &amp;lt;/xsl:template&amp;gt; &amp;lt;/xsl:stylesheet&amp;gt;`&lt;/pre&gt; 通过第二次转换，我们获得了完全符合XML标准的文件 &lt;pre&gt;`&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&amp;gt; &amp;lt;PR&amp;gt; &amp;lt;PRHead&amp;gt; &amp;lt;PONo&amp;gt;201609223&amp;lt;/PONo&amp;gt; &amp;lt;POVendor&amp;gt;1&amp;lt;/POVendor&amp;gt; &amp;lt;/PRHead&amp;gt; &amp;lt;PRItemD&amp;gt; &amp;lt;ItemNo&amp;gt;11223344&amp;lt;/ItemNo&amp;gt; &amp;lt;ItemDesc&amp;gt;11223344 desc&amp;lt;/ItemDesc&amp;gt; &amp;lt;ItemQty&amp;gt;1&amp;lt;/ItemQty&amp;gt; &amp;lt;ItemPrice&amp;gt;1203.2&amp;lt;/ItemPrice&amp;gt; &amp;lt;ItemAmt&amp;gt;1203.2&amp;lt;/ItemAmt&amp;gt; &amp;lt;/PRItemD&amp;gt; &amp;lt;/PR&amp;gt;`&lt;/pre&gt; 接下来在Data Flow Task里添加XML source组件。 点击生成XSD按钮，可以自动从XML数据中获得，你可以需要对字段类型进行修改，因为默认导出的不一致符合字段的要求。 &lt;pre&gt;`&amp;lt;?xml version=&quot;1.0&quot;?&amp;gt; &amp;lt;xs:schema attributeFormDefault=&quot;unqualified&quot; elementFormDefault=&quot;qualified&quot; xmlns:xs=&quot;http://www.w3.org/2001/XMLSchema&quot;&amp;gt; &amp;lt;xs:element name=&quot;PR&quot;&amp;gt; &amp;lt;xs:complexType&amp;gt; &amp;lt;xs:sequence&amp;gt; &amp;lt;xs:element minOccurs=&quot;0&quot; name=&quot;PRHead&quot;&amp;gt; &amp;lt;xs:complexType&amp;gt; &amp;lt;xs:sequence&amp;gt; &amp;lt;xs:element minOccurs=&quot;0&quot; name=&quot;PrNo&quot; /&amp;gt; &amp;lt;xs:element minOccurs=&quot;0&quot; name=&quot;POVendor&quot; type=&quot;xs:unsignedByte&quot; /&amp;gt; &amp;lt;/xs:sequence&amp;gt; &amp;lt;/xs:complexType&amp;gt; &amp;lt;/xs:element&amp;gt; &amp;lt;xs:element minOccurs=&quot;0&quot; name=&quot;PRItemD&quot;&amp;gt; &amp;lt;xs:complexType&amp;gt; &amp;lt;xs:sequence&amp;gt; &amp;lt;xs:element minOccurs=&quot;0&quot; name=&quot;ItemNo&quot; type=&quot;xs:unsignedInt&quot; /&amp;gt; &amp;lt;xs:element minOccurs=&quot;0&quot; name=&quot;ItemDesc&quot; type=&quot;xs:string&quot; /&amp;gt; &amp;lt;xs:element minOccurs=&quot;0&quot; name=&quot;ItemQty&quot; type=&quot;xs:unsignedByte&quot; /&amp;gt; &amp;lt;xs:element minOccurs=&quot;0&quot; name=&quot;ItemPrice&quot; type=&quot;xs:decimal&quot; /&amp;gt; &amp;lt;xs:element minOccurs=&quot;0&quot; name=&quot;ItemAmt&quot; type=&quot;xs:decimal&quot; /&amp;gt; &amp;lt;/xs:sequence&amp;gt; &amp;lt;/xs:complexType&amp;gt; &amp;lt;/xs:element&amp;gt; &amp;lt;/xs:sequence&amp;gt; &amp;lt;/xs:complexType&amp;gt; &amp;lt;/xs:element&amp;gt; &amp;lt;/xs:schema&amp;gt; 通过xsd我们读取了我们需要的数据。如图，我们同时看到了head和detail分组。接下来我们就可以指定输出文件了，为简单起见，示列文件我们输出到文本文件，根据项目需要，可以输出至数据库。如图，Data flow任务，这里我们分别输出head和detail到不同的文件。整个package运行如下。对于需要处理大量Infopath XML 数据，只要把以上组件放在一个Foreach Loop Container里就可以了。附示列package包下载","tags":[{"name":"ETL","slug":"ETL","permalink":"http://blog.yaodataking.com/tags/ETL/"},{"name":"InfoPath","slug":"InfoPath","permalink":"http://blog.yaodataking.com/tags/InfoPath/"},{"name":"SSIS","slug":"SSIS","permalink":"http://blog.yaodataking.com/tags/SSIS/"},{"name":"XML","slug":"XML","permalink":"http://blog.yaodataking.com/tags/XML/"}]},{"title":"Kafka入门之四:Kafka如何使用zookeeper","date":"2016-10-30T15:43:20.000Z","path":"2016/10/30/kafka-4/","text":"作为解决分布式一致性问题的工具，zookeeper应用广泛，在Hadoop集群部署时，我们已经领略过了，具体可参见博文Hadoop 2.7.1 实现HA集群部署。今天我们在kafka之前三节课的基础上演示如何部署zookeeper集群。1.docker-compose安装因为要使用docker-compose这个工具来部署zookeeper和kafka容器。所以先安装docker-compose。 sudo -i curl -L &quot;https://github.com/docker/compose/releases/download/1.8.1/docker-compose-$(uname -s)-$(uname -m)&quot; &amp;gt; /usr/local/bin/docker-compose chmod +x /usr/local/bin/docker-compose exit`&lt;/pre&gt; [![2016-10-30_9-07-18](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/10/2016-10-30_9-07-18.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/10/2016-10-30_9-07-18.jpg) 2.zookeeper镜像制作 在之前的基础上增加ZOOKEEPER_ID，ZOOKEEPER_PORT，ZOOKEEPER_SERVERS参数使之动态配置。 &lt;pre&gt;`FROM java:openjdk-8-jre-alpine ARG MIRROR=http://mirrors.aliyun.com/ ARG VERSION=3.4.6 LABEL name=&quot;zookeeper&quot; version=$VERSION RUN apk update &amp;amp;&amp;amp; apk add ca-certificates &amp;amp;&amp;amp; \\ apk add tzdata &amp;amp;&amp;amp; \\ ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime &amp;amp;&amp;amp; \\ echo &quot;Asia/Shanghai&quot; &amp;gt; /etc/timezone RUN apk add --no-cache wget bash \\ &amp;amp;&amp;amp; mkdir /opt \\ &amp;amp;&amp;amp; wget -q -O - $MIRROR/apache/zookeeper/zookeeper-$VERSION/zookeeper-$VERSION.tar.gz | tar -xzf - -C /opt \\ &amp;amp;&amp;amp; mv /opt/zookeeper-$VERSION /opt/zookeeper \\ &amp;amp;&amp;amp; cp /opt/zookeeper/conf/zoo_sample.cfg /opt/zookeeper/conf/zoo.cfg \\ &amp;amp;&amp;amp; mkdir -p /tmp/zookeeper RUN echo &quot;[ ! -z $&quot;&quot;ZOOKEEPER_PORT&quot;&quot; ] &amp;amp;&amp;amp; sed -i &apos;s%clientPort=.*$%clientPort=&apos;$&quot;&quot;ZOOKEEPER_PORT&apos;&quot;&quot;%g&apos; /opt/zookeeper/conf/zoo.cfg&quot; &amp;gt; /opt/zookeeper/start.sh &amp;amp;&amp;amp;\\ echo &quot;[ ! -z $&quot;&quot;ZOOKEEPER_ID&quot;&quot; ] &amp;amp;&amp;amp; echo $&quot;&quot;ZOOKEEPER_ID &amp;gt; /tmp/zookeeper/myid&quot; &amp;gt;&amp;gt; /opt/zookeeper/start.sh &amp;amp;&amp;amp;\\ echo &quot;[[ ! -z $&quot;&quot;ZOOKEEPER_SERVERS&quot;&quot; ]] &amp;amp;&amp;amp; for server in $&quot;&quot;ZOOKEEPER_SERVERS&quot;&quot;; do echo $&quot;&quot;server&quot;&quot; &amp;gt;&amp;gt; /opt/zookeeper/conf/zoo.cfg; done&quot; &amp;gt;&amp;gt; /opt/zookeeper/start.sh &amp;amp;&amp;amp;\\ echo &quot;/opt/zookeeper/bin/zkServer.sh start-foreground&quot; &amp;gt;&amp;gt; /opt/zookeeper/start.sh EXPOSE 2181 WORKDIR /opt/zookeeper VOLUME [&quot;/opt/zookeeper/conf&quot;, &quot;/tmp/zookeeper&quot;] ENTRYPOINT [&quot;bash&quot;, &quot;/opt/zookeeper/start.sh&quot;]`&lt;/pre&gt; 3.kafka镜像制作 在之前的基础上增加BROKER_ID，BROKER_PORT，ADVERTISED_HOST_NAME，HOST_NAME，ZOOKEEPER_CONNECT参数使之动态配置。 &lt;pre&gt;`FROM java:openjdk-8-jre-alpine ARG MIRROR=http://mirrors.aliyun.com/ ARG SCALA_VERSION=2.11 ARG KAFKA_VERSION=0.8.2.2 LABEL name=&quot;kafka&quot; version=$VERSION RUN apk update &amp;amp;&amp;amp; apk add ca-certificates &amp;amp;&amp;amp; \\ apk add tzdata &amp;amp;&amp;amp; \\ ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime &amp;amp;&amp;amp; \\ echo &quot;Asia/Shanghai&quot; &amp;gt; /etc/timezone RUN apk add --no-cache wget bash \\ &amp;amp;&amp;amp; mkdir /opt \\ &amp;amp;&amp;amp; wget -q -O - $MIRROR/apache/kafka/$KAFKA_VERSION/kafka_$SCALA_VERSION-$KAFKA_VERSION.tgz | tar -xzf - -C /opt \\ &amp;amp;&amp;amp; mv /opt/kafka_$SCALA_VERSION-$KAFKA_VERSION /opt/kafka \\ &amp;amp;&amp;amp; sed -i &apos;s/num.partitions.*$/num.partitions=3/g&apos; /opt/kafka/config/server.properties RUN echo &quot;cd /opt/kafka&quot; &amp;gt; /opt/kafka/start.sh &amp;amp;&amp;amp;\\ echo &quot;[ ! -z $&quot;&quot;ZOOKEEPER_CONNECT&quot;&quot; ] &amp;amp;&amp;amp; sed -i &apos;s%zookeeper.connect=.*$%zookeeper.connect=&apos;$&quot;&quot;ZOOKEEPER_CONNECT&apos;&quot;&quot;%g&apos; /opt/kafka/config/server.properties&quot; &amp;gt;&amp;gt; /opt/kafka/start.sh &amp;amp;&amp;amp;\\ echo &quot;[ ! -z $&quot;&quot;BROKER_ID&quot;&quot; ] &amp;amp;&amp;amp; sed -i &apos;s%broker.id=.*$%broker.id=&apos;$&quot;&quot;BROKER_ID&apos;&quot;&quot;%g&apos; /opt/kafka/config/server.properties&quot; &amp;gt;&amp;gt; /opt/kafka/start.sh &amp;amp;&amp;amp;\\ echo &quot;[ ! -z $&quot;&quot;BROKER_PORT&quot;&quot; ] &amp;amp;&amp;amp; sed -i &apos;s%port=.*$%port=&apos;$&quot;&quot;BROKER_PORT&apos;&quot;&quot;%g&apos; /opt/kafka/config/server.properties&quot; &amp;gt;&amp;gt; /opt/kafka/start.sh &amp;amp;&amp;amp;\\ echo &quot;[ ! -z $&quot;&quot;ADVERTISED_HOST_NAME&quot;&quot; ] &amp;amp;&amp;amp; sed -i &apos;s%#advertised.host.name=.*$%advertised.host.name=&apos;$&quot;&quot;ADVERTISED_HOST_NAME&apos;&quot;&quot;%g&apos; /opt/kafka/config/server.properties&quot; &amp;gt;&amp;gt; /opt/kafka/start.sh &amp;amp;&amp;amp;\\ echo &quot;[ ! -z $&quot;&quot;HOST_NAME&quot;&quot; ] &amp;amp;&amp;amp; sed -i &apos;s%#host.name=.*$%host.name=&apos;$&quot;&quot;HOST_NAME&apos;&quot;&quot;%g&apos; /opt/kafka/config/server.properties&quot; &amp;gt;&amp;gt; /opt/kafka/start.sh &amp;amp;&amp;amp;\\ echo &quot;delete.topic.enable=true&quot; &amp;gt;&amp;gt; /opt/kafka/config/server.properties &amp;amp;&amp;amp;\\ echo &quot;bin/kafka-server-start.sh config/server.properties&quot; &amp;gt;&amp;gt; /opt/kafka/start.sh EXPOSE 9092 WORKDIR /opt/kafka ENTRYPOINT [&quot;bash&quot;, &quot;/opt/kafka/start.sh&quot;]`&lt;/pre&gt; 4.docker-compose文件制作 创建3个zookeeper容器和3个kafka容器。 &lt;pre&gt;`version: &apos;2.0&apos; services: zookeeper0: build: context: . dockerfile: myzookeeper.cluster.Dockerfile image: alex/zookeeper_cluster:3.4.6 container_name: zookeeper0 hostname: zookeeper0 ports: - &quot;2181:2181&quot; - &quot;2888:2888&quot; - &quot;3888:3888&quot; expose: - 2181 - 2888 - 3888 environment: ZOOKEEPER_PORT: 2181 ZOOKEEPER_ID: 0 ZOOKEEPER_SERVERS: server.0=zookeeper0:2888:3888 server.1=zookeeper1:28881:38881 server.2=zookeeper2:28882:38882 zookeeper1: build: context: . dockerfile: myzookeeper.cluster.Dockerfile image: alex/zookeeper_cluster:3.4.6 container_name: zookeeper1 hostname: zookeeper1 ports: - &quot;2182:2182&quot; - &quot;28881:28881&quot; - &quot;38881:38881&quot; expose: - 2182 - 2888 - 3888 environment: ZOOKEEPER_PORT: 2182 ZOOKEEPER_ID: 1 ZOOKEEPER_SERVERS: server.0=zookeeper0:2888:3888 server.1=zookeeper1:28881:38881 server.2=zookeeper2:28882:38882 zookeeper2: build: context: . dockerfile: myzookeeper.cluster.Dockerfile image: alex/zookeeper_cluster:3.4.6 container_name: zookeeper2 hostname: zookeeper2 ports: - &quot;2183:2183&quot; - &quot;28882:28882&quot; - &quot;38882:38882&quot; expose: - 2183 - 2888 - 3888 environment: ZOOKEEPER_PORT: 2183 ZOOKEEPER_ID: 2 ZOOKEEPER_SERVERS: server.0=zookeeper0:2888:3888 server.1=zookeeper1:28881:38881 server.2=zookeeper2:28882:38882 kafka0: build: context: . dockerfile: mykafka.cluster.Dockerfile image: alex/kafka_cluster:0.8.2.2 container_name: kafka0 hostname: kafka0 ports: - &quot;9092:9092&quot; environment: ZOOKEEPER_CONNECT: zookeeper0:2181,zookeeper1:2182,zookeeper2:2183/kafka BROKER_ID: 0 BROKER_PORT: 9092 ADVERTISED_HOST_NAME: kafka0 HOST_NAME: kafka0 volumes: - /var/run/docker.sock:/var/run/docker.sock depends_on: - zookeeper0 - zookeeper1 - zookeeper2 expose: - 9092 kafka1: build: context: . dockerfile: mykafka.cluster.Dockerfile image: alex/kafka_cluster:0.8.2.2 container_name: kafka1 hostname: kafka1 ports: - &quot;9093:9093&quot; environment: ZOOKEEPER_CONNECT: zookeeper0:2181,zookeeper1:2182,zookeeper2:2183/kafka BROKER_ID: 1 BROKER_PORT: 9093 ADVERTISED_HOST_NAME: kafka1 HOST_NAME: kafka1 volumes: - /var/run/docker.sock:/var/run/docker.sock depends_on: - zookeeper0 - zookeeper1 - zookeeper2 expose: - 9093 kafka2: build: . build: context: . dockerfile: mykafka.cluster.Dockerfile image: alex/kafka_cluster:0.8.2.2 container_name: kafka2 hostname: kafka2 ports: - &quot;9094:9094&quot; environment: ZOOKEEPER_CONNECT: zookeeper0:2181,zookeeper1:2182,zookeeper2:2183/kafka BROKER_ID: 2 BROKER_PORT: 9094 ADVERTISED_HOST_NAME: kafka2 HOST_NAME: kafka2 volumes: - /var/run/docker.sock:/var/run/docker.sock depends_on: - zookeeper0 - zookeeper1 - zookeeper2 expose: - 9094`&lt;/pre&gt; 这里要注意的是ZOOKEEPER_CONNECT参数中使用/kafka作为根目录。 4.启动docker-compose &lt;pre&gt;`docker-compose -f mykafka-compose.yml up -d`&lt;/pre&gt; 正常情况下，3个kafka容器，3个zookeeper容器启动。 [![2016-10-30_23-23-15](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/10/2016-10-30_23-23-15.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/10/2016-10-30_23-23-15.jpg) 5.验证zookeeper及目录结构 进入一个zookeeper容器使用zkCli.sh客户端命令查看。所有broker信息在kafka目录下 &lt;pre&gt;`docker exec -it zookeeper0 bash bin/zkCli.sh -server zookeeper0:2181`&lt;/pre&gt; [![2016-10-30_22-50-58](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/10/2016-10-30_22-50-58.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/10/2016-10-30_22-50-58.jpg) 如果不设置/kakfa，一般目录结构如下。 [![2016-10-30_22-26-55](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/10/2016-10-30_22-26-55.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/10/2016-10-30_22-26-55.jpg) 6.验证kafka消息发送 进入一个kafka容器 &lt;pre&gt;`docker exec -it kafka0 bash`&lt;/pre&gt; 创建一个topic，注意的是，我们之前配置的是/kafka,所以创建topic时一定要带上。 &lt;pre&gt;`bash-4.3# bin/kafka-topics.sh --zookeeper zookeeper0:2181/kafka --topic topic1 --create --replication-factor 2 --partitions 3 Created topic &quot;topic1&quot;. bash-4.3# `&lt;/pre&gt; 如果没有带/kafka,我们看到会直接出错。 &lt;pre&gt;`bash-4.3# bin/kafka-topics.sh --zookeeper zookeeper0:2181 --topic topic1 --create --replication-factor 2 --partitions 3 Error while executing topic command org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /brokers/ids org.I0Itec.zkclient.exception.ZkNoNodeException: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /brokers/ids at org.I0Itec.zkclient.exception.ZkException.create(ZkException.java:47) at org.I0Itec.zkclient.ZkClient.retryUntilConnected(ZkClient.java:685) at org.I0Itec.zkclient.ZkClient.getChildren(ZkClient.java:413) at org.I0Itec.zkclient.ZkClient.getChildren(ZkClient.java:409) at kafka.utils.ZkUtils$.getChildren(ZkUtils.scala:462) at kafka.utils.ZkUtils$.getSortedBrokerList(ZkUtils.scala:78) at kafka.admin.AdminUtils$.createTopic(AdminUtils.scala:170) at kafka.admin.TopicCommand$.createTopic(TopicCommand.scala:93) at kafka.admin.TopicCommand$.main(TopicCommand.scala:55) at kafka.admin.TopicCommand.main(TopicCommand.scala) Caused by: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /brokers/ids at org.apache.zookeeper.KeeperException.create(KeeperException.java:111) at org.apache.zookeeper.KeeperException.create(KeeperException.java:51) at org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1472) at org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1500) at org.I0Itec.zkclient.ZkConnection.getChildren(ZkConnection.java:99) at org.I0Itec.zkclient.ZkClient$2.call(ZkClient.java:416) at org.I0Itec.zkclient.ZkClient$2.call(ZkClient.java:413) at org.I0Itec.zkclient.ZkClient.retryUntilConnected(ZkClient.java:675) ... 8 more`&lt;/pre&gt; 启动consumer也需带/kafka &lt;pre&gt;`bin/kafka-console-consumer.sh --zookeeper zookeeper0:2181/kafka --topic topic1 --from-beginning`&lt;/pre&gt; 启动producer，发送消息，consumer端可收到。 &lt;pre&gt;`bin/kafka-console-producer.sh --broker-list kafka0:9092 --topic topic1 再看zookeeper的topic目录，看到有新的topic1的目录结构生成。","tags":[{"name":"Docker","slug":"Docker","permalink":"http://blog.yaodataking.com/tags/Docker/"},{"name":"zookeeper","slug":"zookeeper","permalink":"http://blog.yaodataking.com/tags/zookeeper/"},{"name":"Kafka","slug":"Kafka","permalink":"http://blog.yaodataking.com/tags/Kafka/"},{"name":"docker-compose","slug":"docker-compose","permalink":"http://blog.yaodataking.com/tags/docker-compose/"}]},{"title":"Docker快速搭建oracle xe 11g开发环境","date":"2016-10-29T09:01:44.000Z","path":"2016/10/29/docker-oracle-xe-11g/","text":"Oracle 11g XE 是 Oracle 数据库的免费版本，支持标准版的大部分功能，11g XE 提供 Windows 和 Linux 版本。做为免费的 Oracle 数据库版本，XE 的限制是： 最大数据库大小为 11 GB 可使用的最大内存是 1G 一台机器上只能安装一个 XE 实例 XE 只能使用单 CPU，无法在多CPU上进行分布处理`&lt;/pre&gt; 一般情况下你需要去[oracle网站](http://www.oracle.com/technetwork/database/database-technologies/express-edition/downloads/index.html)下载所需版本，然后安装配置。现在你不需要这般麻烦，只需使用docker下载镜像即可。 你可以一键下载，基于ubuntu16.04的版本 &lt;pre&gt;`docker pull wnameless/oracle-xe-11g:16.04`&lt;/pre&gt; 或者基于ubuntu14.04 &lt;pre&gt;`docker pull wnameless/oracle-xe-11g:14.04.4`&lt;/pre&gt; 容器创建 &lt;pre&gt;`docker create --name oraclexe11g01 -p 49160:22 -p 49161:1521 wnameless/oracle-xe-11g:16.04`&lt;/pre&gt; 其中容器名字，你可以自己命名。如果需要启动多个容器，可以02,03...编下去。宿主机映射的端口号根据你的需要可以自己定义。 创建时使用ORACLE_ALLOW_REMOTE参数可以打开远程连接功能。 &lt;pre&gt;`docker create --name oraclexe11g01 -p 49160:22 -p 49161:1521 -e ORACLE_ALLOW_REMOTE=true wnameless/oracle-xe-11g:16.04`&lt;/pre&gt; 容器启动 &lt;pre&gt;`docker start oraclexe11g01`&lt;/pre&gt; 容器关闭 &lt;pre&gt;`docker stop oraclexe11g01`&lt;/pre&gt; oracle连接参数 &lt;pre&gt;`hostname: localhost port: 49161 sid: xe username: system password: oracle Password for SYS &amp;amp; SYSTEM oracle`&lt;/pre&gt; 使用SSH登录容器 &lt;pre&gt;`ssh root@localhost -p 49160 password: admin 登陆后，你可以sqlplus登录oracle你可以随时去github检查有无更新。","tags":[{"name":"Docker","slug":"Docker","permalink":"http://blog.yaodataking.com/tags/Docker/"},{"name":"Oracle","slug":"Oracle","permalink":"http://blog.yaodataking.com/tags/Oracle/"}]},{"title":"《罗马人的故事13:最后一搏》读书笔记","date":"2016-10-28T15:01:46.000Z","path":"2016/10/28/rome-story-13/","text":"公元284年，摆在初登帝位的戴克里先面前的形势是严峻的。如何对付罗马帝国北方防线莱茵河、多瑙河对面的蛮族？如何处理与东方大国波斯的关系？如何消灭盗匪维护国内的治安？一个人精力不够，戴克里先就玩起了两帝共治制度，初见成效后，又开始了四帝共治。的确这个制度解决了当务之急，但是随之而来的副作用很明显，维持庞大的兵力需要的费用猛增。很难说戴克里先的各项改革是成功的， 对罗马的改造也使得罗马帝国一天比一天更不像罗马，而罗马这座城市也早已不再是罗马帝国的首都了。总之，戴克里先的四帝共治制，就像一剂猛药，看似给迷途的帝国带来了一丝复兴迹象，确无助于罗马帝国的逐步衰落。随着戴克里先的退位，原来靠戴克里先的权威稳定的四帝共治制因这个制度本身的缺陷而走向完结，罗马帝国迎来了著名的君士坦丁时代。随着“米兰赦令”的发布，基督教取得了合法的地位，此时的罗马帝国，彻底变成了东方的君主专制模式。很多历史学家在叙述罗马史的时候，到君士坦丁时代就戛然而止，其理由是，此时的罗马帝国已经名存实亡。但是一个人在接受明知不可能完成的任务时到底需不需要搏一下，我的回答是，你不试一下怎么知道成不成功呢？","tags":[{"name":"罗马","slug":"罗马","permalink":"http://blog.yaodataking.com/tags/罗马/"}]},{"title":"Kafka入门之三:Kafka集群安装及演示","date":"2016-10-22T15:49:09.000Z","path":"2016/10/22/kafka-3/","text":"1. 简介在Kafka入门之一:使用Docker安装Kafka和Zookeeper里我们已经演示使用docker安装单节点的Kafka，也就是一个broker。但实际上kafka是天生支持多broker的。在安装之前，我们先来看几个broker参数。 更多参数参见官网 Property Default Description broker.id &nbsp; 每个broker都可以用一个唯一的非负整数id进行标识；这个id可以作为broker的&#8220;名字&#8221;，并且它的存在使得broker无须混淆consumers就可以迁移到不同的host/port上。你可以选择任意你喜欢的数字作为id，只要id是唯一的即可。 log.dirs /tmp/kafka-logs kafka存放数据的路径。这个路径并不是唯一的，可以是多个，路径之间只需要使用逗号分隔即可；每当创建新partition时，都会选择在包含最少partitions的路径下进行。 port 9092 server接受客户端连接的端口 zookeeper.connect null ZooKeeper连接字符串的格式为：hostname:port，此处hostname和port分别是ZooKeeper集群中某个节点的host和port；为了当某个host宕掉之后你能通过其他ZooKeeper节点进行连接，你可以按照一下方式制定多个hosts：hostname1:port1, hostname2:port2, hostname3:port3.ZooKeeper 允许你增加一个&#8220;chroot&#8221;路径，将集群中所有kafka数据存放在特定的路径下。当多个Kafka集群或者其他应用使用相同ZooKeeper集群时，可以使用这个方式设置数据存放路径。这种方式的实现可以通过这样设置连接字符串格式，如下所示： hostname1：port1，hostname2：port2，hostname3：port3/chroot/path 这样设置就将所有kafka集群数据存放在/chroot/path路径下。注意，在你启动broker之前，你必须创建这个路径，并且consumers必须使用相同的连接格式。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;message.max.bytes&lt;/td&gt; &lt;td&gt;1000000&lt;/td&gt; &lt;td&gt;server可以接收的消息最大尺寸。重要的是，consumer和producer有关这个属性的设置必须同步，否则producer发布的消息对consumer来说太大。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;num.network.threads&lt;/td&gt; &lt;td&gt;3&lt;/td&gt; &lt;td&gt;server用来处理网络请求的网络线程数目；一般你不需要更改这个属性。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;num.io.threads&lt;/td&gt; &lt;td&gt;8&lt;/td&gt; &lt;td&gt;server用来处理请求的I/O线程的数目；这个线程数目至少要等于硬盘的个数。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;background.threads&lt;/td&gt; &lt;td&gt;4&lt;/td&gt; &lt;td&gt;用于后台处理的线程数目，例如文件删除；你不需要更改这个属性。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;queued.max.requests&lt;/td&gt; &lt;td&gt;500&lt;/td&gt; &lt;td&gt;在网络线程停止读取新请求之前，可以排队等待I/O线程处理的最大请求个数。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;host.name&lt;/td&gt; &lt;td&gt;null&lt;/td&gt; &lt;td&gt;broker的hostname；如果hostname已经设置的话，broker将只会绑定到这个地址上；如果没有设置，它将绑定到所有接口，并发布一份到ZK&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;advertised.host.name&lt;/td&gt; &lt;td&gt;null&lt;/td&gt; &lt;td&gt;如果设置，则就作为broker 的hostname发往producer、consumers以及其他brokers&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;advertised.port&lt;/td&gt; &lt;td&gt;null&lt;/td&gt; &lt;td&gt;此端口将给与producers、consumers、以及其他brokers，它会在建立连接时用到； 它仅在实际端口和server需要绑定的端口不一样时才需要设置。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;num.partitions&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;如果创建topic时没有给出划分partitions个数，这个数字将是topic下partitions数目的默认数值。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;auto.create.topics.enable&lt;/td&gt; &lt;td&gt;true&lt;/td&gt; &lt;td&gt;是否允许自动创建topic。如果是真的，则produce或者fetch 不存在的topic时，会自动创建这个topic。否则需要使用命令行创建topic&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;default.replication.factor&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;默认备份份数，仅指自动创建的topics&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;replica.lag.time.max.ms&lt;/td&gt; &lt;td&gt;10000&lt;/td&gt; &lt;td&gt;如果一个follower在这个时间内没有发送fetch请求，leader将从ISR重移除这个follower，并认为这个follower已经挂了&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;replica.lag.max.messages&lt;/td&gt; &lt;td&gt;4000&lt;/td&gt; &lt;td&gt;如果一个replica没有备份的条数超过这个数值，则leader将移除这个follower，并认为这个follower已经挂了&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;replica.socket.timeout.ms&lt;/td&gt; &lt;td&gt;30*1000&lt;/td&gt; &lt;td&gt;leader 备份数据时的socket网络请求的超时时间&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;replica.socket.receive.buffer.bytes&lt;/td&gt; &lt;td&gt;64*1024&lt;/td&gt; &lt;td&gt;备份时向leader发送网络请求时的socket receive buffer&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;replica.fetch.max.bytes&lt;/td&gt; &lt;td&gt;1024*1024&lt;/td&gt; &lt;td&gt;备份时每次fetch的最大值&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;replica.fetch.min.bytes&lt;/td&gt; &lt;td&gt;500&lt;/td&gt; &lt;td&gt;leader发出备份请求时，数据到达leader的最长等待时间&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;replica.fetch.min.bytes&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;备份时每次fetch之后回应的最小尺寸&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;num.replica.fetchers&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;从leader备份数据的线程数&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;replica.high.watermark.checkpoint.interval.ms &lt;/td&gt; &lt;td&gt;5000&lt;/td&gt; &lt;td&gt;每个replica检查是否将最高水位进行固化的频率&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;zookeeper.session.timeout.ms&lt;/td&gt; &lt;td&gt;6000&lt;/td&gt; &lt;td&gt;zookeeper会话超时时间。&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;zookeeper.connection.timeout.ms&lt;/td&gt; &lt;td&gt;6000&lt;/td&gt; &lt;td&gt;客户端等待和zookeeper建立连接的最大时间&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;zookeeper.sync.time.ms&lt;/td&gt; &lt;td&gt;2000&lt;/td&gt; &lt;td&gt;zk follower落后于zk leader的最长时间&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;auto.leader.rebalance.enable&lt;/td&gt; &lt;td&gt;true&lt;/td&gt; &lt;td&gt;如果这是true，控制者将会自动平衡brokers对于partitions的leadership&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;leader.imbalance.per.broker.percentage&lt;/td&gt; &lt;td&gt;10&lt;/td&gt; &lt;td&gt;每个broker所允许的leader最大不平衡比率&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;leader.imbalance.check.interval.seconds&lt;/td&gt; &lt;td&gt;300&lt;/td&gt; &lt;td&gt;检查leader不平衡的频率&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;offset.metadata.max.bytes&lt;/td&gt; &lt;td&gt;4096&lt;/td&gt; &lt;td&gt;允许客户端保存他们offsets的最大个数&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;max.connections.per.ip&lt;/td&gt; &lt;td&gt;Int.MaxValue&lt;/td&gt; &lt;td&gt;每个ip地址上每个broker可以被连接的最大数目&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;max.connections.per.ip.overrides&lt;/td&gt; &lt;td&gt;&amp;nbsp;&lt;/td&gt; &lt;td&gt;每个ip或者hostname默认的连接的最大覆盖&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;connections.max.idle.ms&lt;/td&gt; &lt;td&gt;600000&lt;/td&gt; &lt;td&gt;空连接的超时限制&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;log.roll.jitter.{ms,hours}&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;从logRollTimeMillis抽离的jitter最大数目&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;num.recovery.threads.per.data.dir&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;每个数据目录用来日志恢复的线程数目&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;unclean.leader.election.enable&lt;/td&gt; &lt;td&gt;true&lt;/td&gt; &lt;td&gt;指明了是否能够使不在ISR中replicas设置用来作为leader&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;delete.topic.enable&lt;/td&gt; &lt;td&gt;false&lt;/td&gt; &lt;td&gt;能够删除topic&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; 这些参数在你还没有了解具体用途之前，你都可以默认，今天我们将会使用几个参数。2. Kafka集群2.1 Dockerfile我们在dockeefile中加入一些broker的参数，即在server.properites文件中设置的参数。 FROM java:openjdk-8-jre-alpine ARG MIRROR=http://mirrors.aliyun.com/ ARG SCALA_VERSION=2.11 ARG KAFKA_VERSION=0.8.2.2 LABEL name=&quot;kafka&quot; version=$VERSION RUN apk update &amp;amp;&amp;amp; apk add ca-certificates &amp;amp;&amp;amp; \\ apk add tzdata &amp;amp;&amp;amp; \\ ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime &amp;amp;&amp;amp; \\ echo &quot;Asia/Shanghai&quot; &amp;gt; /etc/timezone RUN apk add --no-cache wget bash \\ &amp;amp;&amp;amp; mkdir /opt \\ &amp;amp;&amp;amp; wget -q -O - $MIRROR/apache/kafka/$KAFKA_VERSION/kafka_$SCALA_VERSION-$KAFKA_VERSION.tgz | tar -xzf - -C /opt \\ &amp;amp;&amp;amp; mv /opt/kafka_$SCALA_VERSION-$KAFKA_VERSION /opt/kafka \\ &amp;amp;&amp;amp; sed -i &apos;s/num.partitions.*$/num.partitions=3/g&apos; /opt/kafka/config/server.properties RUN echo &quot;cd /opt/kafka&quot; &amp;gt; /opt/kafka/start.sh &amp;amp;&amp;amp;\\ echo &quot;sed -i &apos;s%zookeeper.connect=.*$%zookeeper.connect=zookeeper:2181%g&apos; /opt/kafka/config/server.properties&quot; &amp;gt;&amp;gt; /opt/kafka/start.sh &amp;amp;&amp;amp;\\ echo &quot;[ ! -z $&quot;&quot;BROKER_ID&quot;&quot; ] &amp;amp;&amp;amp; sed -i &apos;s%broker.id=.*$%broker.id=&apos;$&quot;&quot;BROKER_ID&apos;&quot;&quot;%g&apos; /opt/kafka/config/server.properties&quot; &amp;gt;&amp;gt; /opt/kafka/start.sh &amp;amp;&amp;amp;\\ echo &quot;[ ! -z $&quot;&quot;BROKER_PORT&quot;&quot; ] &amp;amp;&amp;amp; sed -i &apos;s%port=.*$%port=&apos;$&quot;&quot;BROKER_PORT&apos;&quot;&quot;%g&apos; /opt/kafka/config/server.properties&quot; &amp;gt;&amp;gt; /opt/kafka/start.sh &amp;amp;&amp;amp;\\ echo &quot;sed -i &apos;s%#advertised.host.name=.*$%advertised.host.name=&apos;$&quot;&quot;(hostname -i)&apos;&quot;&quot;%g&apos; /opt/kafka/config/server.properties&quot; &amp;gt;&amp;gt; /opt/kafka/start.sh &amp;amp;&amp;amp;\\ echo &quot;[ ! -z $&quot;&quot;ADVERTISED_HOST_NAME&quot;&quot; ] &amp;amp;&amp;amp; sed -i &apos;s%.*advertised.host.name=.*$%advertised.host.name=&apos;$&quot;&quot;ADVERTISED_HOST_NAME&apos;&quot;&quot;%g&apos; /opt/kafka/config/server.properties&quot; &amp;gt;&amp;gt; /opt/kafka/start.sh &amp;amp;&amp;amp;\\ echo &quot;sed -i &apos;s%#host.name=.*$%host.name=&apos;$&quot;&quot;(hostname -i)&apos;&quot;&quot;%g&apos; /opt/kafka/config/server.properties&quot; &amp;gt;&amp;gt; /opt/kafka/start.sh &amp;amp;&amp;amp;\\ echo &quot;[ ! -z $&quot;&quot;HOST_NAME&quot;&quot; ] &amp;amp;&amp;amp; sed -i &apos;s%.*host.name=.*$%host.name=&apos;$&quot;&quot;HOST_NAME&apos;&quot;&quot;%g&apos; /opt/kafka/config/server.properties&quot; &amp;gt;&amp;gt; /opt/kafka/start.sh &amp;amp;&amp;amp;\\ echo &quot;delete.topic.enable=true&quot; &amp;gt;&amp;gt; /opt/kafka/config/server.properties &amp;amp;&amp;amp;\\ echo &quot;bin/kafka-server-start.sh config/server.properties&quot; &amp;gt;&amp;gt; /opt/kafka/start.sh &amp;amp;&amp;amp;\\ chmod a+x /opt/kafka/start.sh EXPOSE 9092 WORKDIR /opt/kafka ENTRYPOINT [&quot;sh&quot;, &quot;/opt/kafka/start.sh&quot;]`&lt;/pre&gt; 2.2启动zookeeper 为简单起见，zookeeper我们暂时使用单个容器。 &lt;pre&gt;`sudo docker build -f zookeeper.Dockerfile -t alex/zookeeper:3.4.6 .`&lt;/pre&gt; 2.3启动Kafka集群 使用下面命令启动三个Kafka容器，分别是kafka0，kafka1，kafka2，我们看到启动容器时传入了BROKER_ID，BROKER_PORT等变量。这就可以很方便的启动多个容器。 &lt;pre&gt;`docker run -itd --name kafka0 -h kafka0 -p9092:9092 -e BROKER_ID=0 -e BROKER_PORT=9092 --link zookeeper alex/kafka_cluster:0.8.2.2 docker run -itd --name kafka1 -h kafka1 -p9093:9092 -e BROKER_ID=1 -e BROKER_PORT=9092 --link zookeeper alex/kafka_cluster:0.8.2.2 docker run -itd --name kafka2 -h kafka2 -p9094:9092 -e BROKER_ID=2 -e BROKER_PORT=9092 --link zookeeper alex/kafka_cluster:0.8.2.2`&lt;/pre&gt; 3.验证是否集群正常 进入kafka0容器内, 创建topic1 &lt;pre&gt;`docker exec -it kafka0 bash bin/kafka-topics.sh --zookeeper zookeeper:2181 --topic topic1 --create --replication-factor 2 --partitions 3 bin/kafka-topics.sh --zookeeper zookeeper:2181 --topic topic1 --describe `&lt;/pre&gt; [![2016-10-22_21-17-05](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/10/2016-10-22_21-17-05.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/10/2016-10-22_21-17-05.jpg) 启动consumer &lt;pre&gt;`bin/kafka-console-consumer.sh --zookeeper zookeeper:2181 --topic topic1`&lt;/pre&gt; 进入kafka2容器内, 启动producer &lt;pre&gt;`bin/kafka-console-producer.sh --broker-list kafka2:9092 --topic topic1`&lt;/pre&gt; [![2016-10-22_21-16-06](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/10/2016-10-22_21-16-06.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/10/2016-10-22_21-16-06.jpg) 发送一些消息，我看到consumer端正常接收。 [![2016-10-22_21-16-26](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/10/2016-10-22_21-16-26.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/10/2016-10-22_21-16-26.jpg) 这就说明集群配置成功。 4.验证min.insync.replicas和request.required.acks的作用。 我们创建topic2，并将min.insync.replicas设为2，partitions为1。 &lt;pre&gt;`bin/kafka-topics.sh --zookeeper zookeeper:2181 --topic topic2 --create --config min.insync.replicas=2 --replication-factor 2 --partitions 1 bin/kafka-topics.sh --zookeeper zookeeper:2181 --topic topic2 --describe `&lt;/pre&gt; [![2016-10-22_23-26-17](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/10/2016-10-22_23-26-17.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/10/2016-10-22_23-26-17.jpg) 我们看到topic2分配在了kafka2上，并且kafka0是备份，ISR是0和2,现在我们将kafka1和kafka2的broker关闭。我们看到leader已自动转为broker0。然后启动consumer &lt;pre&gt;`bin/kafka-console-consumer.sh --zookeeper zookeeper:2181 --topic topic2`&lt;/pre&gt; 在另一个终端上启动producer,并将request-required-acks设置为-1&lt;pre&gt;`bin/kafka-console-producer.sh --broker-list kafka0:9092 --topic topic2 --request-required-acks -1 我们看到此时发送消息，kafka出现错误。原因是我们在producer发送时设置了检查replicas是否都收到数据的确认，但是因为我们已经关闭broker2, 所以尝试发送3次未果后，producer返回错误。显然，我们将producer以默认设置发送消息时，consumer马上收到消息了。","tags":[{"name":"Docker","slug":"Docker","permalink":"http://blog.yaodataking.com/tags/Docker/"},{"name":"Kafka","slug":"Kafka","permalink":"http://blog.yaodataking.com/tags/Kafka/"}]},{"title":"Kafka入门之二:Producer分发策略","date":"2016-10-16T04:41:11.000Z","path":"2016/10/16/kafka-2/","text":"为了更好的实现负载均衡和消息的顺序性，Kafka Producer可以通过分发策略发送给指定的Partition。Kafka保证在partition中的消息是有序的。&bull;一个Partition只分布于一个Broker上（不考虑备份）&bull;一个Partition物理上对应一个文件夹&bull;一个Partition包含多个Segment（Segment对用户透明）&bull;一个Segment对应一个文件&bull;Segment由一个个不可变记录组成&bull;记录只会被append到Segment中，不会被单独删除或者修改&bull;清除过期日志时，直接删除一个或多个Segment。消息被路由到哪个partition上,是有producer客户端决定的.比如客户端采用random,hash及RoundRobin轮询等,如果一个topic中有多个partitions,那么在producer端实现”消息均衡分发”是必要的，producer通过设置partitioner.class的属性来指定向那个分区发送数据。下面我们就实际演示Producer的分发策略。1. 准备工作1.1 启动zookeeper,kafka sudo docker start zookeeper sudo docker start kafka`&lt;/pre&gt; Docker设置方法参见[上文](http://blog.yaodataking.com/2016/09/kafka-1.html)。 1.2 参数修改 将kafka docker中的server.properties配置文件的advertised.host.name改成docker宿主机的IP地址，以便远程客户端访问。 &lt;pre&gt;`advertised.host.name=192.168.199.122`&lt;/pre&gt; [![2016-10-16_0-03-56](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/10/2016-10-16_0-03-56.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/10/2016-10-16_0-03-56.jpg) 2\\. 启动Eclipse(本地windows端) 2.1 新建project 新建一个maven project(Eclipse 安装设置方法参见本博其他文章)。 [![2016-10-15_19-14-47](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/10/2016-10-15_19-14-47.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/10/2016-10-15_19-14-47.jpg) 编辑pom.xml 在下加入以下代码 &lt;pre&gt;`&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.apache.kafka&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;kafka_2.11&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;0.8.2.2&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt;`&lt;/pre&gt; 保存后，eclipse自动获取kafka相关jar文件。 [![2016-10-15_19-29-02](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/10/2016-10-15_19-29-02.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/10/2016-10-15_19-29-02.jpg) 2.2 DemoConsumer &lt;pre lang=&quot;java&quot; line=&quot;1&quot;&gt;package com.alexwu211.kafka.kafa080demo; import java.util.HashMap; import java.util.List; import java.util.Map; import java.util.Properties; import kafka.consumer.Consumer; import kafka.consumer.ConsumerConfig; import kafka.consumer.ConsumerIterator; import kafka.consumer.KafkaStream; import kafka.javaapi.consumer.ConsumerConnector; import kafka.message.MessageAndMetadata; public class DemoConsumer { /** * @param args */ public static void main(String[] args) { args = new String[]{&quot;192.168.199.122:2181&quot;, &quot;topic1&quot;, &quot;group1&quot;, &quot;consumer1&quot;}; if (args == null || args.length != 4) { System.err.print( &quot;Usage:\\n\\tjava -jar kafka_consumer.jar ${zookeeper_list} ${topic_name} ${group_name} ${consumer_id}&quot;); System.exit(1); } String zk = args[0]; String topic = args[1]; String groupid = args[2]; String consumerid = args[3]; Properties props = new Properties(); props.put(&quot;zookeeper.connect&quot;, zk); props.put(&quot;group.id&quot;, groupid); props.put(&quot;autooffset.reset&quot;, &quot;largest&quot;); props.put(&quot;autocommit.enable&quot;, &quot;true&quot;); props.put(&quot;client.id&quot;, &quot;test&quot;); props.put(&quot;auto.commit.interval.ms&quot;, &quot;1000&quot;); ConsumerConfig consumerConfig = new ConsumerConfig(props); ConsumerConnector consumerConnector = Consumer.createJavaConsumerConnector(consumerConfig); Map topicCountMap = new HashMap(); topicCountMap.put(topic, 1); Map&amp;lt;String, List&amp;lt;KafkaStream&amp;gt;&amp;gt; consumerMap = consumerConnector.createMessageStreams(topicCountMap); KafkaStream stream1 = consumerMap.get(topic).get(0); ConsumerIterator it1 = stream1.iterator(); while (it1.hasNext()) { MessageAndMetadata messageAndMetadata = it1.next(); String message = String.format(&quot;Consumer ID:%s, Topic:%s, GroupID:%s, PartitionID:%s, Offset:%s, Message Key:%s, Message Payload: %s&quot;, consumerid, messageAndMetadata.topic(), groupid, messageAndMetadata.partition(), messageAndMetadata.offset(), new String(messageAndMetadata.key()),new String(messageAndMetadata.message())); System.out.println(message); } } }&lt;/pre&gt; 2.3 ProducerDemo 使用RandomPartitioner.class.getName()确保每个producer实例使用单独Partitioner实例。下面的例子使用了两个producer实例。 &lt;pre lang=&quot;java&quot; line=&quot;1&quot;&gt;package com.alexwu211.kafka.kafa080demo; import java.util.ArrayList; import java.util.List; import java.util.Properties; import java.util.Scanner; import kafka.javaapi.producer.Producer; import kafka.producer.KeyedMessage; import kafka.producer.ProducerConfig; import kafka.serializer.StringEncoder; public class ProducerDemo { static private final String TOPIC = &quot;topic1&quot;; static private final String ZOOKEEPER = &quot;192.168.199.122:2181&quot;; static private final String BROKER_LIST = &quot;192.168.199.122:9092&quot;; // static private final int PARTITIONS = TopicAdmin.partitionNum(ZOOKEEPER, TOPIC); static private final int PARTITIONS = 3; public static void main(String[] args) throws Exception { String pt = &quot;RoundRobinShare2&quot;; Producer producer = initProducer(pt); Producer producer1 = initProducer(pt); sendOne(producer,producer1, TOPIC,pt); } private static Producer initProducer(String pt) { Properties props = new Properties(); props.put(&quot;metadata.broker.list&quot;, BROKER_LIST); // props.put(&quot;serializer.class&quot;, &quot;kafka.serializer.StringEncoder&quot;); props.put(&quot;serializer.class&quot;, StringEncoder.class.getName()); if (pt ==&quot;Random&quot; ){ props.put(&quot;partitioner.class&quot;, RandomPartitioner.class.getName()); } if (pt ==&quot;Hash&quot;) { props.put(&quot;partitioner.class&quot;, HashPartitioner.class.getName()); } if (pt ==&quot;RoundRobin&quot;) { props.put(&quot;partitioner.class&quot;, RoundRobinPartitioner.class.getName()); } // props.put(&quot;partitioner.class&quot;, &quot;kafka.producer.DefaultPartitioner&quot;); // props.put(&quot;compression.codec&quot;, &quot;0&quot;); props.put(&quot;producer.type&quot;, &quot;sync&quot;); props.put(&quot;batch.num.messages&quot;, &quot;3&quot;); props.put(&quot;queue.buffer.max.ms&quot;, &quot;10000000&quot;); props.put(&quot;queue.buffering.max.messages&quot;, &quot;1000000&quot;); props.put(&quot;queue.enqueue.timeout.ms&quot;, &quot;20000000&quot;); ProducerConfig config = new ProducerConfig(props); Producer producer = new Producer(config); return producer; } public static void sendOne(Producer producer,Producer producer1,String topic,String pt) throws InterruptedException { KeyedMessage message1 = new KeyedMessage(topic, &quot;31&quot;, pt + &quot; test 31&quot;); producer.send(message1); Thread.sleep(5000); KeyedMessage message2 = new KeyedMessage(topic, &quot;31&quot;, pt + &quot; test 32&quot;); producer1.send(message2); Thread.sleep(5000); KeyedMessage message3 = new KeyedMessage(topic, &quot;31&quot;, pt + &quot; test 33&quot;); producer.send(message3); Thread.sleep(5000); KeyedMessage message4 = new KeyedMessage(topic, &quot;31&quot;, pt + &quot; test 34&quot;); producer1.send(message4); Thread.sleep(5000); KeyedMessage message5 = new KeyedMessage(topic, &quot;31&quot;, pt + &quot; test 35&quot;); producer.send(message5); Thread.sleep(5000); producer.close(); } }&lt;/pre&gt; 2.4 Hash策略 &lt;pre lang=&quot;java&quot; line=&quot;1&quot;&gt;package com.alexwu211.kafka.kafa080demo; import kafka.producer.Partitioner; import kafka.utils.VerifiableProperties; public class HashPartitioner implements Partitioner { public HashPartitioner(VerifiableProperties verifiableProperties) {} public int partition(Object key, int numPartitions) { if ((key instanceof Integer)) { return Math.abs(Integer.parseInt(key.toString())) % numPartitions; } return Math.abs(key.hashCode() % numPartitions); } }&lt;/pre&gt; 我们看到多个producer实例下Key:31会一直发往hash指定的PartitionID:1 &lt;pre&gt;`Consumer ID:consumer1, Topic:topic1, GroupID:group1, PartitionID:1, Offset:16, Message Key:31, Message Payload: Hash test 31 Consumer ID:consumer1, Topic:topic1, GroupID:group1, PartitionID:1, Offset:17, Message Key:31, Message Payload: Hash test 32 Consumer ID:consumer1, Topic:topic1, GroupID:group1, PartitionID:1, Offset:18, Message Key:31, Message Payload: Hash test 33 Consumer ID:consumer1, Topic:topic1, GroupID:group1, PartitionID:1, Offset:19, Message Key:31, Message Payload: Hash test 34 Consumer ID:consumer1, Topic:topic1, GroupID:group1, PartitionID:1, Offset:20, Message Key:31, Message Payload: Hash test 35`&lt;/pre&gt; 2.5 Random策略 &lt;pre lang=&quot;java&quot; line=&quot;1&quot;&gt;package com.alexwu211.kafka.kafa080demo; import java.util.Random; import kafka.producer.Partitioner; import kafka.utils.VerifiableProperties; public class RandomPartitioner implements Partitioner { public RandomPartitioner(VerifiableProperties verifiableProperties) {} public int partition(Object key, int numPartitions) { Random random = new Random(); return random.nextInt(numPartitions); } }&lt;/pre&gt; 由于是Random策略，Key:31会随机发往Partition &lt;pre&gt;`Consumer ID:consumer1, Topic:topic1, GroupID:group1, PartitionID:1, Offset:21, Message Key:31, Message Payload: Random test 31 Consumer ID:consumer1, Topic:topic1, GroupID:group1, PartitionID:2, Offset:15, Message Key:31, Message Payload: Random test 32 Consumer ID:consumer1, Topic:topic1, GroupID:group1, PartitionID:2, Offset:16, Message Key:31, Message Payload: Random test 33 Consumer ID:consumer1, Topic:topic1, GroupID:group1, PartitionID:2, Offset:17, Message Key:31, Message Payload: Random test 34 Consumer ID:consumer1, Topic:topic1, GroupID:group1, PartitionID:1, Offset:22, Message Key:31, Message Payload: Random test 35`&lt;/pre&gt; 2.6 RoundRobin策略 &lt;pre lang=&quot;java&quot; line=&quot;1&quot;&gt;package com.alexwu211.kafka.kafa080demo; import java.util.concurrent.atomic.AtomicLong; import kafka.producer.Partitioner; import kafka.utils.VerifiableProperties; public class RoundRobinPartitioner implements Partitioner { private static AtomicLong next = new AtomicLong(); public RoundRobinPartitioner(VerifiableProperties verifiableProperties) {} public int partition(Object key, int numPartitions) { long nextIndex = next.incrementAndGet(); return (int)nextIndex % numPartitions; } }&lt;/pre&gt; 我们注意到我们使用了AtomicLong来保证多实例共享下线程的安全，关于Atomic的作用，可以参见[AtomicInteger在实际项目中的应用](http://haininghacker-foxmail-com.iteye.com/blog/1401346)。我们看到多个producer实例下Key:31会轮流发往topic1可用的Partition。 &lt;pre&gt;`Consumer ID:consumer1, Topic:topic1, GroupID:group1, PartitionID:0, Offset:14, Message Key:31, Message Payload: RoundRobin test 31 Consumer ID:consumer1, Topic:topic1, GroupID:group1, PartitionID:1, Offset:29, Message Key:31, Message Payload: RoundRobin test 32 Consumer ID:consumer1, Topic:topic1, GroupID:group1, PartitionID:2, Offset:21, Message Key:31, Message Payload: RoundRobin test 33 Consumer ID:consumer1, Topic:topic1, GroupID:group1, PartitionID:0, Offset:15, Message Key:31, Message Payload: RoundRobin test 34 Consumer ID:consumer1, Topic:topic1, GroupID:group1, PartitionID:1, Offset:30, Message Key:31, Message Payload: RoundRobin test 35`&lt;/pre&gt; 3\\. 验证Partitioner的实例个数及其是否需要保证线程安全 从第2步各种分发策略的实验可知，当指定使用自定义的Partitioner实现后，Producer会使用该实现来作路由决策（即决定消息应该发送到哪个Broker上的哪个Partition）。这就涉及到该类如果是一个实例被共享，需要考虑线程安全的问题，以上我们使用AtomicLong来保证多实例共享下线程的安全。那么如果不使用AtomicLong的情况下，会发生怎样的情况呢? 我们增加一个RoundRobin2策略来演示在共享实例情况下使用和不使用AtomicLong的结果。 &lt;pre lang=&quot;java&quot; line=&quot;1&quot;&gt;package com.alexwu211.kafka.kafa080demo; import kafka.producer.Partitioner; import kafka.utils.VerifiableProperties; public class RoundRobinPartitioner2 implements Partitioner { private int i = 0; public RoundRobinPartitioner2(VerifiableProperties verifiableProperties) {} public int partition(Object key, int numPartitions) { long nextIndex = i++; return (int)nextIndex % numPartitions; } }&lt;/pre&gt; 相应的在preducerdemo增加以下代码,RoundRobinShare使用AtomicLong，RoundRobinShare2不使用AtomicLong。 &lt;pre lang=&quot;java&quot;&gt;if (pt ==&quot;RoundRobinShare&quot;) { props.put(&quot;partitioner.class&quot;, &quot;com.alexwu211.kafka.kafa080demo.RoundRobinPartitioner&quot;); } if (pt ==&quot;RoundRobinShare2&quot;) { props.put(&quot;partitioner.class&quot;, &quot;com.alexwu211.kafka.kafa080demo.RoundRobinPartitioner2&quot;); }&lt;/pre&gt; 在使用AtomicLong情况下，轮询正常。 &lt;pre&gt;`Consumer ID:consumer1, Topic:topic1, GroupID:group1, PartitionID:1, Offset:61, Message Key:31, Message Payload: RoundRobinShare test 31 Consumer ID:consumer1, Topic:topic1, GroupID:group1, PartitionID:2, Offset:47, Message Key:31, Message Payload: RoundRobinShare test 32 Consumer ID:consumer1, Topic:topic1, GroupID:group1, PartitionID:0, Offset:36, Message Key:31, Message Payload: RoundRobinShare test 33 Consumer ID:consumer1, Topic:topic1, GroupID:group1, PartitionID:1, Offset:62, Message Key:31, Message Payload: RoundRobinShare test 34 Consumer ID:consumer1, Topic:topic1, GroupID:group1, PartitionID:2, Offset:48, Message Key:31, Message Payload: RoundRobinShare test 35`&lt;/pre&gt; 在不使用AtomicLong情况下，轮询失败。 &lt;pre&gt;`Consumer ID:consumer1, Topic:topic1, GroupID:group1, PartitionID:0, Offset:37, Message Key:31, Message Payload: RoundRobinShare2 test 31 Consumer ID:consumer1, Topic:topic1, GroupID:group1, PartitionID:0, Offset:38, Message Key:31, Message Payload: RoundRobinShare2 test 32 Consumer ID:consumer1, Topic:topic1, GroupID:group1, PartitionID:1, Offset:63, Message Key:31, Message Payload: RoundRobinShare2 test 33 Consumer ID:consumer1, Topic:topic1, GroupID:group1, PartitionID:1, Offset:64, Message Key:31, Message Payload: RoundRobinShare2 test 34 Consumer ID:consumer1, Topic:topic1, GroupID:group1, PartitionID:2, Offset:49, Message Key:31, Message Payload: RoundRobinShare2 test 35`&lt;/pre&gt; 结论，由此我们看到实例被共享时，需要一些手段保证线程的安全。 4\\. 同步异步的参数 上面的例子我们都使用了同步的方法，也就是实时发送，但是如果遇到IO操作等耗时操作时并且不需要让程序等待对方返回，我们可以使用异步发送。异步的好处很明显的，异步可以增加客户体验，可以释放占用资源从而提高系统性能。 kafka中可以使用producer.type参数设置同步还是异步(async/sync),默认是sync。 下面是其它一些相关参数 &lt;pre&gt;`batch.num.messages 异步发送 每次批量发送的条目 queue.buffering.max.ms 异步发送的时候 发送时间间隔 单位是毫秒 queue.buffering.max.messages 每次最大的提交量 queue.enqueue.timeout.ms 0代表队列没满的时候直接入队，满了立即扔弃，-1代表无条件阻塞且不丢弃","tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://blog.yaodataking.com/tags/Kafka/"}]},{"title":"数据湖：提供大数据解决方案的架构","date":"2016-10-15T04:50:02.000Z","path":"2016/10/15/data-lake-intro/","text":"自从Dixon James在2010年提出数据湖(data lake)这个概念以来(原文见 “Pentaho, Hadoop, and Data Lakes”)，数据湖逐渐成为各大厂商提供大数据解决方案的架构。 那么什么是数据湖呢？数据湖具有灵活的定义。 其核心是一个数据存储和处理存储库，其中可以放置组织中的所有数据，以便每个内部和外部系统，合作伙伴和协作者的数据流入其中，并输出分析结果。 下面的列表详细说明了数据湖是什么：&bull;数据湖是一个巨大的资源库，各种数据保存其原始格式，直到组织中的任何人需要分析。&bull;数据湖不是Hadoop。它使用各种不同的工具。Hadoop只是实现的功能的一个子集。&bull;数据湖不是传统字面上理解的数据库。典型的数据湖实施使用各种NoSQL和内存数据库，可以与它的关系并存。&bull;数据湖不能孤立地实施。它必须与数据仓库一起实施因为它补充了数据仓库的各种功能。&bull;它存储大量的非结构化和结构化数据。它还存储来自机器传感器的快速移动的数据流数据和日志。&bull;它提供一种Store-All方法来处理大量数据。&bull;它针对具有高延迟批处理模式的数据处理进行了优化，其不适用于事务处理。&bull;它有助于创建灵活的数据模型，并可以修改，而不需要数据库重新设计。&bull;它可以快速地进行数据丰富化，有助于实现数据的增强，增加，分类和标准化。&bull;所有存储在数据湖中的数据可用于获得全包视图， 这有助于生成多维模型。&bull;这是一个数据科学家最喜欢的狩猎场。 他可以最细粒度地访问存储在其原始光荣中的数据，以便他可以执行任何特别查询，并且可以随时迭代地构建高级模型。经典的数据仓库的方法不支持这种在数据采集和洞察生成之间缩减处理时间的能力。 优势：&bull;尽可能地缩放&bull;提供不同的数据源的插件&bull;获取高速数据&bull;处理非结构化数据&bull;以原生格式存储&bull;不要担心模式&bull;使用您最喜欢的SQL&bull;提供高级算法&bull;节省人力资源 挑战：数据湖是一个复杂的解决方案，因为构建它涉及到许多层，每个层利用大量的大数据工具和技术来完成其功能。 这需要在部署，管理和维护方面的大量工作。 另一个重要方面是数据治理; 由于数据湖旨在将所有组织的数据集中在一起，因此它应该建立起足够的治理，使其不会变成一组无关的数据孤岛。 推荐阅读：Data Lake Development with Big DataData Lake Architecture: Designing the Data Lake and Avoiding the Garbage DumpAzure Data LakeData Lake on AWS","tags":[{"name":"big data","slug":"big-data","permalink":"http://blog.yaodataking.com/tags/big-data/"},{"name":"data lake","slug":"data-lake","permalink":"http://blog.yaodataking.com/tags/data-lake/"},{"name":"数据湖","slug":"数据湖","permalink":"http://blog.yaodataking.com/tags/数据湖/"}]},{"title":"Redis加速你的WordPress站点","date":"2016-10-10T12:25:15.000Z","path":"2016/10/10/redis-wordpress/","text":"Redis是一个开源的内存存储的数据结构服务器，可用作数据库，高速缓存和消息队列代理。WordPress是一种使用PHP语言开发的博客平台，用户可以在支持PHP和MySQL数据库的服务器上架设属于自己的网站。也可以把 WordPress当作一个内容管理系统（CMS）来使用。WordPress简单又功能强大让大家爱不释手，但也导致了WordPress在架构大型网站和博客时成为了消耗资源“大户”，如何让Wordpress更好更有效率地运行，是我们一直不断追求的目标。那么是否可用Redis来做Wordpress的缓存来加速访问呢？答案是肯定的。步骤也非常简单。1.安装Redis首先要安装Redis。访问Redis官网,下载源代码，并编译。 wget http://download.redis.io/releases/redis-3.2.4.tar.gz tar xzf redis-3.2.4.tar.gz cd redis-3.2.4 make`&lt;/pre&gt; 安装过程中，如果发现没有编译器，可安装gcc， &lt;pre&gt;`sudo yum install gcc`&lt;/pre&gt; 2.设置Redis 建立redis命令，配置，log等文件存放的文件夹。 &lt;pre&gt;`sudo mkdir -p /usr/local/redis/{bin,var,etc} cd src/ sudo cp redis-benchmark redis-check-aof redis-check-rdb redis-cli redis-sentinel redis-server /usr/local/redis/bin/ sudo cp ../redis.conf /usr/local/redis/etc sudo ln -s /usr/local/redis/bin/* /usr/bin/`&lt;/pre&gt; 编辑配置文件 &lt;pre&gt;`sudo vim /usr/local/redis/etc/redis.conf`&lt;/pre&gt; 设置以下参数 &lt;pre&gt;`daemonize yes bind 127.0.0.1 logfile /usr/local/redis/var/redis.log dir /usr/local/redis/var `&lt;/pre&gt; 3.编辑Redis启动文件 下载初始配置文件,复制到/etc/init.d &lt;pre&gt;`sudo wget https://raw.github.com/saxenap/install-redis-amazon-linux-centos/master/redis-server sudo mv redis-server /etc/init.d sudo chmod 755 /etc/init.d/redis-server`&lt;/pre&gt; 编辑文件&lt;pre&gt;`sudo vim /etc/init.d/redis-server`&lt;/pre&gt; 修改redis的文件路径，这个路径就是我们第2步复制的地方。 &lt;pre&gt;`redis=&quot;/usr/local/redis/bin/redis-server&quot;`&lt;/pre&gt; 4.设置Redis开机启动 接下来我们设置开机启动，并启动Redis。 &lt;pre&gt;`sudo chkconfig --add redis-server sudo chkconfig --level 345 redis-server on sudo service redis-server restart`&lt;/pre&gt; 5.Redis验证 [![redis-test1](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/10/redis-test1.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/10/redis-test1.jpg) 6.安装Redis的PHP客户端 为了使WordPress的PHP与Redis通信，必须安装Redis的PHP客户端，主要有三种predis，phpredis和Rediska。这里采用predis。 下载[predis.php](https://uploads.staticjw.com/ji/jim/predis.php)并上传至网站wordpress的根目录。并且使用[index-with-redis.php](index-with-redis.php)替换index.php文件 &lt;pre&gt;`sudo wget http://uploads.staticjw.com/ji/jim/predis.php sudo chown nginx:nginx predis.php sudo wget https://gist.githubusercontent.com/JimWestergren/3053250/raw/d9e279e31cbee4a1520f59108a4418ae396b2dde/index-with-redis.php sudo chown nginx:nginx index-with-redis.php sudo mv index.php index.php.bak sudo mv index-with-redis.php index.php`&lt;/pre&gt; 注意根据需要修改以下参数值。 &lt;pre&gt;`$cf = 0; // set to 1 if you are using cloudflare $debug = 1; // set to 1 if you wish to see execution time and cache actions $display_powered_by_redis = 0; // set to 1 if you want to display a powered by redis message with execution time, see below 7.测试效果所有工作完成，你可以刷新你的网页，查看一下效果。如果你想在页面上看到脚本执行时间和缓存加载时间，请设置$debug = 1; 浏览器最下方第一次会显示cache is set:0.23617,接下来访问你会看到速度明显快了，浏览器最下方会显示this is a cache:0.00418,如果你用F5刷新，cache会清除。浏览器最下方会显示cache of page deleted:0.24268， 如果再访问会重新cache。","tags":[{"name":"Redis","slug":"Redis","permalink":"http://blog.yaodataking.com/tags/Redis/"},{"name":"WordPress","slug":"WordPress","permalink":"http://blog.yaodataking.com/tags/WordPress/"}]},{"title":"《罗马人的故事12:迷途帝国》读书笔记","date":"2016-10-06T08:08:26.000Z","path":"2016/10/06/rome-story-12/","text":"康茂德的暴政，虽然开启了罗马帝国的衰落之门，但是如果没有卡拉卡拉的激进政策，以及其他皇帝的昏招频出， 三世纪的危机或许还可以能够以不同方式化解。皇帝卡拉卡拉颁布的“安东尼努斯敕令”，无条件地赋予了行省居民以罗马公民权。这一“慷慨”之举不仅给帝国财政带来了灾难，而且使各社会阶层的流动性不存在了，帝国失去了活力。由此看来，一个善意出发点的决定也会导致糟糕的结果。蛮族的入侵，经济的衰退，政局的动荡以及基督教的崛起，必须要在一个才能出众的皇帝的带领下来逐一克服。而73年中，22位皇帝的频繁更迭给帝国的统治造成了非常深远的影响。奥勒良或者给罗马带来了一丝希望，重新统一了帝国，然而历史没有给奥勒良更长时间的机会。随着奥勒良和普罗普斯这样有贤帝风范的皇帝的被刺杀，罗马帝国终于在迷途中越陷越远。罗马的衰退，带来的后果是罗马的诸神再也无法给罗马人带来帮助与指明方向。蛮族的入侵，家园的破坏，疾病的流行，贫穷的烦恼甚至连死亡都是基督教神的旨意，也是神对人类的考验。于是越来越多的罗马人加入了基督教，包括曾经拥有非常坚定的现实主义世界观的罗马知识分子。","tags":[{"name":"罗马","slug":"罗马","permalink":"http://blog.yaodataking.com/tags/罗马/"}]},{"title":"《增长黑客》思维导图","date":"2016-09-28T12:37:26.000Z","path":"2016/09/28/growthhacker/","text":"","tags":[]},{"title":"《罗马人的故事11:结局的开始》读书笔记","date":"2016-09-27T16:20:24.000Z","path":"2016/09/28/rome-story-11/","text":"在阅读本册前，我特地先阅读了马可奥勒留的《沉思录》，的确作为斯多葛学派晚期的代表人物，皇帝马可奥勒留在这方面的成就令后人赞叹。这里简单介绍一下什么是斯多葛学派，斯多葛学派作为希腊哲学派别之一，属于伦理学范畴，主要有三个主要主张：“美德即幸福”、“情感源于主观判断”、“顺应自然地生活”。《沉思录》这部马可奥勒留写给自己的书，是自己与自己的对话，大部分是在鞍马劳顿中写成的，当然这些都是他对当时社会及环境深思的结果。也许是马可奥勒留觉得自己生活在一个个人无能为力的时代，生活在一个混乱的世界上，所以对超出力量范围的事情，但是符合宇宙理性的，必须欣然接受它们，比如个人的失意、痛苦、疾病、死亡甚至社会上的丑恶现象等等；对于力量范围之内的事情，就按照本性生活，做一个正直、高尚、有道德的人。诚然，《沉思录》有一种不可思议的魅力，甜美、忧郁和高贵。作为哲学家，马可奥勒留完成了使命，但是作为皇帝及父亲，马可奥勒留还是有欠缺的，主要表现在，1.在两帝并立事件，出发点是好的，但是没有发挥应有功能，对政务军务完全不入门的路奇乌斯是一个好的选择吗？2.把13岁的女儿嫁给33岁的路奇乌斯是否妥当？3.把”奥古斯塔“的尊称授予女儿是否欠缺考虑？因为20年后这个称号成为宫廷内斗的导火索。4.对唯一的儿子康茂德的教育是否也是顺应本性？当然，马可奥勒留发现哪些不足之处立即努力弥补还是令人赞叹的，这也是马可奥勒留被当作贤君的原因 。帝国的军队在半个世纪的和平之后，重新恢复了战斗力。然而，就像马可奥勒留预感的那样，帝国的衰落已经是无可奈何的事情，而他的儿子康茂德的暴政无疑加深了后人对他的看法。","tags":[{"name":"罗马","slug":"罗马","permalink":"http://blog.yaodataking.com/tags/罗马/"}]},{"title":"Kafka入门之一:使用Docker安装Kafka和Zookeeper","date":"2016-09-25T01:41:33.000Z","path":"2016/09/25/kafka-1/","text":"1.介绍Kafka是一个分布式的、可分区的、可复制的提交日志服务。它提供了消息系统功能，但具有自己独特的设计。几个基本的消息系统术语：Broker：Kafka集群包含一个或多个服务器，这种服务器被称为broker。Topic:每条发布到Kafka集群的消息都有一个类别，这个类别被称为Topic。（物理上不同Topic的消息分开存储，逻辑上一个Topic的消息虽然保存于一个或多个broker上但用户只需指定消息的Topic即可生产或消费数据而不必关心数据存于何处）Partition:Partition是物理上的概念，每个Topic包含一个或多个Partition.Producer:负责发布消息到Kafka brokerConsumer:消息消费者，向Kafka broker读取消息的客户端。Consumer Group:每个Consumer属于一个特定的Consumer Group（可为每个Consumer指定group name，若不指定group name则属于默认的group）。producers通过网络将消息发送到Kafka集群，集群向消费者提供消息，如下图所示：Topics主题和Logs 日志先来看一下Kafka提供的一个抽象概念:topic.一个topic是对一组消息的归纳。对每个topic，Kafka 对它的日志进行了分区，如下图所示：每个分区都由一系列有序的、不可变的消息组成，这些消息被连续的追加到分区中。分区中的每个消息都有一个连续的序列号叫做offset,用来在分区中唯一的标识这个消息。在一个可配置的时间段内，Kafka集群保留所有发布的消息，不管这些消息有没有被消费。比如，如果消息的保存策略被设置为2天，那么在一个消息被发布的两天时间内，它都是可以被消费的。之后它将被丢弃以释放空间。Kafka的性能是和数据量大小是无关的，所以保留太多的数据并不是问题。实际上每个consumer唯一需要维护的数据是消息在日志中的位置，也就是offset.这个offset有consumer来维护：一般情况下随着consumer不断的读取消息，这offset的值不断增加，但其实consumer可以以任意的顺序读取消息，比如它可以将offset设置成为一个旧的值来重读之前的消息。以上特点的结合，使Kafka consumers非常的轻量级：它们可以在不对集群和其他consumer造成影响的情况下读取消息。你可以使用命令行来”tail”消息而不会对其他正在消费消息的consumer造成影响。将日志分区可以达到以下目的：首先这使得每个日志的数量不会太大，可以在单个服务上保存。另外每个分区可以单独发布和消费，为并发操作topic提供了一种可能。Distribution分布式每个分区在Kafka集群的若干服务中都有副本，这样这些持有副本的服务可以共同处理数据和请求，副本数量是可以配置的。副本使Kafka具备了容错能力。每个分区都由一个服务器作为“leader”，零或若干服务器作为“followers”,leader负责处理消息的读和写，followers则去复制leader.如果leader down了，followers中的一台则会自动成为leader。集群中的每个服务都会同时扮演两个角色：作为它所持有的一部分分区的leader，同时作为其他分区的followers，这样集群就会据有较好的负载均衡。Producers生产者Producer将消息发布到它指定的topic中,并负责决定发布到哪个分区。通常简单的由负载均衡机制随机选择分区，但也可以通过特定的分区函数选择分区。使用的更多的是第二种。Consumers消费者发布消息通常有两种模式：队列模式（queuing）和发布-订阅模式(publish-subscribe)。队列模式中，consumers可以同时从服务端读取消息，每个消息只被其中一个consumer读到；发布-订阅模式中消息被广播到所有的consumer中。Consumers可以加入一个consumer 组，共同竞争一个topic，topic中的消息将被分发到组中的一个成员中。同一组中的consumer可以在不同的程序中，也可以在不同的机器上。如果所有的consumer都在一个组中，这就成为了传统的队列模式，在各consumer中实现负载均衡。如果所有的consumer都不在不同的组中，这就成为了发布-订阅模式，所有的消息都被分发到所有的consumer中。更常见的是，每个topic都有若干数量的consumer组，每个组都是一个逻辑上的“订阅者”，为了容错和更好的稳定性，每个组由若干consumer组成。这其实就是一个发布-订阅模式，只不过订阅者是个组而不是单个consumer。由两个机器组成的集群拥有4个分区 (P0-P3) 2个consumer组. A组有两个consumerB组有4个。相比传统的消息系统，Kafka可以很好的保证有序性。传统的队列在服务器上保存有序的消息，如果多个consumers同时从这个服务器消费消息，服务器就会以消息存储的顺序向consumer分发消息。虽然服务器按顺序发布消息，但是消息是被异步的分发到各consumer上，所以当消息到达时可能已经失去了原来的顺序，这意味着并发消费将导致顺序错乱。为了避免故障，这样的消息系统通常使用“专用consumer”的概念，其实就是只允许一个消费者消费消息，当然这就意味着失去了并发性。在这方面Kafka做的更好，通过分区的概念，Kafka可以在多个consumer组并发的情况下提供较好的有序性和负载均衡。将每个分区分只分发给一个consumer组，这样一个分区就只被这个组的一个consumer消费，就可以顺序的消费这个分区的消息。因为有多个分区，依然可以在多个consumer组之间进行负载均衡。注意consumer组的数量不能多于分区的数量，也就是有多少分区就允许多少并发消费。Kafka只能保证一个分区之内消息的有序性，在不同的分区之间是不可以的，这已经可以满足大部分应用的需求。如果需要topic中所有消息的有序性，那就只能让这个topic只有一个分区，当然也就只有一个consumer组消费它。 2. 实例演示这里我们使用Docker 来分别安装演示Kafka和Zookeeper。2.1 制作dockerfile2.1.1 Zookeeper首先制作Zookeeper的dockerfile，这里采用java:openjdk-8-jre-alpine作为源镜像，zookeeper版本3.4.6，使用国内镜像源阿里云。 FROM java:openjdk-8-jre-alpine ARG MIRROR=http://mirrors.aliyun.com/ ARG VERSION=3.4.6 LABEL name=&quot;zookeeper&quot; version=$VERSION RUN apk update &amp;amp;&amp;amp; apk add ca-certificates &amp;amp;&amp;amp; \\ apk add tzdata &amp;amp;&amp;amp; \\ ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime &amp;amp;&amp;amp; \\ echo &quot;Asia/Shanghai&quot; &amp;gt; /etc/timezone RUN apk add --no-cache wget bash \\ &amp;amp;&amp;amp; mkdir /opt \\ &amp;amp;&amp;amp; wget -q -O - $MIRROR/apache/zookeeper/zookeeper-$VERSION/zookeeper-$VERSION.tar.gz | tar -xzf - -C /opt \\ &amp;amp;&amp;amp; mv /opt/zookeeper-$VERSION /opt/zookeeper \\ &amp;amp;&amp;amp; cp /opt/zookeeper/conf/zoo_sample.cfg /opt/zookeeper/conf/zoo.cfg \\ &amp;amp;&amp;amp; mkdir -p /tmp/zookeeper EXPOSE 2181 WORKDIR /opt/zookeeper VOLUME [&quot;/opt/zookeeper/conf&quot;, &quot;/tmp/zookeeper&quot;] ENTRYPOINT [&quot;/opt/zookeeper/bin/zkServer.sh&quot;] CMD [&quot;start-foreground&quot;]`&lt;/pre&gt; 2.1.2 Kafka 同样Kafka的dockerfile 如下,kafka版本采用0.8.2.2 &lt;pre&gt;`FROM java:openjdk-8-jre-alpine ARG MIRROR=http://mirrors.aliyun.com/ ARG SCALA_VERSION=2.11 ARG KAFKA_VERSION=0.8.2.2 LABEL name=&quot;kafka&quot; version=$VERSION RUN apk update &amp;amp;&amp;amp; apk add ca-certificates &amp;amp;&amp;amp; \\ apk add tzdata &amp;amp;&amp;amp; \\ ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime &amp;amp;&amp;amp; \\ echo &quot;Asia/Shanghai&quot; &amp;gt; /etc/timezone RUN apk add --no-cache wget bash \\ &amp;amp;&amp;amp; mkdir /opt \\ &amp;amp;&amp;amp; wget -q -O - $MIRROR/apache/kafka/$KAFKA_VERSION/kafka_$SCALA_VERSION-$KAFKA_VERSION.tgz | tar -xzf - -C /opt \\ &amp;amp;&amp;amp; mv /opt/kafka_$SCALA_VERSION-$KAFKA_VERSION /opt/kafka \\ &amp;amp;&amp;amp; sed -i &apos;s/num.partitions.*$/num.partitions=3/g&apos; /opt/kafka/config/server.properties \\ &amp;amp;&amp;amp; sed -i &apos;s/zookeeper.connect=.*$/zookeeper.connect=zookeeper:2181/g&apos; /opt/kafka/config/server.properties EXPOSE 9092 ENTRYPOINT [&quot;/opt/kafka/bin/kafka-server-start.sh&quot;] CMD [&quot;/opt/kafka/config/server.properties&quot;]`&lt;/pre&gt; 2.2 Build dockerfile 分别build zookeeper和kafka，build过程略，因为采用了国内镜像源，速度还是比较快的。 &lt;pre&gt;`sudo docker build -f zookeeper.Dockerfile -t alex/zookeeper:3.4.6 .`&lt;/pre&gt; &lt;pre&gt;`sudo docker build -f kafka.Dockerfile -t alex/kafka:0.8.2.2 .`&lt;/pre&gt; 完成的镜像文件如下，我们看到由于采用了java:openjdk-8-jre-alpine，整个镜像还是比较小的。 [![2016-09-25_7-42-04](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/09/2016-09-25_7-42-04.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/09/2016-09-25_7-42-04.jpg) 2.3 启动 zookeeper &lt;pre&gt;`sudo docker run --name zookeeper -itd -p2181:2181 alex/zookeeper:3.4.6`&lt;/pre&gt; 2.4 启动 kafka 这里要注意link zookeeper容器。 &lt;pre&gt;`sudo docker run --name kafka -itd -p9092:9092 --link zookeeper alex/kafka:0.8.2.2`&lt;/pre&gt; 检查端口是否都已启动。 [![2016-09-25_8-56-19](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/09/2016-09-25_8-56-19.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/09/2016-09-25_8-56-19.jpg) 如果没有看到，你可能要使用以下命令检查容器启动的logs sudo docker logs zookeeper sudo docker logs kafka 2.5 验证1 进入kafka容器，创建两个topics，分别叫test1，test2 &lt;pre&gt;`sudo docker exec -it kafka bash cd /opt/kafka source /root/.bash_profile bin/kafka-topics.sh --create --topic test1 --zookeeper zookeeper:2181 --partition 3 --replication-factor 1 bin/kafka-topics.sh --create --topic test2 --zookeeper zookeeper:2181 --partition 3 --replication-factor 1 bin/kafka-topics.sh --describe --topic test1 --zookeeper zookeeper:2181 bin/kafka-topics.sh --describe --topic test2 --zookeeper zookeeper:2181 bin/kafka-topics.sh --list --zookeeper zookeeper:2181`&lt;/pre&gt; [![2016-09-25_7-16-28](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/09/2016-09-25_7-16-28.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/09/2016-09-25_7-16-28.jpg) 启动consumer,以后我把这里叫做consumer端，以示区分。 &lt;pre&gt;`bin/kafka-console-consumer.sh --zookeeper zookeeper:2181 --topic test1`&lt;/pre&gt; 另外开启一个终端(producer端)，进入kafka容器，启动producer，并发送几条消息。 &lt;pre&gt;`sudo docker exec -it kafka bash cd /opt/kafka source /root/.bash_profile bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test1`&lt;/pre&gt; [![2016-09-25_7-18-07](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/09/2016-09-25_7-18-07.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/09/2016-09-25_7-18-07.jpg) 我们看到cunsumer端，接收到了这些消息。 [![2016-09-25_7-18-34](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/09/2016-09-25_7-18-34.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/09/2016-09-25_7-18-34.jpg) 2.6 验证2 接下来我们将演示消息转发的功能，将我们刚才在topic test1输入的消息转发给topic test2。 在cunsumer端，启动接收test2 &lt;pre&gt;`bin/kafka-console-consumer.sh --zookeeper zookeeper:2181 --topic test2`&lt;/pre&gt; 在producer端，使用kafka-replay-log-producer.sh将test1的消息转发给test2 &lt;pre&gt;`bin/kafka-replay-log-producer.sh --broker-list localhost:9092 --zookeeper zookeeper:2181 --inputtopic test1 --outputtopic test2 我们看到cunsumer的test2成功接收到了之前test1的消息，而我刚才并没有重发这些消息，这也验证了kafka的消息是持久化的。 参考Jason老师的博客kafka官网Kafka入门经典教程","tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"http://blog.yaodataking.com/tags/zookeeper/"},{"name":"Kafka","slug":"Kafka","permalink":"http://blog.yaodataking.com/tags/Kafka/"}]},{"title":"《罗马人的故事10:条条大路通罗马》读书笔记","date":"2016-09-10T05:25:05.000Z","path":"2016/09/10/rome-story-10-html/","text":"不管历史如何变迁，古罗马人硬件基础设施建设的成就永远令世人赞叹，尤其是罗马大道，光干道总长就达8万公里。条条大路通罗马这句话，已经是人人皆知的一句谚语。那么古罗马人为什么这么执着于基础设施建设并且使之网络化呢？从阿皮亚大道开始，古罗马人就认识到大道对于军队快速调遣的好处，同时把道路修建到刚征服的地方，对于同化被征服者，也是一个正确的选择。不仅如此，道路对于罗马人的日常生活，贸易提供了极大的便利，促进了经济的极大发展。正因为如此重要，完善基础设施建设才列为每一任罗马皇帝的三大职责之一。相比之下，罗马的教育，医疗等软件基础设施，没有给人留下深刻印象，也许这些由于历史变迁无法随时代流传下来的缘故。不管怎样，罗马的基础设施建设还是带给我们不少启示，要致富，先修路，就是当今中国改革之初最朴素的应用。","tags":[{"name":"罗马","slug":"罗马","permalink":"http://blog.yaodataking.com/tags/罗马/"}]},{"title":"《工业大数据：工业4.0时代的工业转型与价值创造》读书笔记","date":"2016-09-10T04:04:07.000Z","path":"2016/09/10/industry-4-0-jaylee/","text":"这是一本工业4.0的科普书，也给读者提供了一个新的视角，并为工业4.0的实践者们提供方向性的参考。纵观本书，作者重新理解了老子《道德经》里的 “有之以为利，无之以为用”这句话。一切事物的实体为我们提供可以凭借的、可见的基础条件，而其中所隐藏的空间和可变化的无限可能才是被我们真正使用并创造价值的所在。基于此理解，不可见因素的避免和透明呈现才是工业4.0的关注点和竞争点。而工业4.0的价值所在就是发现用户价值的缺口、发现和管理不可见的问题、实现无忧的生产环境，以及为用户提供定制化的产品和服务。对于每个人来说，未来的机会空间都在以下四个维度。1.满足用户可见的需求和解决可见的问题。2.使用数据挖掘避免可见的问题。3.利用创新方法与技术解决不可见的问题。4.利用数据分析寻找和满足不可见的价值缺口。实现这些，最重要的是需要改变以往从技术端出发看问题的思维，反向思考，从用户的价值端寻找潜在的需求，学会思维的转变。","tags":[{"name":"工业4.0","slug":"工业4-0","permalink":"http://blog.yaodataking.com/tags/工业4-0/"}]},{"title":"《罗马人的故事9:贤君的世纪》读书笔记","date":"2016-08-21T05:04:19.000Z","path":"2016/08/21/rome-story-9/","text":"从涅尔瓦开始，罗马迎来了“五贤帝”时代。统治罗马一年零四个月的涅尔瓦最大的任务是确定下一任皇帝人选，如果设身处地站在涅尔瓦的角度，选择行省出生的图拉真是不是有点冒险？历史已经没有假设， 历史证明涅尔瓦的抉择是正确的。图拉真不仅完成了“罗马皇帝的三大职责”，1.保障边境安全，2.维护国内政治秩序，3.完善基础设施建设，而且扩大了罗马帝国的版图。当然，把达契亚民族彻底从其原住地清除出去，个人觉得这是图拉真武功的一个污点。虽然哈德良时期没有外部的侵略，但是他的文治基本上在各行省的巡视中完成的。不得不提，由于犹太人的叛乱，哈德良把犹太人永远的赶出了耶路撒冷，这也是有失商榷的。从奥古斯都以来，罗马皇帝对犹太人的政策无疑是失败的，罗马人永远无法理解犹太人。从罗马人的角度来讲，我信仰多神，所以我不排斥你的一神信仰，我给你宽容，我给你自由，但是对于一神教的犹太人来说，依然认为自己没有自由。我在想，这带给现在的中国有什么样的启示呢？从幸福指数来说，安敦尼庇护无疑是最幸福的皇帝，罗马皇帝的三大职责基本上图拉真和哈德良都做完了，连继承人哈德良都已经指定了。安敦尼非常满足于对已有建筑、大道、桥梁和引水渠等进行修缮维护的工作。 然而，“不作为”并不是无所作为，从后世称为“哲学家皇帝”的马可奥勒留是这样写的父亲，“决定一件事情时，要慎重、稳健，同时要有持久的恒心；不追逐社会名利而热爱工作，对工作要有耐心；。。。。。。对任何事情他都要经过深思熟虑，。。。。。。”。那么马可奥勒留自己是怎么做皇帝的呢？这样想着，我非常急切的想看关于马可奥勒留的历史及他的《沉思录》。","tags":[{"name":"罗马","slug":"罗马","permalink":"http://blog.yaodataking.com/tags/罗马/"}]},{"title":"《罗马人的故事8:危机与克服》读书笔记","date":"2016-08-14T05:10:53.000Z","path":"2016/08/14/rome-story-8/","text":"虽然罗马人对尼禄的死拍手称快，但是罗马人显然没有准备好谁可以做下一任好皇帝，罗马出现了短暂的危机。经历了尤里乌斯-克劳狄乌斯王朝的统治 ，元老院已经没有了能力指定谁为下一任皇帝，成为皇帝的三个条件，正当性，权威，能力，正当性已不复存在，现在只能看权威与能力了。从 公元68年夏天至公元69年底一年间推举出的三个皇帝，加尔巴、奥托、维特里乌斯虽然是军团负责人具有一定的权威，但是都不是治国之才，没有雄才大略，也不会使用人才，其结局不是自杀就是被杀。在克劳狄乌斯时期就显露头角的韦斯帕芗终于意识到，他可以再次开创罗马帝国的辉煌，通过团队积极的策略，稳扎稳打，韦斯帕芗成功登基了，于是罗马帝国进入了弗拉维王朝时代。这里不得不提一下，穆奇阿努斯，他的存在就像开国皇帝奥古斯都手下的梅塞纳斯。韦斯帕芗的两个儿子提图斯和图密善相继继承了皇位。历史上著名的维苏威火山大爆发就发生在提图斯执政时期。总的来说，弗拉维王朝最大的功绩是缓解了罗马帝国的危机，让帝国重新步入了正轨，并且通过实施修建日耳曼长城的多项政策，恢复了帝国的活力。奠定了让罗马帝国取得进一步繁荣的基础。","tags":[{"name":"罗马","slug":"罗马","permalink":"http://blog.yaodataking.com/tags/罗马/"}]},{"title":"《罗马人的故事7:臭名昭著的皇帝》读书笔记","date":"2016-08-07T00:36:42.000Z","path":"2016/08/07/rome-story-7/","text":"从公元14年至68年间，罗马帝国迎来了四位公认的臭名昭著的皇帝。其实我觉得除了卡利古拉和尼禄这两位少年皇帝，提比略和克劳狄乌斯对帝国事业兢兢业业，从后人角度看，无疑是归在好皇帝之列。也许从性格上来说，这四位都没有强烈的做皇帝的意愿。论才干，提比略完全可以评为罗马帝国最优秀的帝王之一，但是晚年时一个人生活在一个岛上，一边享受着宁静，一边控制着帝国，可见让他做皇帝多么的勉为其难。按说卡利古拉这位最受万民拥戴的，最有奥古斯都血统的皇帝前程似锦，然而自作孽不可活，他是被认为最忠心的信仰者刺杀的。至于克劳狄乌斯来说，完全是命运对这位历史学家开了个玩笑，由于卡利古拉短短4年的暴政，身有残疾的克劳狄乌斯不得不在50岁被人推上了皇帝的宝座。克劳狄乌斯也不负众望成功收拾卡利古拉留下的烂摊子。而尼禄继任罗马帝国皇帝时，时年16岁10个月。12年的皇帝生涯，与其说是在做皇帝，还不如说他在玩皇帝，最终，尼禄留下了“可怜的艺术家将要死了”这样一句遗言。总之，集权力于一身的罗马皇帝如果没有驾驭这些权力的天赋或者没有勇气使用这些权力，那么拥有权力也是枉然。 随着尼禄的自杀，尤里乌斯-克劳狄乌斯王朝结束了，重视血缘关系的奥古斯都也终于让后代尝到了苦果。","tags":[{"name":"罗马","slug":"罗马","permalink":"http://blog.yaodataking.com/tags/罗马/"}]},{"title":"《罗马人的故事6:罗马统治下的和平》读书笔记","date":"2016-07-21T13:33:12.000Z","path":"2016/07/21/rome-story-6/","text":"恺撒是一个很懂得自我宣传的人，但他的后继者、罗马帝国的开国皇帝奥古斯都技艺更胜一筹。尽管恺撒备受争议，有一点却毋庸置疑，他不是一个伪君子，而他的养子却能够毫无愧色地欺世盗名。奥古斯都的矫言伪行，首先是为了保护自己的利益，同时，也是为了达到他“罗马统治下的和平”之目的。对个人而言，没有比自身的利益能和共同体（国家）的利益相符来得更幸福的事情。凯撒的这句话， “人不管是谁都无法看清现实中的一切，大多数人只希望看到自己想看到的和想要的现实而已。”也许只有他理解的最深刻。奥古斯都的确是一个精明狡猾的人物，也正因为他这高超的政治手腕，Pax（“罗马统治下的和平”）才能得以实现。毕竟，和平不是轻松地唱着歌就能降临的。事实上，罗马共和时代正是霸权扩张的年代，而帝国时代则进入了防御守卫的时期。这些贬低罗马帝国的研究者们主要论点是罗马进入帝制之后，失去了自由，即决定国家政策的自由。那么，罗马在共和时代，真享受过这类的自由吗？罗马共和时代的政体，并非是雅典般的直接民主制，虽然有一个公民大会的决议，实际上元老院掌握了国家的权力，罗马共和体制是一个历史上称为“寡头政治”的少数人领导的制度。奥古斯都去世前坐船畅游那不勒斯时，曾经发生过一段小插曲。当他的船经过波佐利（Pozzuoli）港时，一艘从亚历山大港来的商船刚好抵达，船上乘客及船员都认出了附近船上正在休息的老皇帝，所有的人向着他齐声合唱般地叫道：因您所赐，我们才有今天的生活。因您所赐，我们才有安全之旅。因您所赐，我们才有自由，享受和平。突如其来的赞美，让老迈的奥古斯都感到无比喜悦，他下令赐予每个人40个金币，要求大家发誓保证把这笔钱全部用来购买埃及的货物，然后再将购买的货物销售到其他地方去。奥古斯都在生命即将终结之时，仍然是一个清醒、现实的人。只有物品的自由流通，才能提升帝国整体经济和人民的生活水平。而这一切必须在“罗马统治下的和平”之下才能得以实现。","tags":[{"name":"罗马","slug":"罗马","permalink":"http://blog.yaodataking.com/tags/罗马/"}]},{"title":"OpenStack入门之九:使用REST API启动虚拟机实例","date":"2016-07-03T15:01:35.000Z","path":"2016/07/03/openstack-rest-api/","text":"上文OpenStack入门之八:使用python SDK代码启动虚拟机实例我们使用python SDK的代码来启动一个虚拟机实例。本文将演示如何使用REST API来启动一个虚拟机实例。如果你不知道什么是REST请参考这篇博文，理解RESTful架构从上文OpenStack入门之八:使用python SDK代码启动虚拟机实例我们知道要想创建一个虚拟机，至少知道三个参数，虚拟机名称，镜像名称以及类型模板(flavor)。所以我们首先要获取image ID及flavor ID,但是要得到这些资源，首先要获取TOKEN数据以及NOVA的URL地址。1.1引用库文件那么通过什么工具来与openstack API 通讯呢，这里我们用到了PYHON的标准库urllib2,另外我们使用JSON格式来传递数据，所以还要引用JSON库。 import json import urllib2 `&lt;/pre&gt; 1.2获取token数据 我们通过POST用户名密码的方式获取token数据,这里简单起见直接定义。 &lt;pre&gt;`auth = {&quot;auth&quot;:{&quot;identity&quot;:{&quot;methods&quot;:[&quot;password&quot;],&quot;password&quot;:{&quot;user&quot;:{&quot;domain&quot;:{&quot;id&quot;:&quot;default&quot;},&quot;name&quot;:&quot;demo&quot;,&quot;password&quot;:&quot;passw0rd&quot;}}},&quot;scope&quot;:{&quot;project&quot;:{&quot;domain&quot;:{&quot;id&quot;:&quot;default&quot;},&quot;name&quot;:&quot;demo&quot;}}}} `&lt;/pre&gt; URL地址简单起见，也直接定义，这两个参数后续可以定义在配置文件抓取 &lt;pre&gt;`auth_url = &apos;http://192.168.199.20:35357/v3/&apos; `&lt;/pre&gt; 现在我们可以获取token数据了。 &lt;pre&gt;`def token_post(auth,auth_url): headers = {&quot;Content-type&quot;:&quot;application/json&quot;,&quot;Accept&quot;: &quot;application/json;charset=UTF-8&quot;} token_url = auth_url+&quot;auth/tokens&quot; req = urllib2.Request(token_url, auth, headers) response = urllib2.urlopen(req) token = response.info().getheader(&quot;X-Subject-Token&quot;) response.encoding=&apos;utf-8&apos; response_json = json.loads(response.read()) response_json = byteify(response_json) response_json[&apos;authtoken&apos;] = token return response_json `&lt;/pre&gt; 1.3获取NOVA URL地址 通过token返回的数据，我们可以直接筛选出NOVA的URL地址 &lt;pre&gt;`def get_nova_endpoint(response): services = response[&apos;token&apos;][&apos;catalog&apos;] for i in range(0, len(services)): if services[i][&apos;name&apos;] == &apos;nova&apos;: nova_endpoint = services[i][&apos;endpoints&apos;][0][&apos;url&apos;] return nova_endpoint `&lt;/pre&gt; 1.4获取image ID 通过image名字获取iamge ID &lt;pre&gt;`def get_image_id(novaurl,tokenid,imagename): servers_url = novaurl+&apos;/&apos;+&apos;images&apos; req = urllib2.Request(servers_url) req.add_header(&apos;X-Auth-Token&apos;, tokenid) response = urllib2.urlopen(req) services = json.loads(response.read())[&apos;images&apos;] for i in range(0, len(services)): if services[i][&apos;name&apos;] == imagename: image_id = services[i][&apos;id&apos;] return image_id `&lt;/pre&gt; 1.5获取flavor ID 通过flavor名字获取flavor ID &lt;pre&gt;`def get_flavor_id(novaurl,tokenid,flavorname): servers_url = novaurl+&apos;/&apos;+&apos;flavors&apos; req = urllib2.Request(servers_url) req.add_header(&apos;X-Auth-Token&apos;, tokenid) response = urllib2.urlopen(req) services = json.loads(response.read())[&apos;flavors&apos;] for i in range(0, len(services)): if services[i][&apos;name&apos;] == flavorname: flavor_id = services[i][&apos;id&apos;] return flavor_id `&lt;/pre&gt; 1.6 创建虚拟机 现在我们可以创建虚拟机了。 &lt;pre&gt;`def create_vmserver(novaurl,tokenid,vmname,imange_id,flaver_id): headers = {&quot;Content-type&quot;:&quot;application/json&quot;,&quot;Accept&quot;: &quot;application/json;charset=UTF-8&quot;,&quot;X-Auth-Token&quot;:tokenid} #print headers servers_url = novaurl+&quot;/servers&quot; #print servers_url body={&quot;server&quot;: {&quot;name&quot;: vmname, &quot;imageRef&quot;: imange_id, &quot;flavorRef&quot;: flaver_id}} body=json.dumps(body) #print body req = urllib2.Request(servers_url,body,headers) response = urllib2.urlopen(req) response_json = json.loads(response.read()) return response_json `&lt;/pre&gt; 1.7查看结果 运行源代码 [![2016-07-03_15-44-08](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/07/2016-07-03_15-44-08.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/07/2016-07-03_15-44-08.jpg) 我们查看instance，看到test1已经创建。 [![2016-07-03_15-17-01](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/07/2016-07-03_15-17-01.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/07/2016-07-03_15-17-01.jpg) 本文参考 http://developer.openstack.org/api-ref/compute/?expanded=create-server-detail 附整个源代码。 &lt;pre&gt;`import json import urllib2 def byteify(input): if isinstance(input, dict): return {byteify(key): byteify(value) for key, value in input.iteritems()} elif isinstance(input, list): return [byteify(element) for element in input] elif isinstance(input, unicode): return input.encode(&apos;utf-8&apos;) else: return input def token_post(auth,auth_url): headers = {&quot;Content-type&quot;:&quot;application/json&quot;,&quot;Accept&quot;: &quot;application/json;charset=UTF-8&quot;} token_url = auth_url+&quot;auth/tokens&quot; req = urllib2.Request(token_url, auth, headers) response = urllib2.urlopen(req) token = response.info().getheader(&quot;X-Subject-Token&quot;) response.encoding=&apos;utf-8&apos; response_json = json.loads(response.read()) response_json = byteify(response_json) response_json[&apos;authtoken&apos;] = token return response_json def get_nova_endpoint(response): services = response[&apos;token&apos;][&apos;catalog&apos;] for i in range(0, len(services)): if services[i][&apos;name&apos;] == &apos;nova&apos;: nova_endpoint = services[i][&apos;endpoints&apos;][0][&apos;url&apos;] return nova_endpoint def get_image_id(novaurl,tokenid,imagename): servers_url = novaurl+&apos;/&apos;+&apos;images&apos; req = urllib2.Request(servers_url) req.add_header(&apos;X-Auth-Token&apos;, tokenid) response = urllib2.urlopen(req) services = json.loads(response.read())[&apos;images&apos;] for i in range(0, len(services)): if services[i][&apos;name&apos;] == imagename: image_id = services[i][&apos;id&apos;] return image_id def get_flavor_id(novaurl,tokenid,flavorname): servers_url = novaurl+&apos;/&apos;+&apos;flavors&apos; req = urllib2.Request(servers_url) req.add_header(&apos;X-Auth-Token&apos;, tokenid) response = urllib2.urlopen(req) services = json.loads(response.read())[&apos;flavors&apos;] for i in range(0, len(services)): if services[i][&apos;name&apos;] == flavorname: flavor_id = services[i][&apos;id&apos;] return flavor_id def create_vmserver(novaurl,tokenid,vmname,imange_id,flaver_id): headers = {&quot;Content-type&quot;:&quot;application/json&quot;,&quot;Accept&quot;: &quot;application/json;charset=UTF-8&quot;,&quot;X-Auth-Token&quot;:tokenid} #print headers servers_url = novaurl+&quot;/servers&quot; #print servers_url body={&quot;server&quot;: {&quot;name&quot;: vmname, &quot;imageRef&quot;: imange_id, &quot;flavorRef&quot;: flaver_id}} body=json.dumps(body) #print body req = urllib2.Request(servers_url,body,headers) response = urllib2.urlopen(req) response_json = json.loads(response.read()) return response_json if __name__ == &apos;__main__&apos;: auth = {&quot;auth&quot;:{&quot;identity&quot;:{&quot;methods&quot;:[&quot;password&quot;],&quot;password&quot;:{&quot;user&quot;:{&quot;domain&quot;:{&quot;id&quot;:&quot;default&quot;},&quot;name&quot;:&quot;demo&quot;,&quot;password&quot;:&quot;passw0rd&quot;}}},&quot;scope&quot;:{&quot;project&quot;:{&quot;domain&quot;:{&quot;id&quot;:&quot;default&quot;},&quot;name&quot;:&quot;demo&quot;}}}} auth = json.dumps(auth) auth_url = &apos;http://192.168.199.20:35357/v3/&apos; response = token_post(auth,auth_url) #print response tokenid = response[&apos;authtoken&apos;] novaurl = get_nova_endpoint(response) #print novaurl imange_id=get_image_id(novaurl,tokenid,&apos;cirros-0.3.4-x86_64-uec&apos;) flaver_id=get_flavor_id(novaurl,tokenid,&apos;m1.tiny&apos;) print create_vmserver(novaurl,tokenid,&apos;test1&apos;,imange_id,flaver_id)","tags":[{"name":"openstack","slug":"openstack","permalink":"http://blog.yaodataking.com/tags/openstack/"},{"name":"Python","slug":"Python","permalink":"http://blog.yaodataking.com/tags/Python/"}]},{"title":"《罗马人的故事5:凯撒时代（下）》读书笔记","date":"2016-07-03T12:41:13.000Z","path":"2016/07/03/rome-story-5/","text":"越过卢比孔河无疑是古罗马历史上的重大事件，从这一刻起，恺撒才真正成为后人意思上的有着无比荣耀的恺撒，如果听从元老院的最终劝告，那么，他最多跟苏拉，马略，或者庞培一样的下场。凯撒的不同之处在于，他给自己立下的最重要课题，是建立罗马的新秩序，为实现这个目标，他不得不诉诸战争。所以对恺撒来说，这场与庞培的内战，只是一个手段，不是目的。为了实现他的伟大目标，他果敢，行动迅速；他宽恕，忠实内心；他自信， 我来，我见，我征服。而庞培，这位少年就得志的天才将领，在王者之气的凯撒面前，节节败退，最终被杀，这样的下场不禁令人惋惜，甚至凯撒见到庞培的首级，也禁不住地流下了眼泪。此时的凯撒集各种权力于一身，各种事实表明，凯撒已经事实上成了帝王–罗马走进了帝制。这里我不讨论共和制和帝制孰优孰劣，然而对于刚走进帝制的某些古罗马人来说，出于对帝制的恐惧，想要用刺杀的手段恢复共和国。恺撒的悲剧就在于他用最宽容的手段行最专制的目的，3月15日，历史永远记住这天，身中23剑的凯撒倒在了地上。然而历史的车轮已经无法靠几个人可以停止了，在凯撒指定的继承人屋大维与安东尼争夺的内战结束后，古罗马第一任皇帝横空出世了。这里也不得不赞叹凯撒的眼光，他没有指定与他一起出身入死过的副将安东尼作为他的继承人，而是当时年仅18岁的侄子屋大维。而屋大维终于不负凯撒的重托，经过14年的努力，从当时无人知晓屋大维为何许人，成为无人不知的古罗马首任皇帝。这段历史虽然短短不到20年，然而在古罗马史上无疑是继往开来的时代，随着屋大维下令关闭雅努斯神殿的大门，古罗马迎来了“罗马统治下的和平”。","tags":[{"name":"罗马","slug":"罗马","permalink":"http://blog.yaodataking.com/tags/罗马/"}]},{"title":"OpenStack入门之八:使用python SDK代码启动虚拟机实例","date":"2016-06-26T04:46:32.000Z","path":"2016/06/26/openstack-python-sdk-createvm/","text":"本文基于OpenStack入门之二:devstack开发环境搭建一文配置的环境，使用python SDK的代码来启动一个虚拟机实例。1.建立PyDev项目1.1新建PyDev项目启动eclipse，新建PyDev项目，命名为nova-createvm1.2新建包设置包名com.nova.demo1.3新建Python文件新建Python源代码文件NovaApiCreateVM.py，要启动一个新实例，可以使用 Client.servers.create 方法，需要传递一个 image 对象和 flavor 对象，代码如下： import time from keystoneclient.auth.identity import v3 from keystoneclient import session from keystoneclient.v3 import client as keystoneapi from novaclient import client as novapi auth_url = &apos;http://192.168.199.20:35357/v3/&apos; username = &apos;demo&apos; user_domain_name = &apos;Default&apos; project_name = &apos;demo&apos; project_domain_name = &apos;Default&apos; password = &apos;passw0rd&apos; auth = v3.Password(auth_url=auth_url, username=username, password=password, project_name=project_name, project_domain_name=project_domain_name, user_domain_name=user_domain_name) sess = session.Session(auth=auth) keystone = keystoneapi.Client(session=sess) nova = novapi.Client(2, session=keystone.session) image = nova.images.find(name=&quot;cirros-0.3.4-x86_64-uec&quot;) flavor = nova.flavors.find(name=&quot;m1.tiny&quot;) instance = nova.servers.create(name=&quot;test&quot;, image=image, flavor=flavor) # Poll at 5 second intervals, until the status is no longer &apos;BUILD&apos; status = instance.status while status == &apos;BUILD&apos;: time.sleep(5) # Retrieve the instance again so the status field updates instance = nova.servers.get(instance.id) status = instance.status print &quot;status: %s&quot; % status `&lt;/pre&gt; nova-createvm文件如下， [![2016-06-26_12-29-37](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/06/2016-06-26_12-29-37.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/06/2016-06-26_12-29-37.jpg) 2.运行验证 2.1运行 右键文件名--&amp;gt;Run As--&amp;gt; Python Run，等待几秒钟，我们看到运行结果的状态为acvtive [![2016-06-26_12-08-50](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/06/2016-06-26_12-08-50.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/06/2016-06-26_12-08-50.jpg) 2.2验证 进入网站http://192.168.199.20.使用用户名demo/passw0rd登录，点击instance，我们看到test的instance已经运行中。 [![2016-06-26_12-06-37](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/06/2016-06-26_12-06-37.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/06/2016-06-26_12-06-37.jpg) 以上的例子还可以使用 Client.keypairs.create 方法创建ssh key来验证，有兴趣的同学可以试验一下。 &lt;pre&gt;` creds = get_nova_creds() nova = nvclient.Client(**creds) if not nova.keypairs.findall(name=&quot;mykey&quot;): with open(os.path.expanduser(&apos;~/.ssh/id_rsa.pub&apos;)) as fpubkey: nova.keypairs.create(name=&quot;mykey&quot;, public_key=fpubkey.read())","tags":[{"name":"openstack","slug":"openstack","permalink":"http://blog.yaodataking.com/tags/openstack/"},{"name":"Python","slug":"Python","permalink":"http://blog.yaodataking.com/tags/Python/"}]},{"title":"OpenStack入门之七:基于NFS共享存储的在线迁移","date":"2016-06-21T01:14:22.000Z","path":"2016/06/21/openstack-live-migration-nfs/","text":"上文我们做了虚拟机的在线迁移实验，由于在两台主机之间迁移，性能和速度可想而知了。本文我们将实验基于NFS共享存储进行在线迁移，以达到秒速的迁移体验。1.在NFS服务器上设置挂载点1.1安装NFS #yum install -y nfs-utils1.2创建共享目录 #mkdir /data1.3修改/etc/exports文件添加以下一行 /data compute-node-01(rw,sync,fsid=0,no_root_squash) compute-node-02(rw,sync,fsid=0,no_root_squash) 1.4启动nfs服务systemctl enable rpcbind.servicesystemctl enable nfs-server.servicesystemctl start rpcbind.servicesystemctl start nfs-server.service 2在所有计算节点上完成以下操作2.1安装nfs #yum install -y nfs-utils #systemctl enable rpcbind.service #systemctl start rpcbind.service2.2挂载 #mount -t nfs 192.168.199.87:/data /var/lib/nova/instances192.168.199.87是nfs服务器的IP.2.3重新设置文件夹所有者及权限 #chown -R nova:nova /var/lib/nova/instances 3迁移3.1创建虚拟机为了更清楚的演示，我们将之前的虚拟机删除，重新创建一个新的虚拟机。按照上文的定义，我们看到新的虚拟机安装在compute-node-02上。3.2开始热迁移3.3迁移完成（很快）看目录文件，compute-node-01和compute-node-02迁移前后没有变化。其实他们都共享了nfs server 的data目录，这也是迁移速度很快的原因，也是使用NFS的好处","tags":[{"name":"openstack","slug":"openstack","permalink":"http://blog.yaodataking.com/tags/openstack/"}]},{"title":"OpenStack入门之六:调度服务与在线迁移实验","date":"2016-06-21T00:40:01.000Z","path":"2016/06/21/openstack-scheduler-live-migration/","text":"本文我们在OpenStack入门之三:Kilo版本centos 7平台搭建基础上设置调度服务与在线迁移实验。为了完成调度，我们需要设置一个新的计算节点compute-node-02。（具体设置过程略)1.调度服务1.1创建主机集 #source /root/admin-openrc.sh #nova aggregate-create mynova01 nova [root@controller ~]# nova aggregate-create mynova01 nova +----+----------+-------------------+-------+--------------------------+ | Id | Name | Availability Zone | Hosts | Metadata | +----+----------+-------------------+-------+--------------------------+ | 1 | mynova01 | nova | | &apos;availability_zone=nova&apos; | +----+----------+-------------------+-------+--------------------------+ [root@controller ~]# `&lt;/pre&gt; 1.2加入主机 #nova aggregate-add-host 1 compute-node-02 &lt;pre&gt;` [root@controller ~]# nova aggregate-add-host 1 compute-node-02 Host compute-node-02 has been successfully added for aggregate 1 +----+----------+-------------------+-------------------+--------------------------+ | Id | Name | Availability Zone | Hosts | Metadata | +----+----------+-------------------+-------------------+--------------------------+ | 1 | mynova01 | nova | &apos;compute-node-02&apos; | &apos;availability_zone=nova&apos; | +----+----------+-------------------+-------------------+--------------------------+ [root@controller ~]# `&lt;/pre&gt; 1.3设置元数据 #nova aggregate-set-metadata 1 test02=true &lt;pre&gt;` [root@controller ~]# nova aggregate-set-metadata 1 test02=true Metadata has been successfully updated for aggregate 1. +----+----------+-------------------+-------------------+-----------------------------------------+ | Id | Name | Availability Zone | Hosts | Metadata | +----+----------+-------------------+-------------------+-----------------------------------------+ | 1 | mynova01 | nova | &apos;compute-node-02&apos; | &apos;availability_zone=nova&apos;, &apos;test02=true&apos; | +----+----------+-------------------+-------------------+-----------------------------------------+ [root@controller ~]# `&lt;/pre&gt; #nova flavor-key m1.nano set test02=true 1.4修改控制节点上的nova.conf &lt;pre&gt;` scheduler_default_filters=AggregateInstanceExtraSpecsFilter,AvailabilityZoneFilter,RamFilter,ComputeFilter `&lt;/pre&gt; 1.5.重启 #systemctl restart openstack-nova-api.service openstack-nova-scheduler.service openstack-nova-conductor.service 1.6.创建主机 1.6.1生成sshkey #ssh-keygen -t rsa -f cloud.key 将生成的公钥复制在新增的key pair页面。 [![2016-06-19_21-52-42](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/06/2016-06-19_21-52-42.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/06/2016-06-19_21-52-42.jpg) 1.6.2设置启动后更改密码 [![2016-06-19_21-52-05](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/06/2016-06-19_21-52-05.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/06/2016-06-19_21-52-05.jpg) 1.7 验证 我们看到主机已启动并运行在compute-node-02上 [![2016-06-19_21-55-09](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/06/2016-06-19_21-55-09.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/06/2016-06-19_21-55-09.jpg) 使用更改后的密码登录虚拟机 [![2016-06-19_21-56-36](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/06/2016-06-19_21-56-36.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/06/2016-06-19_21-56-36.jpg) 2.虚拟机在线迁移 2.1计算节点更新组件 #yum install libvirt qemu-* -y 2.2修改计算节点相关配置 修改/etc/sysconfig/libvirtd 文件。 &lt;pre&gt;` LIBVIRTD_ARGS=&quot;--listen&quot; `&lt;/pre&gt; 在/etc/libvirt/libvirtd.conf 文件中做如下配置。 &lt;pre&gt;` listen_tls=0 listen_tcp=1 auth_tcp=&quot;none&quot; 2.3重启计算主机Reboot2.4在线移动2.4.1设置2.4.2移动中2.4.3移动完成等待一段时间，我们看到host已变为compute-node-01了。从上面的截图我们看到存放虚拟机信息的目录文件从compute-node-02实际移至compute-node-01了。由于在两台主机之间迁移，性能和速度可想而知了，下节我们将实验基于NFS共享存储进行在线迁移，以达到秒速的迁移体验。","tags":[{"name":"openstack","slug":"openstack","permalink":"http://blog.yaodataking.com/tags/openstack/"}]},{"title":"OpenStack入门之五:负载均衡和防火墙设置","date":"2016-06-21T00:03:49.000Z","path":"2016/06/21/openstack-lbaas-fwaas/","text":"本文我们在OpenStack入门之三:Kilo版本centos 7平台搭建基础上设置负载均衡和防火墙。 1.负载均衡设置1.1 网络节点安装配置1.1.1安装 #yum install openstack-neutron-lbaas haproxy1.1.2配置 #vi /etc/neutron/lbaas_agent.ini interface_driver = neutron.agent.linux.interface.OVSInterfaceDriver device_driver = neutron.services.loadbalancer.drivers.haproxy.namespace_driver.HaproxyNSDriver `&lt;/pre&gt; #vi /etc/neutron/neutron.conf &lt;pre&gt;&lt;/code&gt; service_plugins = router,lbaas &lt;/code&gt;&lt;/pre&gt; 1.1.3启用，重启 #systemctl enable neutron-lbaas-agent.service #systemctl start neutron-lbaas-agent.service #systemctl restart neutron-openvswitch-agent.service 1.2 控制节点安装配置 1.2.1 安装 #yum install openstack-neutron-lbaas -y 1.2.2 配置 #vi /etc/neutron/neutron.conf &lt;pre&gt;` service_plugins = router,lbaas [service_providers] service_provider = LOADBALANCERV2:Haproxy:neutron_lbaas.drivers.haproxy.driver.HaproxyDriver:default `&lt;/pre&gt; 1.2.3 重启服务 #systemctl restart neutron-server.service 1.3 Dashboard配置及验证 1.3.1 添加资源池 [![2016-06-11_9-11-13](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/06/2016-06-11_9-11-13.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/06/2016-06-11_9-11-13.jpg) 1.3.2 添加成员 [![2016-06-11_9-15-48](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/06/2016-06-11_9-15-48.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/06/2016-06-11_9-15-48.jpg) 1.3.3 设置vip ip [![2016-06-11_9-13-03](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/06/2016-06-11_9-13-03.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/06/2016-06-11_9-13-03.jpg) [![2016-06-11_9-14-17](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/06/2016-06-11_9-14-17.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/06/2016-06-11_9-14-17.jpg) 1.3.4 验证 SSH连接VIP 192.168.2.5我们看到实际连接的是192.168.2.3 [![2016-06-11_20-08-47](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/06/2016-06-11_20-08-47.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/06/2016-06-11_20-08-47.jpg) 我们停止实例192.168.2.3.再去连接192.168.2.5,看到连接到了192.168.2.4去了，这证明负载均衡是起作用了。 [![2016-06-11_20-13-58](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/06/2016-06-11_20-13-58.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/06/2016-06-11_20-13-58.jpg) [![2016-06-11_20-13-44](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/06/2016-06-11_20-13-44.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/06/2016-06-11_20-13-44.jpg) ## 2.防火墙设置 2.1控制节点安装配置 2.1.1安装 #yum install openstack-neutron-fwaas -y 2.1.2配置 #vi /etc/neutron/neutron.conf &lt;pre&gt;` service_plugins = router,lbaas,firewall [service_providers] service_provider = FIREWALL:Iptables:neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver:default `&lt;/pre&gt; 2.1.3重启服务 #systemctl restart neutron-server.service 2.1.4更新数据库 #neutron-db-manage --service fwaas upgrade head &lt;pre&gt;` INFO [alembic.runtime.migration] Context impl MySQLImpl. INFO [alembic.runtime.migration] Will assume non-transactional DDL. INFO [alembic.runtime.migration] Context impl MySQLImpl. INFO [alembic.runtime.migration] Will assume non-transactional DDL. INFO [alembic.runtime.migration] Running upgrade -&amp;gt; start_neutron_fwaas, start neutron-fwaas chain INFO [alembic.runtime.migration] Running upgrade start_neutron_fwaas -&amp;gt; 4202e3047e47, add_index_tenant_id INFO [alembic.runtime.migration] Running upgrade 4202e3047e47 -&amp;gt; 540142f314f4, FWaaS router insertion INFO [alembic.runtime.migration] Running upgrade 540142f314f4 -&amp;gt; 796c68dffbb, cisco_csr_fwaas INFO [alembic.runtime.migration] Running upgrade 796c68dffbb -&amp;gt; kilo, kilo `&lt;/pre&gt; 2.2 网络节点安装配置 2.2.1安装 #yum install openstack-neutron-fwaas -y 2.2.2配置 #vi /etc/neutron/fwaas_driver.ini &lt;pre&gt;` driver = neutron_fwaas.services.firewall.drivers.linux.iptables_fwaas.IptablesFwaasDriver enabled = True `&lt;/pre&gt; #vi /etc/neutron/neutron.conf &lt;pre&gt;` service_plugins = router,lbaas,firewall 2.2.3重启 #systemctl restart neutron-openvswitch-agent.service neutron-l3-agent.service2.3 Dashboard配置及验证2.3.1 设置规则2.3.2 设置策略2.3.3 设置防火墙并关联路由","tags":[{"name":"openstack","slug":"openstack","permalink":"http://blog.yaodataking.com/tags/openstack/"}]},{"title":"OpenStack入门之四:故障诊断实例","date":"2016-06-12T06:27:47.000Z","path":"2016/06/12/openstack-error-checking/","text":"对于openstack入门者来说，最怕的是出现各种错误而不知如何解决，本文简单介绍openstack日志的路径并通过实际解决一个故障来演示。1.日志的级别首先我们来回顾一下日志里各种信息的级别。INFO:突出强调程序的运行过程WARNING:程序可能存在潜在的问题ERROR:程序发生的错误DEBUG:程序调试信息TRACE:记录代码执行的方法和属性2.HTTP状态码由于OpenStack的各个服务之间是通过统一的REST风格的API调用，所以我们有必要回顾一下HTTP状态码。1)100-199 用于指定客户端应相应的某些动作。 100 (Continue/继续) 101 (Switching Protocols/转换协议) `&lt;/pre&gt; 2)200-299 用于表示请求成功。 &lt;pre&gt;`200 (OK/正常) 201 (Created/已创建) 202 (Accepted/接受) 203 (Non-Authoritative Information/非官方信息) 204 (No Content/无内容) 205 (Reset Content/重置内容) 206 (Partial Content/局部内容) `&lt;/pre&gt; 3)300-399 用于已经移动的文件并且常被包含在定位头信息中指定新的地址信息。 &lt;pre&gt;`300 (Multiple Choices/多重选择) 301 (Moved Permanently) 302 (Found/找到) 303 (See Other/参见其他信息) 304 (Not Modified/为修正) 305 (Use Proxy/使用代理) 305 (SC_USE_PROXY)表示所请求的文档要通过定位头信息中的代理服务器获得。这个状态码是新加入 HTTP 1.1中的。 307 (Temporary Redirect/临时重定向) `&lt;/pre&gt; 4)400-499 用于指出客户端的错误。 &lt;pre&gt;`400 (Bad Request/错误请求) 401 (Unauthorized/未授权) 403 (Forbidden/禁止) 404 (Not Found/未找到) 405 (Method Not Allowed/方法未允许) 406 (Not Acceptable/无法访问) 407 (Proxy Authentication Required/代理服务器认证要求) 408 (Request Timeout/请求超时) 409 (Conflict/冲突) 410 (Gone/已经不存在) 411 (Length Required/需要数据长度) 412 (Precondition Failed/先决条件错误) 413 (Request Entity Too Large/请求实体过大) 414 (Request URI Too Long/请求URI过长) 415 (Unsupported Media Type/不支持的媒体格式) 416 (Requested Range Not Satisfiable/请求范围无法满足) 417 (Expectation Failed/期望失败) `&lt;/pre&gt; 5)500-599 用于支持服务器错误。 &lt;pre&gt;`500 (Internal Server Error/内部服务器错误) 501 (Not Implemented/未实现) 502 (Bad Gateway/错误的网关) 503 (Service Unavailable/服务无法获得) 504 (Gateway Timeout/网关超时) 505 (HTTP Version Not Supported/不支持的 HTTP 版本) `&lt;/pre&gt; 3.日志路径 想要查看日志，必须知道各组件日志的路径。 &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;组件&lt;/th&gt; &lt;th&gt;组件log路径&lt;/th&gt; &lt;th&gt;所在节点&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt;&lt;tr&gt; &lt;td&gt;keystone&lt;/td&gt; &lt;td&gt;/var/log/httpd,/var/log/keystone&lt;/td&gt; &lt;td&gt;controller&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;nova&lt;/td&gt; &lt;td&gt;/var/log/nova&lt;/td&gt; &lt;td&gt;controller,compute-node-01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qemu&lt;/td&gt; &lt;td&gt;/var/log/libvirt/qemu&lt;/td&gt; &lt;td&gt;compute-node-01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;glance&lt;/td&gt; &lt;td&gt;/var/log/glance&lt;/td&gt; &lt;td&gt;controller&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;cinder&lt;/td&gt; &lt;td&gt;/var/log/cinder&lt;/td&gt; &lt;td&gt;controller,block-node-01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;neutron&lt;/td&gt; &lt;td&gt;/var/log/neutron&lt;/td&gt; &lt;td&gt;controller,compute-node-01 network-node-01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;openVSwtich&lt;/td&gt; &lt;td&gt;/var/log/openvswitch&lt;/td&gt; &lt;td&gt;compute-node-01 network-node-01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;swift&lt;/td&gt; &lt;td&gt;/var/log/swift&lt;/td&gt; &lt;td&gt;controller,block-node-01,block-node-02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;horizon&lt;/td&gt; &lt;td&gt;/var/log/httpd&lt;/td&gt; &lt;td&gt;controller&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Mariadb&lt;/td&gt; &lt;td&gt;/var/log/mariadb&lt;/td&gt; &lt;td&gt;controller&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;rabbitMQ&lt;/td&gt; &lt;td&gt;/var/log/rabbitmq&lt;/td&gt; &lt;td&gt;controller&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; 4.一个故障实例-Nova instance boot 在本次搭建openstack平台的过程中，出现了不同错误，这里举一个可能大家都会遇到的一个问题，创建实例出错！ 最初通过dashboard遇到的是这样的问题。 [![2016-05-07_23-42-54](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/06/2016-05-07_23-42-54.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/06/2016-05-07_23-42-54.jpg) 这样的信息我们只知道是服务器内部错误，无法知道具体是哪里的问题。 所以我们通过命令行加入debug选项 &lt;pre&gt;` [root@controller ~]# nova --debug boot --flavor m1.nano --image cirros-0.3.4-x86_64 --nic net-id=40f3f1c5-4ce4-4d38-8539-23dd553f3b2f --security-group default --key-name demo-key inst1 `&lt;/pre&gt; 屏幕打印的debug信息如下 &lt;pre&gt;` DEBUG (session:195) REQ: curl -g -i -X GET http://controller:5000/v3 -H &quot;Accept: application/json&quot; -H &quot;User-Agent: python-keystoneclient&quot; INFO (connectionpool:213) Starting new HTTP connection (1): controller DEBUG (connectionpool:393) &quot;GET /v3 HTTP/1.1&quot; 200 249 DEBUG (session:224) RESP: [200] Content-Length: 249 Vary: X-Auth-Token Keep-Alive: timeout=5, max=100 Server: Apache/2.4.6 (CentOS) mod_wsgi/3.4 Python/2.7.5 Connection: Keep-Alive Date: Fri, 10 Jun 2016 13:49:29 GMT Content-Type: application/json x-openstack-request-id: req-9abd7d61-323e-4d76-a778-bcb8f3b381a1 RESP BODY: {&quot;version&quot;: {&quot;status&quot;: &quot;stable&quot;, &quot;updated&quot;: &quot;2015-03-30T00:00:00Z&quot;, &quot;media-types&quot;: [{&quot;base&quot;: &quot;application/json&quot;, &quot;type&quot;: &quot;application/vnd.openstack.identity-v3+json&quot;}], &quot;id&quot;: &quot;v3.4&quot;, &quot;links&quot;: [{&quot;href&quot;: &quot;http://controller:5000/v3/&quot;, &quot;rel&quot;: &quot;self&quot;}]}} DEBUG (base:171) Making authentication request to http://controller:5000/v3/auth/tokens DEBUG (connectionpool:393) &quot;POST /v3/auth/tokens HTTP/1.1&quot; 201 3247 DEBUG (session:195) REQ: curl -g -i -X GET http://controller:8774/v2/276eb659fd4b44859849c773dcfdf138/images -H &quot;User-Agent: python-novaclient&quot; -H &quot;Accept: application/json&quot; -H &quot;X-Auth-Token: {SHA1}c327ae73d3652d44c33cd59907566d40909e631a&quot; INFO (connectionpool:213) Starting new HTTP connection (1): controller DEBUG (connectionpool:393) &quot;GET /v2/276eb659fd4b44859849c773dcfdf138/images HTTP/1.1&quot; 200 508 DEBUG (session:224) RESP: [200] Date: Fri, 10 Jun 2016 13:49:30 GMT Connection: keep-alive Content-Type: application/json Content-Length: 508 X-Compute-Request-Id: req-9b2129f1-f24e-453a-88f3-f05580894e07 RESP BODY: {&quot;images&quot;: [{&quot;id&quot;: &quot;c9509d39-78b3-4509-94b5-8498e8e6acea&quot;, &quot;links&quot;: [{&quot;href&quot;: &quot;http://controller:8774/v2/276eb659fd4b44859849c773dcfdf138/images/c9509d39-78b3-4509-94b5-8498e8e6acea&quot;, &quot;rel&quot;: &quot;self&quot;}, {&quot;href&quot;: &quot;http://controller:8774/276eb659fd4b44859849c773dcfdf138/images/c9509d39-78b3-4509-94b5-8498e8e6acea&quot;, &quot;rel&quot;: &quot;bookmark&quot;}, {&quot;href&quot;: &quot;http://controller:9292/images/c9509d39-78b3-4509-94b5-8498e8e6acea&quot;, &quot;type&quot;: &quot;application/vnd.openstack.image&quot;, &quot;rel&quot;: &quot;alternate&quot;}], &quot;name&quot;: &quot;cirros-0.3.4-x86_64&quot;}]} DEBUG (session:195) REQ: curl -g -i -X GET http://controller:8774/v2/276eb659fd4b44859849c773dcfdf138/images/c9509d39-78b3-4509-94b5-8498e8e6acea -H &quot;User-Agent: python-novaclient&quot; -H &quot;Accept: application/json&quot; -H &quot;X-Auth-Token: {SHA1}c327ae73d3652d44c33cd59907566d40909e631a&quot; DEBUG (connectionpool:393) &quot;GET /v2/276eb659fd4b44859849c773dcfdf138/images/c9509d39-78b3-4509-94b5-8498e8e6acea HTTP/1.1&quot; 500 128 DEBUG (session:224) RESP: DEBUG (shell:914) The server has either erred or is incapable of performing the requested operation. (HTTP 500) (Request-ID: req-e359221b-6be3-4ac2-bb55-5310f5bb0bb2) Traceback (most recent call last): File &quot;/usr/lib/python2.7/site-packages/novaclient/shell.py&quot;, line 911, in main OpenStackComputeShell().main(argv) File &quot;/usr/lib/python2.7/site-packages/novaclient/shell.py&quot;, line 838, in main args.func(self.cs, args) File &quot;/usr/lib/python2.7/site-packages/novaclient/v2/shell.py&quot;, line 495, in do_boot boot_args, boot_kwargs = _boot(cs, args) File &quot;/usr/lib/python2.7/site-packages/novaclient/v2/shell.py&quot;, line 142, in _boot image = _find_image(cs, args.image) File &quot;/usr/lib/python2.7/site-packages/novaclient/v2/shell.py&quot;, line 1894, in _find_image return utils.find_resource(cs.images, image) File &quot;/usr/lib/python2.7/site-packages/novaclient/utils.py&quot;, line 216, in find_resource return manager.find(**kwargs) File &quot;/usr/lib/python2.7/site-packages/novaclient/base.py&quot;, line 196, in find matches = self.findall(**kwargs) File &quot;/usr/lib/python2.7/site-packages/novaclient/base.py&quot;, line 258, in findall found.append(self.get(obj.id)) File &quot;/usr/lib/python2.7/site-packages/novaclient/v2/images.py&quot;, line 53, in get return self._get(&quot;/images/%s&quot; % base.getid(image), &quot;image&quot;) File &quot;/usr/lib/python2.7/site-packages/novaclient/base.py&quot;, line 156, in _get _resp, body = self.api.client.get(url) File &quot;/usr/lib/python2.7/site-packages/keystoneclient/adapter.py&quot;, line 170, in get return self.request(url, &apos;GET&apos;, **kwargs) File &quot;/usr/lib/python2.7/site-packages/novaclient/client.py&quot;, line 96, in request raise exceptions.from_response(resp, body, url, method) ClientException: The server has either erred or is incapable of performing the requested operation. (HTTP 500) (Request-ID: req-e359221b-6be3-4ac2-bb55-5310f5bb0bb2) ERROR (ClientException): The server has either erred or is incapable of performing the requested operation. (HTTP 500) (Request-ID: req-e359221b-6be3-4ac2-bb55-5310f5bb0bb2) `&lt;/pre&gt; 看上去是nova的问题，所以我们再去查看nova日志。下面是nova-api的日志。 &lt;pre&gt;` 2016-06-10 21:49:30.191 7533 ERROR nova.api.openstack [req-e359221b-6be3-4ac2-bb55-5310f5bb0bb2 03384abb3c864c6bb55b09d33371beb4 276eb659fd4b44859849c773dcfdf138 - - -] Caught error: id 2016-06-10 21:49:30.191 7533 TRACE nova.api.openstack Traceback (most recent call last): 2016-06-10 21:49:30.191 7533 TRACE nova.api.openstack File &quot;/usr/lib/python2.7/site-packages/nova/api/openstack/__init__.py&quot;, line 125, in __call__ 2016-06-10 21:49:30.191 7533 TRACE nova.api.openstack return req.get_response(self.application) 2016-06-10 21:49:30.191 7533 TRACE nova.api.openstack File &quot;/usr/lib/python2.7/site-packages/webob/request.py&quot;, line 1317, in send 2016-06-10 21:49:30.191 7533 TRACE nova.api.openstack application, catch_exc_info=False) 2016-06-10 21:49:30.191 7533 TRACE nova.api.openstack File &quot;/usr/lib/python2.7/site-packages/webob/request.py&quot;, line 1281, in call_application 2016-06-10 21:49:30.191 7533 TRACE nova.api.openstack app_iter = application(self.environ, start_response) 2016-06-10 21:49:30.191 7533 TRACE nova.api.openstack File &quot;/usr/lib/python2.7/site-packages/webob/dec.py&quot;, line 144, in __call__ 2016-06-10 21:49:30.191 7533 TRACE nova.api.openstack return resp(environ, start_response) 2016-06-10 21:49:30.191 7533 TRACE nova.api.openstack File &quot;/usr/lib/python2.7/site-packages/keystonemiddleware/auth_token/__init__.py&quot;, line 634, in __call__ 2016-06-10 21:49:30.191 7533 TRACE nova.api.openstack return self._call_app(env, start_response) 2016-06-10 21:49:30.191 7533 TRACE nova.api.openstack File &quot;/usr/lib/python2.7/site-packages/keystonemiddleware/auth_token/__init__.py&quot;, line 554, in _call_app 2016-06-10 21:49:30.191 7533 TRACE nova.api.openstack return self._app(env, _fake_start_response) 2016-06-10 21:49:30.191 7533 TRACE nova.api.openstack File &quot;/usr/lib/python2.7/site-packages/webob/dec.py&quot;, line 144, in __call__ 2016-06-10 21:49:30.191 7533 TRACE nova.api.openstack return resp(environ, start_response) 2016-06-10 21:49:30.191 7533 TRACE nova.api.openstack File &quot;/usr/lib/python2.7/site-packages/webob/dec.py&quot;, line 144, in __call__ 2016-06-10 21:49:30.191 7533 TRACE nova.api.openstack return resp(environ, start_response) 2016-06-10 21:49:30.191 7533 TRACE nova.api.openstack File &quot;/usr/lib/python2.7/site-packages/routes/middleware.py&quot;, line 131, in __call__ 2016-06-10 21:49:30.191 7533 TRACE nova.api.openstack response = self.app(environ, start_response) 2016-06-10 21:49:30.191 7533 TRACE nova.api.openstack File &quot;/usr/lib/python2.7/site-packages/webob/dec.py&quot;, line 144, in __call__ 2016-06-10 21:49:30.191 7533 TRACE nova.api.openstack return resp(environ, start_response) 2016-06-10 21:49:30.191 7533 TRACE nova.api.openstack File &quot;/usr/lib/python2.7/site-packages/webob/dec.py&quot;, line 130, in __call__ 2016-06-10 21:49:30.191 7533 TRACE nova.api.openstack resp = self.call_func(req, *args, **self.kwargs) 2016-06-10 21:49:30.191 7533 TRACE nova.api.openstack File &quot;/usr/lib/python2.7/site-packages/webob/dec.py&quot;, line 195, in call_func 2016-06-10 21:49:30.191 7533 TRACE nova.api.openstack return self.func(req, *args, **kwargs) 2016-06-10 21:49:30.191 7533 TRACE nova.api.openstack File &quot;/usr/lib/python2.7/site-packages/nova/api/openstack/wsgi.py&quot;, line 756, in __call__ 2016-06-10 21:49:30.191 7533 TRACE nova.api.openstack content_type, body, accept) 2016-06-10 21:49:30.191 7533 TRACE nova.api.openstack File &quot;/usr/lib/python2.7/site-packages/nova/api/openstack/wsgi.py&quot;, line 821, in _process_stack 2016-06-10 21:49:30.191 7533 TRACE nova.api.openstack action_result = self.dispatch(meth, request, action_args) 2016-06-10 21:49:30.191 7533 TRACE nova.api.openstack File &quot;/usr/lib/python2.7/site-packages/nova/api/openstack/wsgi.py&quot;, line 911, in dispatch 2016-06-10 21:49:30.191 7533 TRACE nova.api.openstack return method(req=request, **action_args) 2016-06-10 21:49:30.191 7533 TRACE nova.api.openstack File &quot;/usr/lib/python2.7/site-packages/nova/api/openstack/compute/images.py&quot;, line 83, in show 2016-06-10 21:49:30.191 7533 TRACE nova.api.openstack image = self._image_api.get(context, id) 2016-06-10 21:49:30.191 7533 TRACE nova.api.openstack File &quot;/usr/lib/python2.7/site-packages/nova/image/api.py&quot;, line 93, in get 2016-06-10 21:49:30.191 7533 TRACE nova.api.openstack show_deleted=show_deleted) 2016-06-10 21:49:30.191 7533 TRACE nova.api.openstack File &quot;/usr/lib/python2.7/site-packages/nova/image/glance.py&quot;, line 309, in show 2016-06-10 21:49:30.191 7533 TRACE nova.api.openstack include_locations=include_locations) 2016-06-10 21:49:30.191 7533 TRACE nova.api.openstack File &quot;/usr/lib/python2.7/site-packages/nova/image/glance.py&quot;, line 483, in _translate_from_glance 2016-06-10 21:49:30.191 7533 TRACE nova.api.openstack include_locations=include_locations) 2016-06-10 21:49:30.191 7533 TRACE nova.api.openstack File &quot;/usr/lib/python2.7/site-packages/nova/image/glance.py&quot;, line 545, in _extract_attributes 2016-06-10 21:49:30.191 7533 TRACE nova.api.openstack queued = getattr(image, &apos;status&apos;) == &apos;queued&apos; 2016-06-10 21:49:30.191 7533 TRACE nova.api.openstack File &quot;/usr/lib/python2.7/site-packages/glanceclient/openstack/common/apiclient/base.py&quot;, line 491, in __getattr__ 2016-06-10 21:49:30.191 7533 TRACE nova.api.openstack self.get() 2016-06-10 21:49:30.191 7533 TRACE nova.api.openstack File &quot;/usr/lib/python2.7/site-packages/glanceclient/openstack/common/apiclient/base.py&quot;, line 509, in get 2016-06-10 21:49:30.191 7533 TRACE nova.api.openstack new = self.manager.get(self.id) 2016-06-10 21:49:30.191 7533 TRACE nova.api.openstack File &quot;/usr/lib/python2.7/site-packages/glanceclient/openstack/common/apiclient/base.py&quot;, line 494, in __getattr__ 2016-06-10 21:49:30.191 7533 TRACE nova.api.openstack raise AttributeError(k) 2016-06-10 21:49:30.191 7533 TRACE nova.api.openstack AttributeError: id 2016-06-10 21:49:30.191 7533 TRACE nova.api.openstack 2016-06-10 21:49:30.192 7533 INFO nova.api.openstack [req-e359221b-6be3-4ac2-bb55-5310f5bb0bb2 03384abb3c864c6bb55b09d33371beb4 276eb659fd4b44859849c773dcfdf138 - - -] http://controller:8774/v2/276eb659fd4b44859849c773dcfdf138/images/c9509d39-78b3-4509-94b5-8498e8e6acea returned with HTTP 500 2016-06-10 21:49:30.193 7533 INFO nova.osapi_compute.wsgi.server [req-e359221b-6be3-4ac2-bb55-5310f5bb0bb2 03384abb3c864c6bb55b09d33371beb4 276eb659fd4b44859849c773dcfdf138 - - -] 192.168.199.80 &quot;GET /v2/276eb659fd4b44859849c773dcfdf138/images/c9509d39-78b3-4509-94b5-8498e8e6acea HTTP/1.1&quot; status: 500 len: 359 time: 0.0436440 从最后的trace信息来看是AttributeError，是glanceclient的问题。接下来是各种google，我们也可以在openstack官网查询，发现这么一个地址，https://bugs.launchpad.net/glance/+bug/1476770，原来是glanceclient的一个bug。你也可以再查看其他相关网址https://bugs.launchpad.net/python-glanceclient/+bug/1479296https://bugs.launchpad.net/python-glanceclient/+bug/1323660按照网站上面给出的建议，似乎要升级glanceclient至0.17.3，好吧，从下面的网址git clone源码， 再安装替换。https://github.com/openstack/python-glanceclient/tree/stable/kilo重启启动服务systemctl restart openstack-glance-api.service openstack-glance-registry.service最后我们看到实例最终创建成功。当然有的可能不一定是这个原因引起的，如果升级了还是没有解决这个问题，我们还要再尝试其他办法。像https://review.openstack.org/#/c/244899/这个帖子里说要替换http.py和images.py两个文件。如果还是不行，可能要考虑重新安装组件了。总之，时间允许的情况下，多尝试几次总是没错了，通过几次折腾，也会进一步了解openstack组件间的关系了。","tags":[{"name":"openstack","slug":"openstack","permalink":"http://blog.yaodataking.com/tags/openstack/"}]},{"title":"OpenStack入门之三:Kilo版本centos 7平台搭建","date":"2016-06-11T15:14:22.000Z","path":"2016/06/11/openstack-kilo-centos/","text":"虚拟机环境节点名称硬件配置功能/用途IP地址controller4G内存, 1块硬盘，1块网卡控制节点192.168.199.80compute-node-012G内存, 1块硬盘，1块网卡计算节点192.168.199.81network-node-011G内存, 1块硬盘，2块网卡网络节点192.168.199.82block-node-011G内存, 2块硬盘，1块网卡块存储节点192.168.199.83object-node-011G内存, 3块硬盘，1块网卡对象存储节点1192.168.199.84object-node-021G内存, 3块硬盘，1块网卡对象存储节点2192.168.199.85 组件节点分布 1.准备工作以下操作在每个节点上1.1更新centos源 #yum install wget #wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo1.2停止防火墙服务systemctl stop firewalld.servicesystemctl disable firewalld.service1.3安装基础组件及源 #yum install ntp -y #yum install yum-plugin-priorities -y #yum install http://dl.fedoraproject.org/pub/epel/7/x86_64/e/epel-release-7-6.noarch.rpm -y #yum install http://rdo.fedorapeople.org/openstack-kilo/rdo-release-kilo.rpm -y #yum install https://repos.fedorapeople.org/repos/openstack/openstack-kilo/rdo-release-kilo-2.noarch.rpm -y #yum upgrade -y #yum install openstack-selinux -y1.4更新时区 #cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime1.5配置时钟同步(controller节点)1.5.1备份配置文件 #cp /etc/ntp.conf /etc/ntp.conf.bak1.5.2添加参数 #vi /etc/ntp.conf server controller iburst restrict -4 default kod notrap nomodify restrict -6 default kod notrap nomodify `&lt;/pre&gt; 1.5.3注释以下参数 #restrict default nomodify notrap nopeer noquery #restrict 127.0.0.1 #restrict ::1 1.5.4启动时钟同步服务 #systemctl enable ntpd.service #systemctl start ntpd.service 1.6配置时钟同步(其它节点) 1.6.1备份配置文件 #cp /etc/ntp.conf /etc/ntp.conf.bak 1.6.2添加参数 #vi /etc/ntp.conf &lt;pre&gt;` server controller iburst `&lt;/pre&gt; 注释其他server项 1.7本环境所涉及到组件密码。 &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;用户名&lt;/th&gt; &lt;th&gt;密码&lt;/th&gt; &lt;th&gt;所属组件&lt;/th&gt; &lt;th&gt;用途&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt;&lt;tr&gt; &lt;td&gt;openstack&lt;/td&gt; &lt;td&gt;123456&lt;/td&gt; &lt;td&gt;rabbitMQ&lt;/td&gt; &lt;td&gt;Openstack组件间MQ消息通讯&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;admin&lt;/td&gt; &lt;td&gt;123456&lt;/td&gt; &lt;td&gt;Keystone&lt;/td&gt; &lt;td&gt;Keystone管理员&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;demo&lt;/td&gt; &lt;td&gt;123456&lt;/td&gt; &lt;td&gt;Keystone&lt;/td&gt; &lt;td&gt;访问租户项目demo&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;glance&lt;/td&gt; &lt;td&gt;123456&lt;/td&gt; &lt;td&gt;Keystone&lt;/td&gt; &lt;td&gt;glance用户&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;nova&lt;/td&gt; &lt;td&gt;123456&lt;/td&gt; &lt;td&gt;Keystone&lt;/td&gt; &lt;td&gt;nova用户&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;neutron&lt;/td&gt; &lt;td&gt;123456&lt;/td&gt; &lt;td&gt;Keystone&lt;/td&gt; &lt;td&gt;neutron用户&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;cinder&lt;/td&gt; &lt;td&gt;123456&lt;/td&gt; &lt;td&gt;Keystone&lt;/td&gt; &lt;td&gt;cinder用户&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;swift&lt;/td&gt; &lt;td&gt;123456&lt;/td&gt; &lt;td&gt;Keystone&lt;/td&gt; &lt;td&gt;swift用户&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;root&lt;/td&gt; &lt;td&gt;123456&lt;/td&gt; &lt;td&gt;mysql&lt;/td&gt; &lt;td&gt;mysql系统管理员&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;keystone&lt;/td&gt; &lt;td&gt;keystone&lt;/td&gt; &lt;td&gt;mysql&lt;/td&gt; &lt;td&gt;keystone组件访问mysql数据库&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;glance&lt;/td&gt; &lt;td&gt;glance&lt;/td&gt; &lt;td&gt;mysql&lt;/td&gt; &lt;td&gt;Glance组件访问mysql数据库&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;nova&lt;/td&gt; &lt;td&gt;nova&lt;/td&gt; &lt;td&gt;mysql&lt;/td&gt; &lt;td&gt;Nova组件访问mysql数据库&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;neutron&lt;/td&gt; &lt;td&gt;neutron&lt;/td&gt; &lt;td&gt;mysql&lt;/td&gt; &lt;td&gt;neutron组件访问mysql数据库&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;cinder&lt;/td&gt; &lt;td&gt;cinder&lt;/td&gt; &lt;td&gt;mysql&lt;/td&gt; &lt;td&gt;Cinder组件访问mysql数据库&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; ## 2.controller节点安装 2.1安装数据库 2.1.1安装mariadb #yum install mariadb mariadb-server MySQL-python -y 2.1.2备份配置文件 #cp /etc/my.cnf /etc/my.cnf.bak 2.1.3修改配置文件 #vi /etc/my.cnf &lt;pre&gt;` [mysqld] bind-address = controller default-storage-engine = innodb innodb_file_per_table collation-server = utf8_general_ci init-connect = &apos;SET NAMES utf8&apos; character-set-server = utf8 `&lt;/pre&gt; 2.1.4启动数据库服务 #systemctl enable mariadb.service #systemctl start mariadb.service 2.1.5配置数据库服务安全参数,设置root密码 #mysql_secure_installation &lt;pre&gt;` Remove anonymous users? [Y/n] n Disallow root login remotely? [Y/n] Y Remove test database and access to it? [Y/n] n Reload privilege tables now? [Y/n] Y `&lt;/pre&gt; 2.1.6验证mysql是否登录正常 #mysql -uroot -p 2.2安装rabbitmq 2.2.1安装rabbitmq-server yum install rabbitmq-server -y 2.2.2启动服务 #systemctl enable rabbitmq-server.service #systemctl start rabbitmq-server.service 2.2.3添加用户，并允许远程访问 #rabbitmqctl add_user openstack 123456 #rabbitmqctl set_permissions openstack &quot;.*&quot; &quot;.*&quot; &quot;.*&quot; 2.2.4验证 默认端口5672 #telent controller 5672 或者 #rabbitmqctl status 2.3安装keystone 2.3.1创建数据库keystone #mysql -uroot -p123456 &lt;pre&gt;` create database keystone; GRANT ALL PRIVILEGES ON keystone.* TO &apos;keystone&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;keystone&apos;; GRANT ALL PRIVILEGES ON keystone.* TO &apos;keystone&apos;@&apos;%&apos; IDENTIFIED BY &apos;keystone&apos;; `&lt;/pre&gt; 2.3.2安装keystone组件 #yum install openstack-keystone httpd mod_wsgi python-openstackclient memcached python-memcached -yh 2.3.3启动memcached #systemctl enable memcached.service #systemctl start memcached.service 2.3.4配置keystone.conf 2.3.4.1备份配置文件 #cp /etc/keystone/keystone.conf /etc/keystone/keystone.conf.bak 2.3.4.2生成token #openssl rand -hex 10 &gt; 0cadc88a6437bc7da5cd 2.3.4.3修改配置文件 #vi /etc/keystone/keystone.conf &lt;pre&gt;` [DEFAULT] admin_token = 0cadc88a6437bc7da5cd --用你生成的token替换 verbose = True [database] connection = mysql://keystone:keystone@localhost/keystone [memcache] servers = localhost:11211 [revoke] driver = keystone.contrib.revoke.backends.sql.Revoke [token] provider = keystone.token.providers.uuid.Provider driver = keystone.token.persistence.backends.memcache.Token `&lt;/pre&gt; 2.3.5初始化keystone数据库 2.3.5.1执行创建表脚本 #su -s /bin/sh -c &quot;keystone-manage db_sync&quot; keystone 2.3.5.2检查表是否均已创建完成 MariaDB [keystone]&amp;gt; show tables; &lt;pre&gt;` +------------------------+ | Tables_in_keystone | +------------------------+ | access_token | | assignment | | consumer | | credential | | domain | | endpoint | | endpoint_group | | federation_protocol | | group | | id_mapping | | identity_provider | | idp_remote_ids | | mapping | | migrate_version | | policy | | policy_association | | project | | project_endpoint | | project_endpoint_group | | region | | request_token | | revocation_event | | role | | sensitive_config | | service | | service_provider | | token | | trust | | trust_role | | user | | user_group_membership | | whitelisted_config | +------------------------+ 32 rows in set (0.00 sec) `&lt;/pre&gt; 2.3.6配置keystone的http服务 2.3.6.1备份配置文件httpd.conf #cp /etc/httpd/conf/httpd.conf /etc/httpd/conf/httpd.conf.bak 2.3.6.2修改配置文件httpd.conf #vi /etc/httpd/conf/httpd.conf &lt;pre&gt;` ServerName controller `&lt;/pre&gt; 2.3.6.3创建keystone的虚拟站点 #vi /etc/httpd/conf.d/wsgi-keystone.conf &lt;pre&gt;` Listen 5000 Listen 35357 WSGIDaemonProcess keystone-public processes=5 threads=1 user=keystone group=keystone display-name=%{GROUP} WSGIProcessGroup keystone-public WSGIScriptAlias / /var/www/cgi-bin/keystone/main WSGIApplicationGroup %{GLOBAL} WSGIPassAuthorization On LogLevel info ErrorLogFormat &quot;%{cu}t %M&quot; ErrorLog /var/log/httpd/keystone-error.log CustomLog /var/log/httpd/keystone-access.log combined WSGIDaemonProcess keystone-admin processes=5 threads=1 user=keystone group=keystone display-name=%{GROUP} WSGIProcessGroup keystone-admin WSGIScriptAlias / /var/www/cgi-bin/keystone/admin WSGIApplicationGroup %{GLOBAL} WSGIPassAuthorization On LogLevel info ErrorLogFormat &quot;%{cu}t %M&quot; ErrorLog /var/log/httpd/keystone-error.log CustomLog /var/log/httpd/keystone-access.log combined `&lt;/pre&gt; 2.3.6.4创建wsgi 目录 #mkdir -p /var/www/cgi-bin/keystone 2.3.6.5下载程序模板 #curl http://git.openstack.org/cgit/openstack/keystone/plain/httpd/keystone.py?h=stable/kilo \\ | tee /var/www/cgi-bin/keystone/main /var/www/cgi-bin/keystone/admin 2.3.6.6修改文件夹及文件权限 #chown -R keystone:keystone /var/www/cgi-bin/keystone #chmod 755 /var/www/cgi-bin/keystone/* 2.3.6.7启动httpServer #systemctl enable httpd.service #systemctl start httpd.service 2.3.6.8检查http服务是否正常启动 #systemctl status httpd.service 2.3.7注册keystone api服务 2.3.7.1配置系统环境变量(ADMIN_TOKEN为2.3.4.2步骤通过openssl命令生成的值) export OS_TOKEN=ADMIN_TOKEN export OS_URL=http://controller:35357/v2.0 2.3.7.2创建keystone服务信息 openstack service create --name keystone --description &quot;OpenStack Identity&quot; identity 2.3.7.3创建keystone服务实例 #openstack endpoint create \\ --publicurl http://controller:5000/v2.0 \\ --internalurl http://controller:5000/v2.0 \\ --adminurl http://controller:35357/v2.0 \\ --region RegionOne \\ identity 2.3.8创建项目、用户、角色等信息 2.3.8.1 创建项目:admin #openstack project create --description &quot;Admin Project&quot; admin 2.3.8.2 为项目admin创建用户：admin #openstack user create --password-prompt admin 2.3.8.3 创建角色:admin #openstack role create admin 2.3.8.4 将角色admin授权给用户admin #openstack role add --project admin --user admin admin 2.3.9 同样创建项目demo及相关用户信息 #openstack project create --description &quot;Demo Project&quot; demo #openstack user create --password-prompt demo #openstack role create user #openstack role add --project demo --user demo user 2.3.10 创建项目：service #openstack project create --description &quot;Service Project&quot; service 2.3.11 验证keystone服务 2.3.11.1 删除系统配置参数 #unset OS_TOKEN #unset OS_URL 2.3.11.2备份并修改keystone-dist-paste.ini配置文件 #cp /usr/share/keystone/keystone-dist-paste.ini /usr/share/keystone/keystone-dist-paste.ini.bak 删除以下配置参数中的admin_token_auth字符串 #vi /usr/share/keystone/keystone-dist-paste.ini 2.3.11.3为项目admin配置环境变量脚本 #vi /root/admin-openrc.sh &lt;pre&gt;` export OS_PROJECT_DOMAIN_ID=default export OS_USER_DOMAIN_ID=default export OS_PROJECT_NAME=admin export OS_TENANT_NAME=admin export OS_USERNAME=admin export OS_PASSWORD=123456 export OS_AUTH_URL=http://controller:5000/v3 `&lt;/pre&gt; 2.3.11.4为项目demo配置环境变量脚本 #vi /root/demo-openrc.sh &lt;pre&gt;` export OS_PROJECT_DOMAIN_ID=default export OS_USER_DOMAIN_ID=default export OS_PROJECT_NAME=demo export OS_TENANT_NAME=demo export OS_USERNAME=demo export OS_PASSWORD=123456 export OS_AUTH_URL=http://controller:5000/v3 `&lt;/pre&gt; 2.3.11.5 使用admin用户查看项目列表 [root@controller ~]#chmod +x /root/admin-openrc.sh [root@controller ~]#source /root/admin-openrc.sh [root@controller ~]#openstack project list &lt;pre&gt;` +----------------------------------+---------+ | ID | Name | +----------------------------------+---------+ | 54b423c7655146e691207d5c2e91d464 | demo | | 80c94cfaac6d4649b3cdb282cae6f406 | service | | dd3ebd91af2c40e7b2ddc411d8826321 | admin | +----------------------------------+---------+ `&lt;/pre&gt; 2.3.11.6使用demo用户获取token [root@controller ~]# source demo-openrc.sh [root@controller ~]# openstack token issue &lt;pre&gt;` +------------+----------------------------------+ | Field | Value | +------------+----------------------------------+ | expires | 2016-05-07T10:09:52.940266Z | | id | 0808f1b47d794029951d9f0dcb4da62d | | project_id | 54b423c7655146e691207d5c2e91d464 | | user_id | a8ebdcfb9d174e05b4d135a911aa5aad | +------------+----------------------------------+ `&lt;/pre&gt; 2.4安装镜像服务glance 2.4.1创建数据库 #mysql -uroot -p123456 &lt;pre&gt;` CREATE DATABASE glance; GRANT ALL PRIVILEGES ON glance.* TO &apos;glance&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;glance&apos;; GRANT ALL PRIVILEGES ON glance.* TO &apos;glance&apos;@&apos;%&apos; IDENTIFIED BY &apos;glance&apos;; `&lt;/pre&gt; 2.4.2创建用户：glance，并授权 #source /root/admin-openrc.sh #openstack user create --password-prompt glance #openstack role add --project service --user glance admin 2.4.3创建服务及服务实例 #openstack service create --name glance --description &quot;OpenStack Image service&quot; image #openstack endpoint create \\ --publicurl http://controller:9292 \\ --internalurl http://controller:9292 \\ --adminurl http://controller:9292 \\ --region RegionOne \\ image 2.4.4安装glance #yum install openstack-glance python-glance python-glanceclient -y 2.4.5配置glance api组件 2.4.5.1备份配置文件 #cp /etc/glance/glance-api.conf /etc/glance/glance-api.conf.bak 2.4.5.2修改配置文件 #vi /etc/glance/glance-api.conf &lt;pre&gt;` [DEFAULT] verbose = True notification_driver = noop [database] connection = mysql://glance:glance@localhost/glance [keystone_authtoken] auth_uri = http://controller:5000 auth_url = http://controller:35357 auth_plugin = password project_domain_id = default user_domain_id = default project_name = service username = glance password = 123456 [paste_deploy] flavor = keystone [glance_store] default_store = file filesystem_store_datadir = /var/lib/glance/images/ `&lt;/pre&gt; 2.4.6配置glance registry组件 2.4.6.1备份配置文件 #cp /etc/glance/glance-registry.conf /etc/glance/glance-registry.conf.bak 2.4.6.2修改配置文件 #vi /etc/glance/glance-registry.conf &lt;pre&gt;` [DEFAULT] verbose = True notification_driver = noop [database] connection = mysql://glance:glance@localhost/glance [keystone_authtoken] auth_uri = http://controller:5000 auth_url = http://controller:35357 auth_plugin = password project_domain_id = default user_domain_id = default project_name = service username = glance password = 123456 [paste_deploy] flavor = keystone `&lt;/pre&gt; 2.4.7初始化glance数据库 #su -s /bin/sh -c &quot;glance-manage db_sync&quot; glance 2.4.8启动glance服务 #systemctl enable openstack-glance-api.service openstack-glance-registry.service #systemctl start openstack-glance-api.service openstack-glance-registry.service 2.4.9验证glance服务 #echo &quot;export OS_IMAGE_API_VERSION=2&quot; | tee -a admin-openrc.sh \\ demo-openrc.sh #source /root/admin-openrc.sh #glance image-create --name &quot;cirros-0.3.4-x86_64&quot; \\ --file /root/cirros-0.3.4-x86_64-disk.img \\ --disk-format qcow2 --container-format bare --visibility public --progress 使用openstack image list查看镜像是否上传完成。 ## 3\\. Nova组件安装 3.1在controller节点安装 3.1.1创建数据库 #mysql -uroot -p123456 &lt;pre&gt;` CREATE DATABASE nova; GRANT ALL PRIVILEGES ON nova.* TO &apos;nova&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;nova&apos;; GRANT ALL PRIVILEGES ON nova.* TO &apos;nova&apos;@&apos;%&apos; IDENTIFIED BY &apos;nova&apos;; `&lt;/pre&gt; 3.1.2创建用户：nova，并授权用户权限 #openstack user create --password-prompt nova #openstack role add --project service --user nova admin 3.1.3创建服务及服务实例 #openstack service create --name nova \\ --description &quot;OpenStack Compute&quot; compute #openstack endpoint create --publicurl http://controller:8774/v2/%\\(tenant_id\\)s \\ --internalurl http://controller:8774/v2/%\\(tenant_id\\)s \\ --adminurl http://controller:8774/v2/%\\(tenant_id\\)s \\ --region RegionOne \\ compute 3.1.4安装nova组件 #yum install openstack-nova-api openstack-nova-cert openstack-nova-conductor \\ openstack-nova-console openstack-nova-novncproxy openstack-nova-scheduler \\ python-novaclient -y 3.1.5配置nova.conf 3.1.5.1备份配置文件 #cp /etc/nova/nova.conf /etc/nova/nova.conf.bak 3.1.5.2修改配置文件 #vi /etc/nova/nova.conf &lt;pre&gt;` [DEFAULT] verbose = True rpc_backend = rabbit auth_strategy = keystone #必须用ip my_ip = 192.168.199.80 vncserver_listen = 192.168.199.80 vncserver_proxyclient_address = 192.168.199.80 [oslo_messaging_rabbit] rabbit_host = controller rabbit_userid = openstack rabbit_password = 123456 [keystone_authtoken] auth_uri = http://controller:5000 auth_url = http://controller:35357 auth_plugin = password project_domain_id = default user_domain_id = default project_name = service username = nova password = 123456 [glance] host = controller [oslo_concurrency] lock_path = /var/lib/nova/tmp [database] connection = mysql://nova:nova@localhost/nova `&lt;/pre&gt; 3.1.6初始化nova数据库 #su -s /bin/sh -c &quot;nova-manage db sync&quot; nova 3.1.7启动nova #systemctl enable openstack-nova-api.service openstack-nova-cert.service \\ openstack-nova-consoleauth.service openstack-nova-scheduler.service \\ openstack-nova-conductor.service openstack-nova-novncproxy.service #systemctl start openstack-nova-api.service openstack-nova-cert.service \\ openstack-nova-consoleauth.service openstack-nova-scheduler.service \\ openstack-nova-conductor.service openstack-nova-novncproxy.service 3.2compute-node-01安装 3.2.1安装nova组件 #yum install openstack-nova-compute sysfsutils -y 3.2.2配置nova 3.2.2.1备份配置文件 #cp /etc/nova/nova.conf /etc/nova/nova.conf.bak 3.2.2.2修改配置文件 #vi /etc/nova/nova.conf &lt;pre&gt;` [DEFAULT] verbose = True rpc_backend = rabbit auth_strategy = keystone my_ip = 192.168.199.81 vnc_enabled = True vncserver_listen = 0.0.0.0 vncserver_proxyclient_address = 192.168.199.81 novncproxy_base_url = http://controller:6080/vnc_auto.html [oslo_messaging_rabbit] rabbit_host = controller rabbit_userid = openstack rabbit_password = 123456 [keystone_authtoken] auth_uri = http://controller:5000 auth_url = http://controller:35357 auth_plugin = password project_domain_id = default user_domain_id = default project_name = service username = nova password = 123456 [glance] host = controller [oslo_concurrency] lock_path = /var/lib/nova/tmp [libvirt] virt_type = qemu `&lt;/pre&gt; 3.2.3启动nova #systemctl enable libvirtd.service openstack-nova-compute.service #systemctl start libvirtd.service openstack-nova-compute.service 3.2.4验证 以下操作均在controller节点上操作 [root@controller ~]# source /root/admin-openrc.sh [root@controller ~]# nova service-list &lt;pre&gt;` +----+------------------+-----------------+----------+---------+-------+----------------------------+-----------------+ | Id | Binary | Host | Zone | Status | State | Updated_at | Disabled Reason | +----+------------------+-----------------+----------+---------+-------+----------------------------+-----------------+ | 1 | nova-consoleauth | controller | internal | enabled | up | 2016-05-07T13:55:42.000000 | - | | 2 | nova-conductor | controller | internal | enabled | up | 2016-05-07T13:55:42.000000 | - | | 3 | nova-scheduler | controller | internal | enabled | up | 2016-05-07T13:55:43.000000 | - | | 4 | nova-cert | controller | internal | enabled | up | 2016-05-07T13:55:43.000000 | - | | 5 | nova-compute | compute-node-01 | nova | enabled | up | 2016-05-07T13:55:44.000000 | - | +----+------------------+-----------------+----------+---------+-------+----------------------------+-----------------+ `&lt;/pre&gt; [root@controller ~]# nova image-list &lt;pre&gt;` +--------------------------------------+---------------------+--------+--------+ | ID | Name | Status | Server | +--------------------------------------+---------------------+--------+--------+ | 66f03a7f-cdb2-4111-84c0-0af53327329e | cirros-0.3.4-x86_64 | ACTIVE | | +--------------------------------------+---------------------+--------+--------+ `&lt;/pre&gt; ## 4.Neutron组件安装 4.1在controller节点安装 4.1.1创建数据库 #mysql -uroot -p123456 &lt;pre&gt;` create database neutron; GRANT ALL PRIVILEGES ON neutron.* TO &apos;neutron&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;neutron&apos;; GRANT ALL PRIVILEGES ON neutron.* TO &apos;neutron&apos;@&apos;%&apos; IDENTIFIED BY &apos;neutron&apos;; `&lt;/pre&gt; 4.1.2创建用户：neutron，并授权用户权限 #openstack user create --password-prompt neutron #openstack role add --project service --user neutron admin 4.1.3注册服务及服务实例 #openstack service create --name neutron \\ --description &quot;OpenStack Networking&quot; network #openstack endpoint create \\ --publicurl http://controller:9696 \\ --adminurl http://controller:9696 \\ --internalurl http://controller:9696 \\ --region RegionOne \\ network 4.1.4安装neutron组件 #yum install openstack-neutron openstack-neutron-ml2 \\ python-neutronclientwhich -y 4.1.5配置neutron.conf 4.1.5.1备份 #cp /etc/neutron/neutron.conf /etc/neutron/neutron.conf.bak 4.1.5.2修改配置文件 #vi /etc/neutron/neutron.conf &lt;pre&gt;` [DEFAULT] verbose = True rpc_backend = rabbit auth_strategy = keystone core_plugin = ml2 service_plugins = router allow_overlapping_ips = True notify_nova_on_port_status_changes = True notify_nova_on_port_data_changes = True nova_url = http://controller:8774/v2 [oslo_messaging_rabbit] rabbit_host = controller rabbit_userid = openstack rabbit_password = 123456 [database] connection = mysql://neutron:neutron@localhost/neutron [keystone_authtoken] auth_uri = http://controller:5000 auth_url = http://controller:35357 auth_plugin = password project_domain_id = default user_domain_id = default project_name = service username = neutron password = 123456 [nova] auth_url = http://controller:35357 auth_plugin = password project_domain_id = default user_domain_id = default region_name = RegionOne project_name = service username = nova password = 123456 `&lt;/pre&gt; 4.1.6配置ml2插件 4.1.6.1备份 #cp /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugins/ml2/ml2_conf.ini.bak 4.1.6.2配置 #vi /etc/neutron/plugins/ml2/ml2_conf.ini &lt;pre&gt;` [ml2] type_drivers = flat,vlan,gre,vxlan tenant_network_types = gre mechanism_drivers = openvswitch [ml2_type_gre] tunnel_id_ranges = 1:1000 [securitygroup] enable_security_group = True enable_ipset = True firewall_driver = neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver `&lt;/pre&gt; 4.1.6.3建立link文件 #ln -s /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugin.ini 4.1.7配置nova.conf 4.1.7.1备份 #cp /etc/nova/nova.conf /etc/nova/nova.conf.bak1 4.1.7.2配置 #vi /etc/nova/nova.conf &lt;pre&gt;` [DEFAULT] network_api_class = nova.network.neutronv2.api.API security_group_api = neutron linuxnet_interface_driver = nova.network.linux_net.LinuxOVSInterfaceDriver firewall_driver = nova.virt.firewall.NoopFirewallDriver [neutron] url = http://controller:9696 auth_strategy = keystone admin_auth_url = http://controller:35357/v2.0 admin_tenant_name = service admin_username = neutron admin_password = 123456 `&lt;/pre&gt; 4.1.8初始化数据库 #su -s /bin/sh -c &quot;neutron-db-manage --config-file /etc/neutron/neutron.conf \\ --config-file /etc/neutron/plugins/ml2/ml2_conf.ini upgrade head&quot; neutron 4.1.9重启nova服务 #systemctl restart openstack-nova-api.service openstack-nova-scheduler.service \\ openstack-nova-conductor.service 4.1.10启动neutron服务 #systemctl enable neutron-server.service #systemctl start neutron-server.service 4.1.11验证 [root@controller ~]#source /root/admin-openrc.sh [root@controller ~]# neutron ext-list &lt;pre&gt;` +-----------------------+-----------------------------------------------+ | alias | name | +-----------------------+-----------------------------------------------+ | security-group | security-group | | l3_agent_scheduler | L3 Agent Scheduler | | net-mtu | Network MTU | | ext-gw-mode | Neutron L3 Configurable external gateway mode | | binding | Port Binding | | provider | Provider Network | | agent | agent | | quotas | Quota management support | | subnet_allocation | Subnet Allocation | | dhcp_agent_scheduler | DHCP Agent Scheduler | | l3-ha | HA Router extension | | multi-provider | Multi Provider Network | | external-net | Neutron external network | | router | Neutron L3 Router | | allowed-address-pairs | Allowed Address Pairs | | extraroute | Neutron Extra Route | | extra_dhcp_opt | Neutron Extra DHCP opts | | dvr | Distributed Virtual Router | +-----------------------+-----------------------------------------------+ `&lt;/pre&gt; 4.2在network-node-01节点安装 4.2.1启用IP转发功能 4.2.1.1备份配置文件 #cp /etc/sysctl.conf /etc/sysctl.conf.bak 4.2.1.2修改配置 #vi /etc/sysctl.conf &lt;pre&gt;` net.ipv4.ip_forward=1 net.ipv4.conf.all.rp_filter=0 net.ipv4.conf.default.rp_filter=0 `&lt;/pre&gt; 4.2.1.3启用配置 #sysctl -p 4.2.2安装neutron组件 #yum install openstack-neutron openstack-neutron-ml2 \\ openstack-neutron-openvswitch -y 4.2.3配置neutron.conf 4.2.3.1备份 #cp /etc/neutron/neutron.conf /etc/neutron/neutron.conf.bak 4.2.3.2配置 #vi /etc/neutron/neutron.conf &lt;pre&gt;` [DEFAULT] verbose = True rpc_backend = rabbit auth_strategy = keystone core_plugin = ml2 service_plugins = router allow_overlapping_ips = True [oslo_messaging_rabbit] rabbit_host = controller rabbit_userid = openstack rabbit_password = 123456 [keystone_authtoken] auth_uri = http://controller:5000 auth_url = http://controller:35357 auth_plugin = password project_domain_id = default user_domain_id = default project_name = service username = neutron password = 123456 `&lt;/pre&gt; 4.2.4配置ML2插件 备注：配置【OVS】段中的IP为network-node-01节点的桥接网卡的地址，br-ex为虚拟网桥 4.2.4.1备份 #cp /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugins/ml2/ml2_conf.ini.bak 4.2.4.2配置 #vi /etc/neutron/plugins/ml2/ml2_conf.ini &lt;pre&gt;` [ml2] type_drivers = flat,vlan,gre,vxlan tenant_network_types = gre mechanism_drivers = openvswitch [ml2_type_flat] flat_networks = external [ml2_type_gre] tunnel_id_ranges = 1:1000 [securitygroup] enable_security_group = True enable_ipset = True firewall_driver = neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver [ovs] local_ip = 192.168.199.82 bridge_mappings = external:br-ex [agent] tunnel_types = gre `&lt;/pre&gt; 4.2.5创建链接文件 #ln -s /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugin.ini 4.2.6配置ML3插件 4.2.6.1备份 #cp /etc/neutron/l3_agent.ini /etc/neutron/l3_agent.ini.bak 4.2.6.2配置 #vi /etc/neutron/l3_agent.ini &lt;pre&gt;` [DEFAULT] verbose = True interface_driver = neutron.agent.linux.interface.OVSInterfaceDriver router_delete_namespaces = True `&lt;/pre&gt; 4.2.7配置DHCP插件 4.2.7.1备份 #cp /etc/neutron/dhcp_agent.ini /etc/neutron/dhcp_agent.ini.bak 4.2.7.2配置 #vi /etc/neutron/dhcp_agent.ini &lt;pre&gt;` verbose = True interface_driver = neutron.agent.linux.interface.OVSInterfaceDriver dhcp_driver = neutron.agent.linux.dhcp.Dnsmasq dhcp_delete_namespaces = True dnsmasq_config_file = /etc/neutron/dnsmasq-neutron.conf `&lt;/pre&gt; 4.2.7.3创建dnsmasq-neutron.conf并添加以下参数： #vi /etc/neutron/dnsmasq-neutron.conf &lt;pre&gt;` dhcp-option-force=26,1454 `&lt;/pre&gt; 4.2.8配置元数据插件 备注：metadata_proxy_shared_secret密码可以根据需求自行设置 4.2.8.1备份 #cp /etc/neutron/metadata_agent.ini /etc/neutron/metadata_agent.ini.bak 4.2.8.2配置 #vi /etc/neutron/metadata_agent.ini a.删除[DEFAULT]段中的默认配置 b.在[DEFAULT]段中增加以下内容 &lt;pre&gt;` verbose = True auth_uri = http://controller:5000 auth_url = http://controller:35357 auth_region = RegionOne auth_plugin = password project_domain_id = default user_domain_id = default project_name = service username = neutron password = 123456 nova_metadata_ip = controller metadata_proxy_shared_secret = 123456 `&lt;/pre&gt; 4.2.9配置nova.conf(在controller节点配置) 备注：metadata_proxy_shared_secret密码为4.2.8.2节配置的密码 4.2.9.1备份 #cp /etc/nova/nova.conf /etc/nova/nova.conf.bak2 4.2.9.2在[neutron]节点中增加以下内容 #vi /etc/nova/nova.conf &lt;pre&gt;` service_metadata_proxy = True metadata_proxy_shared_secret = 123456 `&lt;/pre&gt; 4.2.9.3重启nova服务 #systemctl restart openstack-nova-api.service openstack-nova-scheduler.service \\ openstack-nova-conductor.service 4.2.10启动服务 #systemctl enable openvswitch.service #systemctl start openvswitch.service 4.2.11创建虚拟路由器 4.2.11.1创建虚拟路由器 #ovs-vsctl add-br br-ex 4.2.11.2添加网卡到虚拟路由器（enp0s8为网络节点第二块网卡名，设备名称先通过ifconfig命令查看，另外这块网卡不需要分配IP地址） #ovs-vsctl add-port br-ex enp0s8 4.2.11.3禁止GRO #ethtool -K enp0s8 gro off 4.2.12启动服务 #systemctl enable neutron-openvswitch-agent.service neutron-l3-agent.service \\ neutron-dhcp-agent.service neutron-metadata-agent.service neutron-ovs-cleanup.service #systemctl start neutron-openvswitch-agent.service neutron-l3-agent.service \\ neutron-dhcp-agent.service neutron-metadata-agent.service 4.2.13验证(在controller节点) [root@controller ~]# source /root/admin-openrc.sh [root@controller ~]# neutron agent-list &lt;pre&gt;` +--------------------------------------+--------------------+-----------------+-------+----------------+---------------------------+ | id | agent_type | host | alive | admin_state_up | binary | +--------------------------------------+--------------------+-----------------+-------+----------------+---------------------------+ | 39affea9-d74b-4eea-857c-10d4e955278a | L3 agent | network-node-01 | :-) | True | neutron-l3-agent | | 56552345-4e2a-488c-a2a0-4cd5c4752a46 | Open vSwitch agent | network-node-01 | :-) | True | neutron-openvswitch-agent | | 8dc9de3c-723d-4e17-9da9-44671401f349 | Metadata agent | network-node-01 | :-) | True | neutron-metadata-agent | | a34a75b7-c3a1-4809-9b22-151604ea1e28 | DHCP agent | network-node-01 | :-) | True | neutron-dhcp-agent | +--------------------------------------+--------------------+-----------------+-------+----------------+---------------------------+ `&lt;/pre&gt; 4.3在compute-node-01节点安装 4.3.1配置IP转发 4.3.1.1备份 #cp /etc/sysctl.conf /etc/sysctl.conf.bak 4.3.1.2配置 #vi /etc/sysctl.conf &lt;pre&gt;` net.ipv4.conf.all.rp_filter=0 net.ipv4.conf.default.rp_filter=0 net.bridge.bridge-nf-call-iptables=1 net.bridge.bridge-nf-call-ip6tables=1 `&lt;/pre&gt; 4.3.1.3生效配置 #sysctl -p #modprobe bridge 4.3.2安装 #yum install openstack-neutron openstack-neutron-ml2 \\ openstack-neutron-openvswitch -y 4.3.3配置neutron 4.3.3.1备份 #cp /etc/neutron/neutron.conf /etc/neutron/neutron.conf.bak 4.3.3.2配置（需要删除[keystone_authtoken]中已存在的配置） #vi /etc/neutron/neutron.conf &lt;pre&gt;` [DEFAULT] verbose = True rpc_backend = rabbit auth_strategy = keystone core_plugin = ml2 service_plugins = router allow_overlapping_ips = True [oslo_messaging_rabbit] rabbit_host = controller rabbit_userid = openstack rabbit_password = 123456 [keystone_authtoken] auth_uri = http://controller:5000 auth_url = http://controller:35357 auth_plugin = password project_domain_id = default user_domain_id = default project_name = service username = neutron password = 123456 `&lt;/pre&gt; 4.3.4配置ml2插件 备注：配置【OVS】段中的IP为compute-node-01节点的桥接网卡的地址，br-ex为虚拟网桥 4.3.4.1备份 #cp /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugins/ml2/ml2_conf.ini.bak 4.3.4.2配置 #vi /etc/neutron/plugins/ml2/ml2_conf.ini &lt;pre&gt;` [ml2] type_drivers = flat,vlan,gre,vxlan tenant_network_types = gre mechanism_drivers = openvswitch [ml2_type_gre] tunnel_id_ranges = 1:1000 [securitygroup] enable_security_group = True enable_ipset = True firewall_driver = neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver [ovs] local_ip = 192.168.199.81 [agent] tunnel_types = gre `&lt;/pre&gt; 4.3.5创建link文件 #ln -s /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugin.ini 4.3.6配置nova.conf 4.3.6.1备份 #cp /etc/nova/nova.conf /etc/nova/nova.conf.bak1 4.3.6.2配置 vi /etc/nova/nova.conf &lt;pre&gt;` [DEFAULT] network_api_class = nova.network.neutronv2.api.API security_group_api = neutron linuxnet_interface_driver = nova.network.linux_net.LinuxOVSInterfaceDriver firewall_driver = nova.virt.firewall.NoopFirewallDriver [neutron] url = http://controller:9696 auth_strategy = keystone admin_auth_url = http://controller:35357/v2.0 admin_tenant_name = service admin_username = neutron admin_password = 123456 `&lt;/pre&gt; 4.3.7启动 #systemctl enable openvswitch.service #systemctl start openvswitch.service #systemctl enable neutron-openvswitch-agent.service #systemctl start neutron-openvswitch-agent.service 4.3.8验证(在controller节点) [root@controller ~]# neutron agent-list &lt;pre&gt;` +--------------------------------------+--------------------+-----------------+-------+----------------+---------------------------+ | id | agent_type | host | alive | admin_state_up | binary | +--------------------------------------+--------------------+-----------------+-------+----------------+---------------------------+ | 07647c62-f1af-4108-9f70-a5c01b870a9c | Open vSwitch agent | compute-node-01 | :-) | True | neutron-openvswitch-agent | | 39affea9-d74b-4eea-857c-10d4e955278a | L3 agent | network-node-01 | :-) | True | neutron-l3-agent | | 56552345-4e2a-488c-a2a0-4cd5c4752a46 | Open vSwitch agent | network-node-01 | :-) | True | neutron-openvswitch-agent | | 8dc9de3c-723d-4e17-9da9-44671401f349 | Metadata agent | network-node-01 | :-) | True | neutron-metadata-agent | | a34a75b7-c3a1-4809-9b22-151604ea1e28 | DHCP agent | network-node-01 | :-) | True | neutron-dhcp-agent | +--------------------------------------+--------------------+-----------------+-------+----------------+---------------------------+ `&lt;/pre&gt; 4.4 创建网络 4.4.1创建扩展网络 #neutron net-create ext-net --router:external \\ --provider:physical_network external --provider:network_type flat 4.4.2创建扩展网络子网 #neutron subnet-create ext-net 192.168.56.0/24 --name ext-subnet \\ --allocation-pool start=192.168.56.101,end=192.168.56.200 \\ --disable-dhcp --gateway 192.168.56.1 4.4.3创建租户网络 #source /root/demo-openrc.sh #neutron net-create demo-net #neutron subnet-create demo-net 192.168.2.0/24 \\ --name demo-subnet --gateway 192.168.2.1 4.4.4创建路由器 #neutron router-create demo-router #neutron router-interface-add demo-router demo-subnet #neutron router-gateway-set demo-router ext-net 4.4.5.设置安全组规则 #nova secgroup-add-rule default icmp -1 -1 0.0.0.0/0 #nova secgroup-add-rule default tcp 22 22 0.0.0.0/0 4.4.6.验证 #source /root/admin-openrc.sh #neutron router-list 4.5 controller节点安装Dashboard 4.5.1安装 #yum install openstack-dashboard -y 4.5.2配置 4.5.2.1备份 cp /etc/openstack-dashboard/local_settings /etc/openstack-dashboard/local_settings.bak 4.5.2.2配置 vi /etc/openstack-dashboard/local_settings &lt;pre&gt;` import sys reload(sys) sys.setdefaultencoding(&apos;utf8&apos;) OPENSTACK_HOST = &quot;controller&quot; ALLOWED_HOSTS = [&apos;*&apos;, ] CACHES = { &apos;default&apos;: { &apos;BACKEND&apos;: &apos;django.core.cache.backends.memcached.MemcachedCache&apos;, &apos;LOCATION&apos;: &apos;127.0.0.1:11211&apos;, } } OPENSTACK_KEYSTONE_DEFAULT_ROLE = &quot;user&quot; TIME_ZONE = &quot;Asia/Shanghai&quot; `&lt;/pre&gt; 4.5.3系统权限配置 #setsebool -P httpd_can_network_connect on #chown -R apache:apache /usr/share/openstack-dashboard/static #systemctl restart httpd.service memcached.service 4.5.4验证 http://192.168.199.80/dashboard demo/123456 [![2016-05-07_23-19-10](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/06/2016-05-07_23-19-10.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/06/2016-05-07_23-19-10.jpg) ## 5.Cinder组件安装 5.1Controller节点安装 5.1.1创建数据库 #mysql -uroot -p123456 &lt;pre&gt;` CREATE DATABASE cinder; GRANT ALL PRIVILEGES ON cinder.* TO &apos;cinder&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;cinder&apos;; GRANT ALL PRIVILEGES ON cinder.* TO &apos;cinder&apos;@&apos;%&apos; IDENTIFIED BY &apos;cinder&apos;; `&lt;/pre&gt; 5.1.2创建账号及服务 #source /root/admin-openrc.sh #openstack user create --password-prompt cinder #openstack role add --project service --user cinder admin #openstack service create --name cinder \\ --description &quot;OpenStack Block Storage&quot; volume #openstack service create --name cinderv2 \\ --description &quot;OpenStack Block Storage&quot; volumev2 #openstack endpoint create \\ --publicurl http://controller:8776/v2/%\\(tenant_id\\)s \\ --internalurl http://controller:8776/v2/%\\(tenant_id\\)s \\ --adminurl http://controller:8776/v2/%\\(tenant_id\\)s \\ --region RegionOne \\ volume #openstack endpoint create \\ --publicurl http://controller:8776/v2/%\\(tenant_id\\)s \\ --internalurl http://controller:8776/v2/%\\(tenant_id\\)s \\ --adminurl http://controller:8776/v2/%\\(tenant_id\\)s \\ --region RegionOne \\ volumev2 5.1.3安装Cinder组件 #yum install openstack-cinder python-cinderclient python-oslo-db -y 5.1.4配置 备注：my_ip为本机IP #cp /usr/share/cinder/cinder-dist.conf /etc/cinder/cinder.conf #chown -R cinder:cinder /etc/cinder/cinder.conf #vi /etc/cinder/cinder.conf &lt;pre&gt;`[DEFAULT] verbose = True rpc_backend = rabbit auth_strategy = keystone my_ip = 192.168.199.80 [database] connection = mysql://cinder:cinder@localhost/cinder [oslo_messaging_rabbit] rabbit_host = controller rabbit_userid = openstack rabbit_password = 123456 [keystone_authtoken] auth_uri = http://controller:5000 auth_url = http://controller:35357 auth_plugin = password project_domain_id = default user_domain_id = default project_name = service username = cinder password = 123456 [oslo_concurrency] lock_path = /var/lock/cinder `&lt;/pre&gt; 5.1.5数据初始化 #su -s /bin/sh -c &quot;cinder-manage db sync&quot; cinder 5.1.6启动服务 #systemctl enable openstack-cinder-api.service openstack-cinder-scheduler.service #systemctl start openstack-cinder-api.service openstack-cinder-scheduler.service 5.2在block-node-01节点上安装 5.2.1安装支持包 5.2.1.1安装 #yum install qemu lvm2 -y 5.2.1.2启动 #systemctl enable lvm2-lvmetad.service #systemctl start lvm2-lvmetad.service 5.2.2创建逻辑卷 #fdisk /dev/sdb &lt;pre&gt;` [root@block-node-01 ~]# fdisk /dev/sdb Welcome to fdisk (util-linux 2.23.2). Changes will remain in memory only, until you decide to write them. Be careful before using the write command. Device does not contain a recognized partition table Building a new DOS disklabel with disk identifier 0xb091be35. Command (m for help): n Partition type: p primary (0 primary, 0 extended, 4 free) e extended Select (default p): p Partition number (1-4, default 1): First sector (2048-4194303, default 2048): Using default value 2048 Last sector, +sectors or +size{K,M,G} (2048-4194303, default 4194303): Using default value 4194303 Partition 1 of type Linux and of size 2 GiB is set Command (m for help): t Selected partition 1 Hex code (type L to list all codes): 8e Changed type of partition &apos;Linux&apos; to &apos;Linux LVM&apos; Command (m for help): p Disk /dev/sdb: 2147 MB, 2147483648 bytes, 4194304 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk label type: dos Disk identifier: 0xb091be35 Device Boot Start End Blocks Id System /dev/sdb1 2048 4194303 2096128 8e Linux LVM Command (m for help):w Calling ioctl() to re-read partition table. Syncing disks. `&lt;/pre&gt; 5.2.3更新内核中的分区表 #partprobe 5.2.4创建物理卷及卷组 #pvcreate /dev/sdb1 #vgcreate cinder-volumes /dev/sdb1 5.2.5修改lvm服务配置 5.2.5.1备份 #cp /etc/lvm/lvm.conf /etc/lvm/lvm.conf.bak 5.2.5.2修改 #vi /etc/lvm/lvm.conf &lt;pre&gt;` devices { filter = [ &quot;a/sda/&quot;, &quot;a/sdb/&quot;, &quot;r/.*/&quot;] `&lt;/pre&gt; 5.2.5.3重启lvm服务 #systemctl restart lvm2-lvmetad.service 5.2.6安装cinder组件 #yum install openstack-cinder targetcli python-oslo-db python-oslo-log MySQL-python -y 5.2.7配置cinder 5.2.7.1备份 #cp /etc/cinder/cinder.conf /etc/cinder/cinder.conf.bak 5.2.7.2配置cinder.conf #vi /etc/cinder/cinder.conf &lt;pre&gt;`[DEFAULT] rpc_backend = rabbit auth_strategy = keystone my_ip = 192.168.199.83 enabled_backends = lvm glance_host = controller verbose = True [oslo_messaging_rabbit] rabbit_host = controller rabbit_userid = openstack rabbit_password = 123456 [database] connection = mysql://cinder:cinder@controller/cinder [keystone_authtoken] auth_uri = http://controller:5000 auth_url = http://controller:35357 auth_plugin = password project_domain_id = default user_domain_id = default project_name = service username = cinder password = 123456 [lvm] volume_driver = cinder.volume.drivers.lvm.LVMVolumeDriver volume_group = cinder-volumes iscsi_protocol = iscsi iscsi_helper = lioadm [oslo_concurrency] lock_path = /var/lock/cinder `&lt;/pre&gt; 5.2.7.3创建文件夹 #mkdir -p /var/lock/cinder #chown cinder:cinder /var/lock/cinder 5.2.8启动cinder服务 #systemctl enable openstack-cinder-volume.service target.service #systemctl start openstack-cinder-volume.service target.service 5.2.9验证(在controller节点) #echo &quot;export OS_VOLUME_API_VERSION=2&quot; | tee -a admin-openrc.sh demo-openrc.sh &lt;pre&gt;`[root@controller ~]# source /root/admin-openrc.sh [root@controller ~]# cinder service-list +------------------+-------------------+------+---------+-------+----------------------------+-----------------+ | Binary | Host | Zone | Status | State | Updated_at | Disabled Reason | +------------------+-------------------+------+---------+-------+----------------------------+-----------------+ | cinder-scheduler | controller | nova | enabled | up | 2016-05-22T04:41:04.000000 | - | | cinder-volume | block-node-01@lvm | nova | enabled | up | 2016-05-22T04:41:01.000000 | - | +------------------+-------------------+------+---------+-------+----------------------------+-----------------+ [root@controller ~]# source demo-openrc.sh [root@controller ~]# cinder create --name demo-volume1 1 +---------------------------------------+--------------------------------------+ | Property | Value | +---------------------------------------+--------------------------------------+ | attachments | [] | | availability_zone | nova | | bootable | false | | consistencygroup_id | None | | created_at | 2016-05-22T04:42:35.000000 | | description | None | | encrypted | False | | id | 75ed439d-9d42-4ae9-8615-c4f1b8ff9ae0 | | metadata | {} | | multiattach | False | | name | demo-volume1 | | os-vol-tenant-attr:tenant_id | 54b423c7655146e691207d5c2e91d464 | | os-volume-replication:driver_data | None | | os-volume-replication:extended_status | None | | replication_status | disabled | | size | 1 | | snapshot_id | None | | source_volid | None | | status | creating | | user_id | a8ebdcfb9d174e05b4d135a911aa5aad | | volume_type | None | +---------------------------------------+--------------------------------------+ [root@controller ~]# cinder list +--------------------------------------+-----------+--------------+------+-------------+----------+-------------+ | ID | Status | Name | Size | Volume Type | Bootable | Attached to | +--------------------------------------+-----------+--------------+------+-------------+----------+-------------+ | 75ed439d-9d42-4ae9-8615-c4f1b8ff9ae0 | available | demo-volume1 | 1 | - | false | | +--------------------------------------+-----------+--------------+------+-------------+----------+-------------+ `&lt;/pre&gt; ## 6.Swift组件安装 6.1在controller节点安装 6.1.1创建账号及服务 #source /root/admin-openrc.sh #openstack user create --password-prompt swift #openstack role add --project service --user swift admin #openstack service create --name swift \\ --description &quot;OpenStack Object Storage&quot; object-store #openstack endpoint create \\ --publicurl &apos;http://controller:8080/v1/AUTH_%(tenant_id)s&apos; \\ --internalurl &apos;http://controller:8080/v1/AUTH_%(tenant_id)s&apos; \\ --adminurl http://controller:8080 \\ --region RegionOne \\ object-store 6.1.2安装swift组件 #yum install openstack-swift-proxy python-swiftclient python-keystoneclient \\ python-keystonemiddleware memcached -y 6.1.3配置proxy-server 6.1.3.1下载配置文件模板 #curl -o /etc/swift/proxy-server.conf \\ https://git.openstack.org/cgit/openstack/swift/plain/etc/proxy-server.conf-sample?h=stable/kilo &lt;pre&gt;` [root@controller ~]# curl -o /etc/swift/proxy-server.conf \\ &amp;gt; https://git.openstack.org/cgit/openstack/swift/plain/etc/proxy-server.conf-sample?h=stable/kilo % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 28586 100 28586 0 0 13043 0 0:00:02 0:00:02 --:--:-- 13047 `&lt;/pre&gt; 6.1.3.2修改配置文件 #vi /etc/swift/proxy-server.conf &lt;pre&gt;` [DEFAULT] bind_port = 8080 user = swift swift_dir = /etc/swift [pipeline:main] pipeline = catch_errors gatekeeper healthcheck proxy-logging cache container_sync bulk ratelimit authtoken keystoneauth container-quotas account-quotas slo dlo proxy-logging proxy-server [app:proxy-server] use = egg:swift#proxy account_autocreate = true [filter:keystoneauth] use = egg:swift#keystoneauth operator_roles = admin,user [filter:healthcheck] use = egg:swift#healthcheck [filter:authtoken] paste.filter_factory = keystonemiddleware.auth_token:filter_factory auth_uri = http://controller:5000 auth_url = http://controller:35357 auth_plugin = password project_domain_id = default user_domain_id = default project_name = service username = swift password = 123456 delay_auth_decision = true [filter:cache] memcache_servers = 127.0.0.1:11211 `&lt;/pre&gt; 6.2在object-node-01节点安装 6.2.1系统配置 6.2.1.1安装系统组件 #yum install xfsprogs rsync -y 6.2.1.2格式化磁盘 #fdisk /dev/sdb #fdisk /dev/sdc #mkfs.xfs /dev/sdb1 #mkfs.xfs /dev/sdc1 6.2.1.3创建文件夹 #mkdir -p /srv/node/sdb1 #mkdir -p /srv/node/sdc1 6.2.1.4修改fstab #vi /etc/fstab &lt;pre&gt;`/dev/sdb1 /srv/node/sdb1 xfs noatime,nodiratime,nobarrier,logbufs=8 0 2 /dev/sdc1 /srv/node/sdc1 xfs noatime,nodiratime,nobarrier,logbufs=8 0 2 `&lt;/pre&gt; 6.2.1.5挂载磁盘到文件夹 #mount /srv/node/sdb1 #mount /srv/node/sdc1 6.2.1.6修改文件同步服务 #cp /etc/rsyncd.conf /etc/rsyncd.conf.bak #vi /etc/rsyncd.conf &lt;pre&gt;` uid = swift gid = swift log file = /var/log/rsyncd.log pid file = /var/run/rsyncd.pid address = 192.168.199.84 [account] max connections = 2 path = /srv/node/ read only = false lock file = /var/lock/account.lock [container] max connections = 2 path = /srv/node/ read only = false lock file = /var/lock/container.lock [object] max connections = 2 path = /srv/node/ read only = false lock file = /var/lock/object.lock `&lt;/pre&gt; 6.2.1.7启动文件同步服务 #systemctl enable rsyncd.service #systemctl start rsyncd.service 6.2.2安装 #yum install openstack-swift-account openstack-swift-container \\ openstack-swift-object -y 6.2.3配置swift 6.2.3.1下载配置文件模板 #curl -o /etc/swift/account-server.conf \\ https://git.openstack.org/cgit/openstack/swift/plain/etc/account-server.conf-sample?h=stable/kilo #curl -o /etc/swift/container-server.conf \\ https://git.openstack.org/cgit/openstack/swift/plain/etc/container-server.conf-sample?h=stable/kilo #curl -o /etc/swift/object-server.conf \\ https://git.openstack.org/cgit/openstack/swift/plain/etc/object-server.conf-sample?h=stable/kilo #curl -o /etc/swift/container-reconciler.conf \\ https://git.openstack.org/cgit/openstack/swift/plain/etc/container-reconciler.conf-sample?h=stable/kilo #curl -o /etc/swift/object-expirer.conf \\ https://git.openstack.org/cgit/openstack/swift/plain/etc/object-expirer.conf-sample?h=stable/kilo #curl -o /etc/swift/swift.conf \\ https://git.openstack.org/cgit/openstack/swift/plain/etc/swift.conf-sample?h=stable/kilo 6.2.3.2修改/etc/swift/account-server.conf &lt;pre&gt;` [DEFAULT] bind_ip = 192.168.199.84 bind_port = 6002 user = swift swift_dir = /etc/swift devices = /srv/node [pipeline:main] pipeline = healthcheck recon account-server [filter:healthcheck] use = egg:swift#healthcheck [filter:recon] use = egg:swift#recon recon_cache_path = /var/cache/swift `&lt;/pre&gt; 6.2.3.3修改/etc/swift/container-server.conf &lt;pre&gt;` [DEFAULT] bind_ip = 192.168.199.84 bind_port = 6001 user = swift swift_dir = /etc/swift devices = /srv/node [pipeline:main] pipeline = healthcheck recon container-server [filter:healthcheck] use = egg:swift#healthcheck [filter:recon] use = egg:swift#recon recon_cache_path = /var/cache/swift `&lt;/pre&gt; 6.2.3.4修改/etc/swift/object-server.conf &lt;pre&gt;` [DEFAULT] bind_ip = 192.168.199.84 bind_port = 6000 user = swift swift_dir = /etc/swift devices = /srv/node [pipeline:main] pipeline = healthcheck recon object-server [filter:healthcheck] use = egg:swift#healthcheck [filter:recon] use = egg:swift#recon recon_cache_path = /var/cache/swift recon_lock_path = /var/lock `&lt;/pre&gt; 6.2.3.5修改/etc/swift/swift.conf 备注：HASH_PATH_SUFFIX 和 HASH_PATH_PREFIX 分别用openssl rand -hex 10命令生成。 &lt;pre&gt;`[swift-hash] swift_hash_path_suffix = HASH_PATH_SUFFIX swift_hash_path_prefix = HASH_PATH_PREFIX [storage-policy:0] name = Policy-0 default = yes 6.2.3.6文件夹授权 #chown -R swift:swift /srv/node #mkdir -p /var/cache/swift #chown -R swift:swift /var/cache/swift #chown -R swift:swift /etc/swift 6.3在object-node-02节点安装参照6.2章节安装。配置文件可以自己从object-node-01节点拷贝过来，只需要修改配置文件中的IP地址即可。6.4controller节点创建环（Ring）备注：IP地址分别为object-node-01和object-node-02所对应的IP6.4.1创建账号环（Account Ring）swift-ring-builder account.builder create 10 3 1swift-ring-builder account.builder add r1z1-192.168.199.84:6002/sdb1 100swift-ring-builder account.builder add r1z1-192.168.199.84:6002/sdc1 100swift-ring-builder account.builder add r1z2-192.168.199.85:6002/sdb1 100swift-ring-builder account.builder add r1z2-192.168.199.85:6002/sdc1 100swift-ring-builder account.builderswift-ring-builder account.builder rebalance6.4.2创建容器环（Container Ring)swift-ring-builder container.builder create 10 3 1swift-ring-builder container.builder create 10 3 1swift-ring-builder container.builder add r1z1-192.168.199.84:6001/sdb1 100swift-ring-builder container.builder add r1z1-192.168.199.84:6001/sdc1 100swift-ring-builder container.builder add r1z2-192.168.199.85:6001/sdb1 100swift-ring-builder container.builder add r1z2-192.168.199.85:6001/sdc1 100swift-ring-builder container.builderswift-ring-builder container.builder rebalance6.4.3创建对象环（Object Ring)swift-ring-builder object.builder create 10 3 1swift-ring-builder object.builder create 10 3 1swift-ring-builder object.builder add r1z1-192.168.199.84:6000/sdb1 100swift-ring-builder object.builder add r1z1-192.168.199.84:6000/sdc1 100swift-ring-builder object.builder add r1z2-192.168.199.85:6000/sdb1 100swift-ring-builder object.builder add r1z2-192.168.199.85:6000/sdc1 100swift-ring-builder object.builderswift-ring-builder object.builder rebalance6.4.4启动服务(controller节点执行) #systemctl enable openstack-swift-proxy.service #systemctl start openstack-swift-proxy.service #systemctl restart memcached.service6.5启动服务（object-node-01, object-node-02节点执行） #systemctl enable openstack-swift-account.service openstack-swift-account-auditor.service \\openstack-swift-account-reaper.service openstack-swift-account-replicator.service #systemctl start openstack-swift-account.service openstack-swift-account-auditor.service \\openstack-swift-account-reaper.service openstack-swift-account-replicator.service #systemctl enable openstack-swift-container.service openstack-swift-container-auditor.service \\openstack-swift-container-replicator.service openstack-swift-container-updater.service #systemctl start openstack-swift-container.service openstack-swift-container-auditor.service \\openstack-swift-container-replicator.service openstack-swift-container-updater.service #systemctl enable openstack-swift-object.service openstack-swift-object-auditor.service \\openstack-swift-object-replicator.service openstack-swift-object-updater.service #systemctl start openstack-swift-object.service openstack-swift-object-auditor.service \\openstack-swift-object-replicator.service openstack-swift-object-updater.service","tags":[{"name":"openstack","slug":"openstack","permalink":"http://blog.yaodataking.com/tags/openstack/"}]},{"title":"《罗马人的故事4:凯撒时代（上）》读书笔记","date":"2016-06-05T06:54:08.000Z","path":"2016/06/05/rome-story-4/","text":"天才之所以为天才只因为他能超越时代的藩篱，但天才之所以能超越时代也正是拜当时的时势所赐。恺撒37岁这年，出人意料的当选了终身制的大祭司，正是从这一年开始，凯撒逐渐地掌控了罗马的权力。“人不管是谁都无法看清现实中的一切，大多数人只希望看到自己想看到的和想要的现实而已。” 这是出自凯撒的一句话。不管什么说，天才的凯撒看到了，他生活的时代，是罗马共和国的尾声。恺撒深感公元前6世纪以来一直持续着的罗马型寡头共和政体，并不适合公元前1世纪中已成为超级大国的罗马现状，必须确立取而代之的秩序。所以，他以在高卢战役中所获得的声望为依托，在执政官这个现行体制内寻求改革之道。于是，历史上就留下了好多凯撒的创举：政治上，古今的史学家无不一致公认”三头政治“是他的杰作。经济上，当年逼死格拉古兄弟的《农地法》在凯撒巧妙的安排下顺利通过了。军事上，战无不胜的凯撒更是开创了很多以少胜多的战例，著名的阿莱夏攻防战， 以不足5万人的兵力，击破内侧8万、外侧26万的敌人，从而奠定了高卢战争的结局。文学上， 《高卢战记》是唯一一部作为战地指挥官详细记载战争过程的重要作品，同时期著名演讲家西塞罗评价说：“它们值得最高称赞，因为它们文风简朴不事雕琢，直率而优美。它们不需任何演说术修饰，就像不穿衣衫的裸体显露其天生丽质那样。在历史记述体裁中没有比这种纯净清澈、简明扼要的叙述更令人满意了。”就像现在意大利普通高中使用的教科书说的那样，“要成为一位领导者，必须具备以下五种特质：智慧、说服力、忍耐力、自控力和坚强的意志。唯有凯撒拥有全部的特质。”","tags":[{"name":"罗马","slug":"罗马","permalink":"http://blog.yaodataking.com/tags/罗马/"}]},{"title":"OpenStack入门之二:devstack开发环境搭建","date":"2016-05-15T06:26:59.000Z","path":"2016/05/15/openstack-devstack/","text":"devstack是用来给开发人员快速部署Openstack的开发环境。操作系统环境：Ubuntu 14.04操作系统ISO安装可以从各大镜像网站下载，这里选择阿里镜像。http://mirrors.aliyun.com/ubuntu-releases/操作系统安装过程略。我们给操作系统分配2块网卡，一为桥接模式，网卡地址192.168.199.20，一为NAT模式。以下是安装devstack过程。1.更新源在软件包管理中心“软件源”中选择“中国的服务器”下mirros.aliyun.com即可自动使用或者直接编辑文件sudo vi /etc/apt/source.listsudo apt-get update2.安装gitsudo apt-get install git3.下载devstack源代码sudo git clone -b stable/kilo https://git.openstack.org/openstack-dev/devstack4.创建用户sudo devstack/tools/create-stack-user.shsudo su - stack5.拷贝devstack目录到/opt/stack下sudo cp -r /home/alex/devstack .6.修改devstack目录权限sudo chown -R stack:stack devstack7.执行安装脚本cd devstack./stack.sh漫长的安装过程，如果安装过程有问题，可以使用git单独下载源代码7.1 使用git单独下载源代码 git clone -b stable/kilo git://git.openstack.org/openstack/horizon.git /opt/stack/horizon git clone -b stable/kilo git://git.openstack.org/openstack/keystone.git /opt/stack/keystone git clone -b stable/kilo git://git.openstack.org/openstack/nova.git /opt/stack/nova git clone -b stable/kilo git://git.openstack.org/openstack/neutron.git /opt/stack/neutron git clone -b stable/kilo git://git.openstack.org/openstack/glance.git /opt/stack/glance git clone -b stable/kilo git://git.openstack.org/openstack/cinder.git /opt/stack/cinder `&lt;/pre&gt; 安装完成，显示结果如下。如果安装过程错误，使用./unstack.sh卸载，再使用./stack.sh重新安装。 [![2016-05-14_0-16-45](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/05/2016-05-14_0-16-45.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/05/2016-05-14_0-16-45.jpg) 8.修改配置文件 编辑local.conf，主要更新IP地址。 &lt;pre&gt;` [[local|localrc]] HOST_IP=192.168.199.20 SERVICE_HOST=$HOST_IP MYSQL_HOST=$HOST_IP RABBIT_HOST=$HOST_IP GLANCE_HOSTPORT=$HOST_IP:9292 ADMIN_PASSWORD=passw0rd DATABASE_PASSWORD=$ADMIN_PASSWORD RABBIT_PASSWORD=$ADMIN_PASSWORD SERVICE_PASSWORD=$ADMIN_PASSWORD SERVICE_TOKEN=$ADMIN_PASSWORD # Do not use Nova-Network disable_service n-net ENABLED_SERVICES+=,q-svc,q-dhcp,q-meta,q-agt,q-l3 ## Neutron options Q_USE_SECGROUP=True FLOATING_RANGE=&quot;192.168.199.0/24&quot; FIXED_RANGE=&quot;10.0.0.0/24&quot; Q_FLOATING_ALLOCATION_POOL=start=192.168.199.21,end=192.168.199.29 PUBLIC_NETWORK_GATEWAY=&quot;192.168.199.1&quot; Q_L3_ENABLED=True PUBLIC_INTERFACE=eth1 Q_USE_PROVIDERNET_FOR_PUBLIC=True OVS_PHYSICAL_BRIDGE=br-ex PUBLIC_BRIDGE=br-ex OVS_BRIDGE_MAPPINGS=public:br-ex `&lt;/pre&gt; 9.设置屏幕监控 screen -list screen -r 4573 script /dev/null [![2016-05-14_0-24-47](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/05/2016-05-14_0-24-47.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/05/2016-05-14_0-24-47.jpg) 10.安装jdk 10.1.安装 cd /opt/stack sudo mkdir -p /usr/lib/java sudo tar -zxvf jdk-8u51-linux-x64.gz -C /usr/lib/java 注，jdk文件可以Oracle网站下载，这里不再赘述。 10.2.配置环境变量 vi .bashrc &lt;pre&gt;` export JAVA_HOME=/usr/lib/java/jdk1.8.0_51 export JRE_HOME=${JAVA_HOME}/jre export CLASSPATH=${JAVA_HOME}/lib:${JRE_HOME}/jre:${CLASSPATH} export PATH=${JAVA_HOME}/bin:${PATH} `&lt;/pre&gt; source .bashrc 10.安装eclipse 10.1从以下网址下载eclipse luna版本 http://www.eclipse.org/downloads/download.php?file=/technology/epp/downloads/release/luna/SR2/eclipse-java-luna-SR2-linux-gtk-x86_64.tar.gz 10.2 编辑配置文件，修改内存参数 vi eclipse.ini &lt;pre&gt;` XX maxPermSize=1024m -Xms40m -Xmx1024m `&lt;/pre&gt; 10.3启动xhost，使所有用户都能访问Xserver. Xhost + 10.4启动eclipse [![2016-05-15_13-49-13](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/05/2016-05-15_13-49-13.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/05/2016-05-15_13-49-13.jpg) 设置workspace为/opt/stack 11.安装pydev 以下地址下载pydev插件 https://marketplace.eclipse.org/content/pydev-python-ide-eclipse 选择信任证书 [![2016-05-15_13-52-26](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/05/2016-05-15_13-52-26.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/05/2016-05-15_13-52-26.jpg) 配置 eclipse-&amp;gt;windows-&amp;gt;preference-&amp;gt;PyDev-&amp;gt;interpreters-&amp;gt;python interpreter-&amp;gt;Quick Auto config [![2016-05-14_1-33-27](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/05/2016-05-14_1-33-27.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/05/2016-05-14_1-33-27.jpg) 至此开发环境搭建完成。 12.验证开发环境是否搭建成功 我们试着创建horizon项目，首先备份原有配置文件 12.1备份原有配置文件 cd /opt/stack/horizon/openstack-dashboard/local/ 备份原有的配置文件 mv local_settings.py local_settings.py.bak mv local_settings.pyc local_settings.pyc.bak cp local_settings.py.example local_settings.py 12.2设置并启动debug manager.py debug configration -&amp;gt;arguments runserver 0.0.0.0:9000 [![2016-05-15_14-04-34](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/05/2016-05-15_14-04-34.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/05/2016-05-15_14-04-34.jpg) 12.3登录9000端口 http://192.168.199.20:9000 使用admin 账号登录 [![2016-05-14_1-55-17](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/05/2016-05-14_1-55-17.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/05/2016-05-14_1-55-17.jpg) 我们看到成功登录，至此可以验证开发环境搭建成功。 13.devstack终止或重启服务 在devstack目录下，运行 ./rejoin-stack.sh，进入控制台。如果出现以下错误，使用script /dev/null重定向。 stack@devstack:~/devstack$ ./rejoin-stack.sh Attaching to already started screen session.. Cannot open your terminal &apos;/dev/pts/39&apos; - please check. stack@devstack:~/devstack$ script /dev/null Script started, file is /dev/null stack@devstack:~/devstack$ ./rejoin-stack.sh 下面是显示信息 &lt;pre&gt;` 2016-05-15 10:17:41.278 DEBUG nova.openstack.common.periodic_task [req-39a9042c-a2f3-4e52-8a5d-5bb1ca80daf8 None None] Running periodic task ComputeManager._check_instance_build_time from (pid=4175) run_periodic_tasks /opt/stack/nova/nova/openstack/common/periodic_task.py:219 2016-05-15 10:17:41.279 DEBUG nova.openstack.common.loopingcall [req-39a9042c-a2f3-4e52-8a5d-5bb1ca80daf8 None None] Dynamic looping call &amp;lt;bound method Service.periodic_tasks of &amp;gt; sleeping for 1.01 seconds from (pid=4175) _inner /opt/stack/nova/nova/openstack/common/loopingcall.py:132 2016-05-15 10:17:42.289 DEBUG nova.openstack.common.periodic_task [req-39a9042c-a2f3-4e52-8a5d-5bb1ca80daf8 None None] Running periodic task ComputeManager._poll_rebooting_instances from (pid=4175) run_periodic_tasks /opt/stack/nova/nova/openstack/common/periodic_task.py:219 2016-05-15 10:17:42.291 DEBUG nova.openstack.common.periodic_task [req-39a9042c-a2f3-4e52-8a5d-5bb1ca80daf8 None None] Running periodic task ComputeManager._poll_unconfirmed_resizes from (pid=4175) run_periodic_tasks /opt/stack/nova/nova/openstack/common/periodic_task.py:219 2016-05-15 10:17:42.292 DEBUG nova.openstack.common.loopingcall [req-39a9042c-a2f3-4e52-8a5d-5bb1ca80daf8 None None] Dynamic looping call &amp;lt;bound method Service.periodic_tasks of &amp;gt; sleeping for 56.00 seconds from (pid=4175) _inner /opt/stack/nova/nova/openstack/common/loopingcall.py:132 n-sch 15$(L) n-novnc 16$(L) n-cpu* 17$(L) c-api 18-$(L) c-sch 19$(L) c-vol 屏幕最后一行的“n-cpu”表示的是nova-compute服务，前面的16表示这个服务的编号，表示是当前服务。切换不同服务的方法为按ctrl+a+’(即ctrl+a+单引号)，这时屏幕左下角会显示“Switch to window:”表示要前往的服务控制台，你可以输入17，表示看c-api (cinder-api)服务的情况。停止服务的方法是在在相应控制台下使用：ctrl+c，再启动这个服务是按下“↑”（即向上键），然后在按enter键。退出控制的方法是使用ctrl+d。","tags":[{"name":"devstack","slug":"devstack","permalink":"http://blog.yaodataking.com/tags/devstack/"},{"name":"openstack","slug":"openstack","permalink":"http://blog.yaodataking.com/tags/openstack/"}]},{"title":"《罗马人的故事3:胜者的迷思》读书笔记","date":"2016-05-14T15:59:20.000Z","path":"2016/05/14/rome-story-3/","text":"在今天看来，正是以消灭罗马为终生夙愿的汉尼拨给了罗马更加强大的力量，让罗马的霸权扩张到了地中海全境。“所谓敌人，不过是那些迫使我们自己变得强大的人”。也许这句话就是从这里来的吧。但是成功是必须付出代价的，罗马人也不例外。主要表现在，首先，随着罗马领土的扩大和人口的增长，平民和贵族之间的矛盾日益尖锐，连年的征战，让罗马公民中的低收入者无法靠种地积累财富，而贵族则通过每次战争中购买奴隶和新征服的土地，大肆发战争财。其次，罗马与同盟国之间的权利与义务的矛盾不断加大。虽然同盟战争使得《尤里乌斯公民权法》发布，所有同盟国家公民都获得了罗马的公民权，这对罗马来说是具有里程碑意义上的关键一步，但是，老公民与新公民的权利并不平等。最后，兵役人口持续减少，军队战斗力大大减弱。而军队的改革使得罗马军队逐渐沦为将领的私人军队。纵观共和制罗马的兴盛，非一人英雄之力，乃国家体制之功。在国家治理模式上，形成了不断试错不断完善的良性机制，在这个机制中，元老院功不可没。但是元老院有着先天的不足，元老院的议员不是罗马公民选举出来的。护民官与元老院的冲突流血一直贯穿着这一阶段的历史， 格拉古兄弟相继死于与元老院的冲突，他们的改革最终流产。虽然后来苏拉推行了一系列改革，苏拉进行政治体制改革的目的，是重建立足于“少数领导制”原则下的固有的共和政体，但是 “苏拉体制”在苏拉死后不到8年的时间里就土崩瓦解了。主要的反对力量不是出自反苏拉的阵营，在苏拉生前的严酷打击下该阵营已经没有杰出人物了，相反苏拉门下以庞培和克拉苏为代表的军界强人是推倒“苏拉体制”的主要力量，“苏拉体制”实际上已经无法解决公元前1世纪罗马所面对的现实问题。那么，共和制罗马到底何去何从呢？我们已经知道，靠元老院自身的改革已经无法解决罗马所面对的现实问题，我们也知道罗马的基因里是没有独裁统治，没有个人英雄崇拜的。但是时势可以造就英雄， 马略、苏拉、庞培应该说都是当时期罗马的英雄人物，但是他们都没有把罗马从“迷失”状态中走出来，罗马还需要继续变革。克服汉尼拔所形容的“内脏疾患”，以适应身体的正常生长，的确需要伟大的人物发挥重要作用。这个人是谁呢？","tags":[{"name":"罗马","slug":"罗马","permalink":"http://blog.yaodataking.com/tags/罗马/"}]},{"title":"OpenStack入门之一:OpenStack概述","date":"2016-05-08T15:51:33.000Z","path":"2016/05/08/openstack-introduce/","text":"一、OpenStack定义：OpenStack既是一个社区，也是一个项目和一个开源软件，它提供了一个部署云的操作平台或工具集。其宗旨在于，帮助组织运行为虚拟计算或存储服务的云，为公有云、私有云，也为大云、小云提供可扩展的、灵活的云计算。OpenStack通过各种互补的服务提供了基础设施即服务（IaaS）的解决方案，每个服务提供API以进行集成。二、Openstack项目组成1.6个核心项目1)Compute Service (“Nova”): 计算资源生命周期管理组件2)NetWork Service (“Neutron”):提供云计算环境下的虚拟网路功能3)Block Storage Service(“Cinder”):管理计算实例所使用的块级存储4)Object Storage Service(“Swift”):对象存储，用于永久类型的静态数据的长期存储5)Image Service (“Glance”):提供虚拟机镜像的发现注册获取服务6)Identity Service(“Keystone”):提供了用户信息管理为其他组件提供认证服务2.13个可选服务1)Horizon:Dashboard OpenStack中各种服务的Web管理门户，用于简化用户对服务的操作，例如：启动实例、分配IP地址、配置访问控制等。2)Ceilometer:Telemetry 像一个漏斗一样，能把OpenStack内部发生的几乎所有的事件都收集起来，然后为计费和监控以及其它服务提供数据支撑。3)Heat:Orchestration 提供了一种通过模板定义的协同部署方式，实现云基础设施软件运行环境（计算、存储和网络资源）的自动化部署。4)Trove:Database 为用户在OpenStack的环境提供可扩展和可靠的关系和非关系数据库引擎服务。5)Sahara:Elastic Map Reduce 刻意快速且经济高效地处理大数据。6)Ironic:Bare-Metal Provisioning 提供裸机管理服务。7)Zaqar:Messaging Service 为web开发者提供多租户的云消息服务。8)Manila:Shared Filesystems 将不同的文件接口接到统一的API上，跨越不同文件系统形成共享池。9)Designate:DNS Service 提供DNS服务10)Barbican:Key Management 提供密钥管理11)Magnum:Containers 容器服务12)Murano:Application Catalog 应用目录13)Congress:Governance 治理三、Openstack的发展历史Openstack自从2010年发布一个版本Austin以来，以每六个月的速度更新一个版本，每个版本以26个英文字母为首字母从A到Z顺序命名。目前最新版本为Mitaka。四、Openstack开发语言所有代码均采用Python开发五、架构设计特点：1.无中心：可以通过增加组件部署实例来实现水平扩展2.分布式：由多个逻辑和物理上均可分离的组件共同实现3.无状态：所有组件无本地持久化状态数据4.异步执行：部分执行流通过消息机制实现异步执行5.插件化可配置：大量使用插件机制、配置参数实现灵活的扩展与变更6.Restful API","tags":[{"name":"openstack","slug":"openstack","permalink":"http://blog.yaodataking.com/tags/openstack/"}]},{"title":"Mycat源码分析之Catlet Sharejoin分析","date":"2016-04-30T15:47:48.000Z","path":"2016/04/30/mycat-sourcecode-catlet-sharejoin/","text":"Catlet是一个实现了Catlet接口的无状态Java类，负责将编码实现某个SQL的处理过程，并返回响应报文给客户端，目前主要用于人工编码实现跨分片SQL的处理逻辑，至今Mycat已经提供了BatchInsertSequence，MyHellowJoin和ShareJoin三个demo案例，本文我们通过分析ShareJoin来看看Catlet是怎么处理的。ShareJoin是一个简单的跨分片Join,基于HBT的方式实现。目前支持2个表的join,原理就是解析SQL语句，拆分成单表的SQL语句执行，然后把各个节点的数据汇集。1.Catlet接口定义首先我们看到Catlet接口定义了processSQL和route。这样我们就可以通过编写自己的processSQL和route来实现特殊的要求。 public interface Catlet { /* * execute sql in EngineCtx and return result to client */ void processSQL(String sql, EngineCtx ctx); void route(SystemConfig sysConfig, SchemaConfig schema, int sqlType, String realSQL, String charset, ServerConnection sc, LayerCachePool cachePool) ; //void setRoute(RouteResultset rrs); } `&lt;/pre&gt; 2.ShareJoin类的processSQL [![2016-04-30_23-06-26](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/04/2016-04-30_23-06-26.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/04/2016-04-30_23-06-26.jpg) 我们看到JoinParser首先解析SQL语句，ShareDBJoinHandler是执行后获取数据的handler，由sqlengine.EngineCtx的executeNativeSQLSequnceJob去执行SQLjob，并有AllJobFinishedListener侦听是否所有任务完成。 3.相关类图 下图使整个与sharejoin相关的类图， [![class](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/04/class.png)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/04/class.png) 4.实际SQL测试 那么实际的sharejoin是不是就是这么实现的呢？我们来实际测试一下。 我们假设两个表company是全局表，customer是跨两个分区的表，我们现在需要JOIN这两个表来抓取数据。SQL语句如下， select a.*,b.id, b.name as tit from customer a,company b on a.company_id=b.id; 正常情况下，mycat能不能解析这个SQL语句呢？ [![2016-05-01_10-01-32](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/04/2016-05-01_10-01-32.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/04/2016-05-01_10-01-32.jpg) 以上我们看到这样一个普通的JOIN,正常的路由是无法解析这个SQL语句的。好，我们现在来看看sharejoin的威力。 [![2016-05-01_10-01-48](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/04/2016-05-01_10-01-48.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/04/2016-05-01_10-01-48.jpg) 我们看到，通过sharejoin的路由，成功的解析了这个SQL语句并且返回了正确的数据。查看mycat.log, 我们看到执行的步骤是跟上面源码看到的是一致的，catlet分别拆分成单表的SQL语句执行并汇总，最后侦听是否所有任务完成后发送数据ok。 &lt;pre&gt;` 04/30 14:02:44.726 INFO [$_NIOREACTOR-0-RW] (JoinParser.java:70) -SQL: select a.*,b.id, b.name as tit from customer a,company b on a.company_id=b.id 04/30 14:02:44.731 INFO [$_NIOREACTOR-0-RW] (ShareJoin.java:157) -Catlet exec:dn1,dn2, sql:select *, company_id from customer 04/30 14:02:44.742 INFO [$_NIOREACTOR-0-RW] (ShareJoin.java:241) -SQLParallJob:dn1, sql:select id, id , name as tit from company where id in (1,2,3) 04/30 14:02:44.751 INFO [$_NIOREACTOR-0-RW] (EngineCtx.java:171) -all job finished for front connection: ServerConnection [id=102, schema=TESTDB, host=172.17.0.1, user=test,txIsolation=3, autocommit=true, schema=TESTDB] 04/30 14:02:44.751 INFO [$_NIOREACTOR-0-RW] (EngineCtx.java:159) -write eof ,packgId:13 04/30 14:02:44.751 INFO [$_NIOREACTOR-0-RW] (ShareJoin.java:166) -发送数据OK 小结，解决跨分片的SQL JOIN的问题，远比我们想象的复杂，而且往往无法实现高效的处理，Mycat提供的这个Catlet接口可以让我们自己实现一些特定的算法。目前的实现sharejoin还是相对来说简单的，我们可以在此基础上去实现更加复杂的JOIN算法，分组算法，排序算法等等。 参考：Mycat权威指南","tags":[{"name":"Mycat","slug":"Mycat","permalink":"http://blog.yaodataking.com/tags/Mycat/"},{"name":"catlet","slug":"catlet","permalink":"http://blog.yaodataking.com/tags/catlet/"}]},{"title":"《罗马人的故事2:汉尼拔战记》读书笔记","date":"2016-04-30T09:11:44.000Z","path":"2016/04/30/rome-story-2/","text":"《罗马人的故事2:汉尼拔战记》描述了自公元前264至公元前146迦太基最终被灭亡之间罗马开始称霸地中海的历史。在第一次布匿战争前，迦太基一直是地中海的霸主，汉尼拔在第二次布匿战争前8年，也是所向披靡，战争的主导权一直在汉尼拔手中。但是为什么胜利的天平还是倒向了罗马呢？我觉得这与罗马用500年时间建国的建立的强大基因的上升趋势是分不开的。首先，不用说罗马的统一战线联盟体制，它不是简单的罗马和其他城邦国家的集合体。罗马联盟的架构体系分四类，首先第一类是盟主罗马，该国居民只要是自由民，不分贵族平民，一律享有罗马市民权，他们有选举权和被选举权，并且有义务服兵役。第二类是自治城市，罗马授予这些自治城市的居民以“无选举权的市民权”，他们不享有罗马国的选举权和被选举权，但是，除此之外的所有事情上，享有和罗马市民相同的权利。第三类是罗马殖民地和拉丁殖民地。主要是战略要塞，战士和当地女子通婚后，生下混血儿，以此进行同化。第四类是同盟城市或同盟国。罗马承认他们完全的国内自治，罗马允许他们拥有双重国籍。对于罗马联盟中的自治城市、殖民地、同盟国都没有义务向罗马交纳年贡或租税，但他们必须提供兵员。罗马与同盟者之间建立起的各种关系，不是统治与被统治的关系，而是共存共荣的关系。他们既不是榨取，也不是利用，而是信义。同时，罗马在基础设施建设方面与这些同盟国实行了统一的规划和建设，使它们成为有机的统一体。其次，罗马独特的共和政体的政治体系，通过执政官制度、元老院制度和市民大会，基本消除了国内的对立关系，因为只有罗马本国自由民才享有市民权，所以罗马自由民的参与意识还是很强的。军事天才西庇阿按照规定是不能在30岁那年成为执政官的，但是那年的市民大会，他被选举为执政官之一，元老院也只能接受。如果不是这样，那么取得第二次布匿战争的胜利没有那么快的到来。当然，这一切都源于罗马人的开放与包容。罗马人从不追究战败者的责任，罗马人也从来没想过自己必须在任何领域都是第一，伊特鲁里亚人依然在土木事业上施展他们的才华，意大利南部的希腊被委以通商业的职责，艺术、哲学、数学领域，罗马完全寄托在希腊人身上，罗马人还热衷学习希腊语。然而不可否认的是，在一步步的征服过程中，罗马也渐渐在迷失，以前一向对被征服者仁慈的罗马人，也渐渐变得暴戾起来，史无前例的将科林斯、迦太基和努曼提亚等被征服的城邦夷为平地。也许罗马想要这样的惩罚来警戒其他的同盟者，然而历史表明，这样的夷为平地副作用太多。而罗马的共和体制，也在一步步滑向帝国的深渊，当然这时期的罗马，还在处于胜利者的亢奋中。","tags":[{"name":"罗马","slug":"罗马","permalink":"http://blog.yaodataking.com/tags/罗马/"}]},{"title":"Mycat源码分析之后端连接处理","date":"2016-04-23T09:18:23.000Z","path":"2016/04/23/mycat-sourcecode-backend-response/","text":"Mycat是一个彻底开源的新颖的数据库中间件产品,它接受客户端SQL请求，根据路由分片发送至后端数据库集群，然后返回响应数据给客户端。它有效解决了传统数据库的瓶颈问题，从而使数据库的高可用，高负载成为可能。那么它的内部是怎么实现的呢？本文我们就Mycat源码分析研究一下后端连接处理的实现方式与内部机制，这里抛砖引玉,希望与感兴趣的朋友共同交流探讨。本文源码分析基于mycat1.6，地址如下：https://github.com/MyCATApache/Mycat-Server/tree/1.6/src/main/java/io/mycat 1.流程图后端连接处理流程主要指mycat server接收到路由以后下发SQL语句至具体的datahost执行并返回报文的一段过程。以下是主要流程图。2. routeEndExecuteSQL类ServerConnection包含routeEndExecuteSQL方法，路由计算成功，去调用类NonBlockingSession的execute方法。 public void routeEndExecuteSQL(String sql, int type, SchemaConfig schema) { // 路由计算 RouteResultset rrs = null; try { rrs = MycatServer .getInstance() .getRouterservice() .route(MycatServer.getInstance().getConfig().getSystem(), schema, type, sql, this.charset, this); } catch (Exception e) { StringBuilder s = new StringBuilder(); LOGGER.warn(s.append(this).append(sql).toString() + &quot; err:&quot; + e.toString(),e); String msg = e.getMessage(); writeErrMessage(ErrorCode.ER_PARSE_ERROR, msg == null ? e.getClass().getSimpleName() : msg); return; } if (rrs != null) { // session执行 session.execute(rrs, type); } } `&lt;/pre&gt; 3.类NonBlockingSession的execute方法 类NonBlockingSession的execute方法对路由结果做了判断，如果不存在任何需要派发的节点则直接返回，如果是单节点，则调用类singleNodeHandler方法execute，如果是多节点，则调用类MultiNodeQueryHandler方法execute。 &lt;pre&gt;` public void execute(RouteResultset rrs, int type) { // clear prev execute resources clearHandlesResources(); if (LOGGER.isDebugEnabled()) { StringBuilder s = new StringBuilder(); LOGGER.debug(s.append(source).append(rrs).toString() + &quot; rrs &quot;); } // 检查路由结果是否为空 RouteResultsetNode[] nodes = rrs.getNodes(); if (nodes == null || nodes.length == 0 || nodes[0].getName() == null || nodes[0].getName().equals(&quot;&quot;)) { source.writeErrMessage(ErrorCode.ER_NO_DB_ERROR, &quot;No dataNode found ,please check tables defined in schema:&quot; + source.getSchema()); return; } if (nodes.length == 1) { singleNodeHandler = new SingleNodeHandler(rrs, this); if( this.isPrepared() ) { singleNodeHandler.setPrepared(true); } try { singleNodeHandler.execute(); } catch (Exception e) { LOGGER.warn(new StringBuilder().append(source).append(rrs).toString(), e); source.writeErrMessage(ErrorCode.ERR_HANDLE_DATA, e.toString()); } } else { boolean autocommit = source.isAutocommit(); multiNodeHandler = new MultiNodeQueryHandler(type, rrs, autocommit, this); if(this.isPrepared()) { multiNodeHandler.setPrepared(true); } try { multiNodeHandler.execute(); } catch (Exception e) { LOGGER.warn(new StringBuilder().append(source).append(rrs).toString(), e); source.writeErrMessage(ErrorCode.ERR_HANDLE_DATA, e.toString()); } } if(this.isPrepared()) { this.setPrepared(false); } } `&lt;/pre&gt; 4.类MultiNodeQueryHandler的execute方法 单节点与多节点的原理是一样的，只是多节点多了一层循环，对每个datanode分别进行了同样的操作。这里先判断session是否已经有该datanode关联的后端连接session.tryExistsCon，如果已有，则调用_execute方法下发SQL指令；反之，则调用getConnection方法从连接池中获取一个可用连接或新建一个连接。 &lt;pre&gt;` public void execute() throws Exception { final ReentrantLock lock = this.lock; lock.lock(); try { this.reset(rrs.getNodes().length); this.fieldsReturned = false; this.affectedRows = 0L; this.insertId = 0L; } finally { lock.unlock(); } MycatConfig conf = MycatServer.getInstance().getConfig(); startTime = System.currentTimeMillis(); LOGGER.debug(&quot;rrs.getRunOnSlave()-&quot; + rrs.getRunOnSlave()); for (final RouteResultsetNode node : rrs.getNodes()) { BackendConnection conn = session.getTarget(node); if (session.tryExistsCon(conn, node)) { LOGGER.debug(&quot;node.getRunOnSlave()-&quot; + node.getRunOnSlave()); node.setRunOnSlave(rrs.getRunOnSlave()); // 实现 master/slave注解 LOGGER.debug(&quot;node.getRunOnSlave()-&quot; + node.getRunOnSlave()); _execute(conn, node); } else { // create new connection LOGGER.debug(&quot;node.getRunOnSlave()1-&quot; + node.getRunOnSlave()); node.setRunOnSlave(rrs.getRunOnSlave()); // 实现 master/slave注解 LOGGER.debug(&quot;node.getRunOnSlave()2-&quot; + node.getRunOnSlave()); PhysicalDBNode dn = conf.getDataNodes().get(node.getName()); dn.getConnection(dn.getDatabase(), autocommit, node, this, node); // 注意该方法不仅仅是获取连接，获取新连接成功之后，会通过层层回调，最后回调到本类 的connectionAcquired // 这是通过 上面方法的 this 参数的层层传递完成的。 // connectionAcquired 进行执行操作: // session.bindConnection(node, conn); // _execute(conn, node); } } } `&lt;/pre&gt; 5.类MySQLConnection的execute方法 这里execute方法判断是否已执行，同时同步并执行synAndDoExecute方法，最后调用底层命令sendQueryCmd。 &lt;pre&gt;` public void execute(RouteResultsetNode rrn, ServerConnection sc, boolean autocommit) throws UnsupportedEncodingException { if (!modifiedSQLExecuted &amp;amp;&amp;amp; rrn.isModifySQL()) { modifiedSQLExecuted = true; } String xaTXID = sc.getSession2().getXaTXID(); synAndDoExecute(xaTXID, rrn, sc.getCharsetIndex(), sc.getTxIsolation(), autocommit); } private void synAndDoExecute(String xaTxID, RouteResultsetNode rrn, int clientCharSetIndex, int clientTxIsoLation, boolean clientAutoCommit) { String xaCmd = null; boolean conAutoComit = this.autocommit; String conSchema = this.schema; // never executed modify sql,so auto commit boolean expectAutocommit = !modifiedSQLExecuted || isFromSlaveDB() || clientAutoCommit; if (expectAutocommit == false &amp;amp;&amp;amp; xaTxID != null &amp;amp;&amp;amp; xaStatus == 0) { clientTxIsoLation = Isolations.SERIALIZABLE; xaCmd = &quot;XA START &quot; + xaTxID + &apos;;&apos;; } int schemaSyn = conSchema.equals(oldSchema) ? 0 : 1; int charsetSyn = 0; if (this.charsetIndex != clientCharSetIndex) { //need to syn the charset of connection. //set current connection charset to client charset. //otherwise while sending commend to server the charset will not coincidence. setCharset(CharsetUtil.getCharset(clientCharSetIndex)); charsetSyn = 1; } int txIsoLationSyn = (txIsolation == clientTxIsoLation) ? 0 : 1; int autoCommitSyn = (conAutoComit == expectAutocommit) ? 0 : 1; int synCount = schemaSyn + charsetSyn + txIsoLationSyn + autoCommitSyn; if (synCount == 0) { // not need syn connection sendQueryCmd(rrn.getStatement()); return; } CommandPacket schemaCmd = null; StringBuilder sb = new StringBuilder(); if (schemaSyn == 1) { schemaCmd = getChangeSchemaCommand(conSchema); // getChangeSchemaCommand(sb, conSchema); } if (charsetSyn == 1) { getCharsetCommand(sb, clientCharSetIndex); } if (txIsoLationSyn == 1) { getTxIsolationCommand(sb, clientTxIsoLation); } if (autoCommitSyn == 1) { getAutocommitCommand(sb, expectAutocommit); } if (xaCmd != null) { sb.append(xaCmd); } if (LOGGER.isDebugEnabled()) { LOGGER.debug(&quot;con need syn ,total syn cmd &quot; + synCount + &quot; commands &quot; + sb.toString() + &quot;schema change:&quot; + (schemaCmd != null) + &quot; con:&quot; + this); } metaDataSyned = false; statusSync = new StatusSync(xaCmd != null, conSchema, clientCharSetIndex, clientTxIsoLation, expectAutocommit, synCount); // syn schema if (schemaCmd != null) { schemaCmd.write(this); } // and our query sql to multi command at last sb.append(rrn.getStatement()); // syn and execute others this.sendQueryCmd(sb.toString()); // waiting syn result... } `&lt;/pre&gt; 6\\. 类MultiNodeQueryHandler的okResponse方法 在类MySQLConnection的execute方法执行前，其实我们已经通过conn.setResponseHandler(this)将接收返回的数据报文。 &lt;pre&gt;` public void okResponse(byte[] data, BackendConnection conn) { this.netOutBytes += data.length; boolean executeResponse = conn.syncAndExcute(); if (LOGGER.isDebugEnabled()) { LOGGER.debug(&quot;received ok response ,executeResponse:&quot; + executeResponse + &quot; from &quot; + conn); } if (executeResponse) { ServerConnection source = session.getSource(); OkPacket ok = new OkPacket(); ok.read(data); //存储过程 boolean isCanClose2Client =(!rrs.isCallStatement()) ||(rrs.isCallStatement() &amp;amp;&amp;amp;!rrs.getProcedure().isResultSimpleValue());; if(!isCallProcedure) { if (clearIfSessionClosed(session)) { return; } else if (canClose(conn, false)) { return; } } lock.lock(); try { // 判断是否是全局表，如果是，执行行数不做累加，以最后一次执行的为准。 if (!rrs.isGlobalTable()) { affectedRows += ok.affectedRows; } else { affectedRows = ok.affectedRows; } if (ok.insertId &amp;gt; 0) { insertId = (insertId == 0) ? ok.insertId : Math.min( insertId, ok.insertId); } } finally { lock.unlock(); } // 对于存储过程，其比较特殊，查询结果返回EndRow报文以后，还会再返回一个OK报文，才算结束 boolean isEndPacket = isCallProcedure ? decrementOkCountBy(1) : decrementCountBy(1); if (isEndPacket&amp;amp;&amp;amp;isCanClose2Client) { if (this.autocommit) {// clear all connections session.releaseConnections(false); } if (this.isFail() || session.closed()) { tryErrorFinished(true); return; } lock.lock(); try { if (rrs.isLoadData()) { byte lastPackId = source.getLoadDataInfileHandler() .getLastPackId(); ok.packetId = ++lastPackId;// OK_PACKET ok.message = (&quot;Records: &quot; + affectedRows + &quot; Deleted: 0 Skipped: 0 Warnings: 0&quot;) .getBytes();// 此处信息只是为了控制台给人看的 source.getLoadDataInfileHandler().clear(); } else { ok.packetId = ++packetId;// OK_PACKET } ok.affectedRows = affectedRows; ok.serverStatus = source.isAutocommit() ? 2 : 1; if (insertId &amp;gt; 0) { ok.insertId = insertId; source.setLastInsertId(insertId); } ok.write(source); } catch (Exception e) { handleDataProcessException(e); } finally { lock.unlock(); } } } } 小结，对比1.5正式版本，1.6版本重构了包名，框架看上去更加清晰。我这里只是粗略的对后端连接处理做了分解，肯定有错误之处，还望谅解。","tags":[{"name":"Mycat","slug":"Mycat","permalink":"http://blog.yaodataking.com/tags/Mycat/"}]},{"title":"《罗马人的故事1:罗马不是一天建成的》读书笔记","date":"2016-04-17T15:55:22.000Z","path":"2016/04/17/rome-story-1/","text":"《罗马人的故事1： 罗马不是一天建成的》是日本作家盐野七生《罗马人的故事》系列作品（共15部）中的第一部，讲述的是古罗马从公元前753年建国到公元前270年完成了意大利半岛统一的五百年间的历史。罗马的确不是一天建成的，五百年对古罗马人来说，才刚刚开始。那么，古罗马人究竟是一些怎样的人？其智力不及希腊人，其体力不及高卢人和日耳曼人，其技术不及埃特鲁利亚人，其经济不及迦太基人，但为什么古罗马人能够一一打败对手，建立并维持一个庞大的罗马帝国？ 关于古罗马兴盛的要因，三位希腊人分别是这样认为的：狄俄尼索斯认为是宗教影响了罗马人的见解。罗马的宗教与其说是用来约束人类的，不如说是用来保佑人类的。这一宗教没有狂热崇拜的倾向，所以，更容易与其他民族建立起相互包容的关系，而不是对立的关系。承认其他宗教就是承认别的民族。波利比乌斯认为罗马兴盛的要因在于罗马确立了独特的政治体系。像王政、贵族政体、民主政体，都比较容易倾向于代表共同体的一部分利益。但是，罗马人不执著于政体。波利比乌斯把罗马兴盛的要因归结于罗马共和政体所特有的政治体系。那就是通过执政官制度、元老院制度和市民大会有效利用王政、贵族政体和民主政体的优势。由于确立了这一独特的政治体系，罗马消除了国内的对立关系，建立了统一的体制。普鲁塔克则明确指出罗马兴盛的关键在于他们采取同化失败者的生活方式。 其实我认为，他们说的都对，而且三点缺一不可。特别是罗马人的开放与包容与同时期的希腊人形成了鲜明的对比，希腊的雅典与斯巴达两个城邦的互不相容， 正是希腊衰落的一个原因。 这本书的故事，很有看头，但是我们的目的不是看故事本身，而是在寻找经验，寻找启发，寻找如何在当今的全球化世界里确定自身的目标。 另外，如果比较同时期的中国历史，会发现很多有意思的问题，也会引起你更多的思考，更多的假设，假如中国这样做，会怎样呢？ 总之，这是一本值得推荐你去看的书。","tags":[{"name":"罗马","slug":"罗马","permalink":"http://blog.yaodataking.com/tags/罗马/"}]},{"title":"Kubernetes入门实战(6)：使用ku8eye快速构建Kubernetes集群","date":"2016-04-10T05:44:59.000Z","path":"2016/04/10/kubernetes-ku8eye/","text":"Ku8eye是Kubernetes的Web一站式管理系统,具有如下的目标：1.图形化一键安装部署多节点的Kubernetes集群。是安装部署谷歌Kubernetes集群的最快以及最佳方式，安装流程会参考当前系统环境，提供默认优化的集群安装参数，实现最佳部署。2.支持多角色多租户的Portal管理界面。通过一个集中化的Portal界面，运营团队可以很方便的调整集群配置以及管理集群资源，实现跨部门的角色及用户管理、多租户管理，通过自助服务可以很容易完成Kubernetes集群的运维管理工作。3.制定一个Kubernetes应用的程序发布包标准(ku8package)并提供一个向导工具，使得专门为Kubernetes设计的应用能够很容易从本地环境中发布到公有云和其他环境中，更进一步的，我们还提供了Kubernetes应用可视化的构建工具，实现Kubernetes Service、RC、Pod以及其他资源的可视化构建和管理功能4.可定制化的监控和告警系统。内建很多系统健康检查工具用来检测和发现异常并触发告警事件，不仅可以监控集群中的所有节点和组件（包括Docker与Kubernetes），还能够很容易的监控业务应用的性能，我们提供了一个强大的Dashboard，可以用来生成各种复杂的监控图表以展示历史信息，并且可以用来自定义相关监控指标的告警阀值。5.具备的综合的、全面的故障排查能力。平台提供唯一的、集中化的日志管理工具，日志系统从集群中各个节点拉取日志并做聚合分析，拉取的日志包括系统日志和用户程序日志，并且提供全文检索能力以方便故障分析和问题排查，检索的信息包括相关告警信息，而历史视图和相关的度量数据则告诉你，什么时候发生了什么事情，有助于快速了解相关时间内系统的行为特征。6.实现Dockers与kubernetes项目的持续集成功能。提供一个可视化工具驱动持续集成的整个流程，包括创建新的Docker镜像、Push镜像到私有仓库中、创建一个Kubernetes测试环境进行测试以及最终滚动升级到生产环境中等各个主要环节。本文就从制作ku8eye镜像文件开始，演示如何使用ku8eye。准备四台虚拟机，环境centos 7,内存2G。 IP 功能 192.168.199.50 Ku8eye 192.168.199.54 Master 192.168.199.55 Node1 192.168.199.56 Node2一、制作Ku8eye镜像文件1.Ku8eye镜像文件 from centos:7 MAINTAINER Alex Wu RUN yum -y install wget #get update from aliyun RUN wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo # set timezone ENV TZ Asia/Shanghai # 1\\. install ansible RUN yum clean all &amp;amp;&amp;amp; \\ yum -y install epel-release &amp;amp;&amp;amp; \\ yum -y install PyYAML python-jinja2 python-httplib2 python-keyczar python-paramiko python-setuptools git python-pip RUN mkdir /etc/ansible/ &amp;amp;&amp;amp; echo -e &apos;[local]\\nlocalhost&apos; &amp;gt; /etc/ansible/hosts RUN pip install ansible # 2\\. install sshpass, and generate ssh keys RUN yum -y install sshpass RUN ssh-keygen -q -t rsa -N &quot;&quot; -f ~/.ssh/id_rsa # make ansible not do key checking from ~/.ssh/known_hosts file ENV ANSIBLE_HOST_KEY_CHECKING false # 3\\. install MariaDB (mysql) RUN yum -y install MariaDB-server MariaDB-client # 4\\. install supervisor RUN pip install supervisor # 5\\. add JRE1.8 COPY jre1.8.0_65 /root/jre1.8.0_65 ENV JAVA_HOME=&quot;/root/jre1.8.0_65&quot; PATH=&quot;$PATH:/root/jre1.8.0_65/bin&quot; RUN chmod +x /root/jre1.8.0_65/bin/* # 6\\. install openssh RUN yum install -y openssh openssh-server RUN mkdir -p /var/run/sshd &amp;amp;&amp;amp; echo &quot;root:root&quot; | chpasswd RUN /usr/sbin/sshd-keygen RUN sed -ri &apos;s/UsePAM yes/#UsePAM yes/g&apos; /etc/ssh/sshd_config &amp;amp;&amp;amp; sed -ri &apos;s/#UsePAM no/UsePAM no/g&apos; /etc/ssh/sshd_config # 7\\. add ku8eye-ansible binary and config files COPY kubernetes_cluster_setup /root/kubernetes_cluster_setup # 8\\. copy shell scripts, SQL scripts, config files # db init SQL COPY db_scripts /root/db_scripts # shell scripts COPY shell_scripts /root/shell_scripts RUN chmod +x /root/shell_scripts/*.sh COPY ku8eye-startup.sh /root/ku8eye-startup.sh RUN chmod +x /root/ku8eye-startup.sh # latest jar COPY ku8eye-web.jar /root/ku8eye-web.jar # 9\\. start mariadb, init db data, and start ku8eye-web app # supervisor config file COPY supervisord.conf /etc/supervisord.conf ENTRYPOINT /usr/bin/supervisord `&lt;/pre&gt; 2.制作镜像 #docker build -t myku8eye . [![2016-03-26_14-46-50](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/04/2016-03-26_14-46-50.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/04/2016-03-26_14-46-50.jpg) &lt;pre&gt;`docker images REPOSITORY TAG IMAGE ID CREATED VIRTUAL SIZE myku8eye latest af10ef44e73d 3 hours ago 2.25 GB `&lt;/pre&gt; 二、启动安装 1.启动容器 &lt;pre&gt;`docker run -tid --name ku8eye-web -p 3306:3306 -p 8080:8080 -p 9001:9001 myku8eye`&lt;/pre&gt; 2.进入容器 &lt;pre&gt;`docker exec -ti ku8eye-web /bin/bash`&lt;/pre&gt; 3.启动安装命令 ku8eye支持图形化一键安装及命令行一键安装，这里使用命令行一键安装。 &lt;pre&gt;`/root/ku8eye-startup.sh &quot;192.168.199.54,192.168.199.55,192.168.199.56&quot; &quot;172.0.0.0/16&quot; &quot;123456&quot; 安装过程信息略…最后显示Node 192.168.199.54 sumary:SUCESSNode 192.168.199.55 sumary:SUCESSNode 192.168.199.56 sumary:SUCESS 在master主机我们查看，我们看到kube nodes已启动。我们看到build镜像文件后，我们只要一个命令就可以轻松安装kubernetes集群。三、web界面访问http://192.168.199.50:8080端口，看到ku8 manager已启动。使用guest/123456登录，我们可以看到左侧菜单有资源管理，应用管理及集群监控。有兴趣的朋友自己测试一下，这里不再赘述。 参考：https://github.com/bestcloud/ku8eye/blob/master/doc/ku8eye-web-dev-env.md","tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://blog.yaodataking.com/tags/Kubernetes/"},{"name":"ku8eye","slug":"ku8eye","permalink":"http://blog.yaodataking.com/tags/ku8eye/"}]},{"title":"JConsole远程监控Mycat示列","date":"2016-04-03T07:24:40.000Z","path":"2016/04/03/jconsole-remote-mycat/","text":"一、简介JConsole是一个基于JMX的GUI工具，用于连接正在运行的JVM。JConsole可以以三种方式连接正在运行的JVM：1.Local：使用JConsole连接一个正在本地系统运行的JVM，并且执行程序的和运行JConsole的需要是同一个用户。JConsole使用文件系统的授权通过RMI连接器连接到平台的MBean服务器上。这种从本地连接的监控能力只有Sun的JDK具有。2.Remote：使用下面的URL通过RMI连接器连接到一个JMX代理：hoostname:port或service:jmx:protocol:sap。JConsole为建立连接，需要在环境变量中设置jmxremote.password来指定用户名和密码从而进行授权。3.Advanced:使用一个特殊的URL连接JMX代理。一般情况使用自己定制的连接器而不是RMI提供的连接器来连接JMX代理，或者是一个使用JDK实现了JMX和JMX Rmote的应用。本文我们就第二种remote方式来监控Mycat的运行情况。本文实验环境还是在docker中运行。 二、配置在使用JConsole监控前，我们必须对mycat的容器做一些设置，增加jmxremote.password文件至容器中，另外增加expose 1984端口。具体builder文件如下： from centos:7 MAINTAINER Alex Wu #install java8 ADD jdk-8u51-linux-x64.gz /usr/local RUN ln -s /usr/local/jdk1.8.0_51 /usr/local/java ENV JAVA_HOME /usr/local/java ENV PATH $PATH:$JAVA_HOME/bin COPY jmxremote.password /usr/local/java/jre/lib/management/ #install mycat VOLUME /opt/mycat/conf ADD Mycat-server-1.5.1-RELEASE-20160328130228-linux.tar.gz /opt EXPOSE 8066 9066 1984 CMD [&quot;/opt/mycat/bin/mycat&quot;,&quot;console&quot;] `&lt;/pre&gt; jmxremote.password文件配置如下，第一列是用户名，第二列是密码： &lt;pre&gt;`monitorRole monitor controlRole monitor `&lt;/pre&gt; 我们还需要在Wrapper.conf文件里配置jmx端口，并指定docker宿主机的IP地址。详细配置如下： &lt;pre&gt;`# Java Additional Parameters #wrapper.java.additional.1= wrapper.java.additional.1=-DMYCAT_HOME=. wrapper.java.additional.2=-server wrapper.java.additional.3=-XX:MaxPermSize=64M wrapper.java.additional.4=-XX:+AggressiveOpts wrapper.java.additional.5=-XX:MaxDirectMemorySize=2G wrapper.java.additional.6=-Dcom.sun.management.jmxremote wrapper.java.additional.7=-Dcom.sun.management.jmxremote.port=1984 wrapper.java.additional.8=-Dcom.sun.management.jmxremote.authenticate=false wrapper.java.additional.9=-Dcom.sun.management.jmxremote.ssl=false wrapper.java.additional.10=-Dcom.sun.management.jmxremote.rmi.port=1984 wrapper.java.additional.11=-Djava.rmi.server.hostname=192.168.199.168 wrapper.java.additional.12=-Xmx4G wrapper.java.additional.13=-Xms1G `&lt;/pre&gt; 配置完成,启动mycat容器。（事先我们已启动mysql容器，可参见本博客以前文章，这里不再详细介绍） &lt;pre&gt;`docker create --name mycat101 -v /mysql/mycatconf:/opt/mycat/conf -p 8066:8066 -p 9066:9066 -p 1984:1984 mycat docker start mycat101 `&lt;/pre&gt; 我们现在可以使用JConsole来监控Mycat了，(JConsole我们这里使用Ubuntu平台),下面是连接画面。 [![2016-04-03_14-51-13](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/04/2016-04-03_14-51-13.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/04/2016-04-03_14-51-13.jpg) 下面显示已连上 [![2016-04-02_10-30-02](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/04/2016-04-02_10-30-02.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/04/2016-04-02_10-30-02.jpg) 三、测试 接下来我们用mycat的测试工具来模拟插入200万条记录。 &lt;pre&gt;`./test_stand_insert_perf.sh jdbc:mysql://localhost:8066/TESTDB test test 100 &quot;0-100M,100M1-200M&quot; `&lt;/pre&gt; 测试数据导入过程中。 [![2016-04-02_10-30-31](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/04/2016-04-02_10-30-31.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/04/2016-04-02_10-30-31.jpg) 导入完成，我们看到tps 4818左右。 [![2016-04-02_10-45-01](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/04/2016-04-02_10-45-01.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/04/2016-04-02_10-45-01.jpg) 查看Mycat 9066端口 &lt;pre&gt;` mysql&amp;gt; show @@threadpool; +------------------+-----------+--------------+-----------------+----------------+------------+ | NAME | POOL_SIZE | ACTIVE_COUNT | TASK_QUEUE_SIZE | COMPLETED_TASK | TOTAL_TASK | +------------------+-----------+--------------+-----------------+----------------+------------+ | Timer | 2 | 0 | 0 | 3137 | 3137 | | BusinessExecutor | 4 | 0 | 0 | 2001010 | 2001010 | +------------------+-----------+--------------+-----------------+----------------+------------+ 2 rows in set (0.00 sec) mysql&amp;gt; show @@heartbeat; +---------+-------+------------+------+---------+-------+--------+---------+--------------+---------------------+-------+ | NAME | TYPE | HOST | PORT | RS_CODE | RETRY | STATUS | TIMEOUT | EXECUTE_TIME | LAST_ACTIVE_TIME | STOP | +---------+-------+------------+------+---------+-------+--------+---------+--------------+---------------------+-------+ | host101 | mysql | 172.17.0.2 | 3306 | 1 | 0 | idle | 0 | 6,10,10 | 2016-04-01 14:41:16 | false | | host102 | mysql | 172.17.0.3 | 3306 | -1 | 0 | idle | 0 | 0,0,0 | 2016-04-01 14:41:16 | false | +---------+-------+------------+------+---------+-------+--------+---------+--------------+---------------------+-------+ 2 rows in set (0.00 sec) mysql&amp;gt; show @@datanode; +------+----------------+-------+-------+--------+------+------+---------+------------+----------+---------+---------------+ | NAME | DATHOST | INDEX | TYPE | ACTIVE | IDLE | SIZE | EXECUTE | TOTAL_TIME | MAX_TIME | MAX_SQL | RECOVERY_TIME | +------+----------------+-------+-------+--------+------+------+---------+------------+----------+---------+---------------+ | dn1 | localhost1/db1 | 0 | mysql | 0 | 9 | 1000 | 18037 | 0 | 0 | 0 | -1 | | dn2 | localhost1/db2 | 0 | mysql | 0 | 0 | 1000 | 35 | 0 | 0 | 0 | -1 | | dn3 | localhost1/db3 | 0 | mysql | 0 | 1 | 1000 | 205 | 0 | 0 | 0 | -1 | +------+----------------+-------+-------+--------+------+------+---------+------------+----------+---------+---------------+ 3 rows in set (0.00 sec) mysql&amp;gt; show @@processor; +------------+-----------+-----------+-------------+---------+---------+-------------+--------------+------------+----------+----------+----------+ | NAME | NET_IN | NET_OUT | REACT_COUNT | R_QUEUE | W_QUEUE | FREE_BUFFER | TOTAL_BUFFER | BU_PERCENT | BU_WARNS | FC_COUNT | BC_COUNT | +------------+-----------+-----------+-------------+---------+---------+-------------+--------------+------------+----------+----------+----------+ | Processor0 | 257904920 | 257967902 | 0 | 0 | 0 | 884 | 1000 | 11 | 334 | 1 | 10 | +------------+-----------+-----------+-------------+---------+---------+-------------+--------------+------------+----------+----------+----------+ 1 row in set (0.00 sec) mysql&amp;gt; show @@datasource; +----------+---------+-------+------------+------+------+--------+------+------+---------+ | DATANODE | NAME | TYPE | HOST | PORT | W/R | ACTIVE | IDLE | SIZE | EXECUTE | +----------+---------+-------+------------+------+------+--------+------+------+---------+ | dn1 | host101 | mysql | 172.17.0.2 | 3306 | W | 0 | 10 | 1000 | 18284 | | dn1 | host102 | mysql | 172.17.0.3 | 3306 | R | 0 | 0 | 1000 | 0 | | dn3 | host101 | mysql | 172.17.0.2 | 3306 | W | 0 | 10 | 1000 | 18284 | | dn3 | host102 | mysql | 172.17.0.3 | 3306 | R | 0 | 0 | 1000 | 0 | | dn2 | host101 | mysql | 172.17.0.2 | 3306 | W | 0 | 10 | 1000 | 18284 | | dn2 | host102 | mysql | 172.17.0.3 | 3306 | R | 0 | 0 | 1000 | 0 | +----------+---------+-------+------------+------+------+--------+------+------+---------+ 6 rows in set (0.01 sec) mysql&amp;gt; show @@cache; +-------------------------------------+-------+------+--------+------+------+---------------+---------------+ | CACHE | MAX | CUR | ACCESS | HIT | PUT | LAST_ACCESS | LAST_PUT | +-------------------------------------+-------+------+--------+------+------+---------------+---------------+ | ER_SQL2PARENTID | 1000 | 0 | 0 | 0 | 0 | 0 | 0 | | SQLRouteCache | 10000 | 1 | 1 | 0 | 1 | 1459522244686 | 1459522244749 | | TableID2DataNodeCache.TESTDB_ORDERS | 50000 | 0 | 0 | 0 | 0 | 0 | 0 | +-------------------------------------+-------+------+--------+------+------+---------------+---------------+ 3 rows in set (0.02 sec) 我们再看看JConsole会记录什么?我们看到CPU基本稳定在30%左右，内存使用量基本在50M至250M波动，线程和类基本固定在一个数值。所以，我们看到使用JConsole可以使我们简单快捷的进行JAVA进程的监控。","tags":[{"name":"JConsole","slug":"JConsole","permalink":"http://blog.yaodataking.com/tags/JConsole/"},{"name":"Mycat","slug":"Mycat","permalink":"http://blog.yaodataking.com/tags/Mycat/"}]},{"title":"基于Mycat的Oracle数据迁移至Mysql方案","date":"2016-03-27T06:58:16.000Z","path":"2016/03/27/mycat-oracle-to-mysql/","text":"我们知道由于各种各样的原因，一些数据库比如oracle不得不迁移至其他数据库而又不能中断业务时，下面的这些问题常常困扰着技术人员，哪些表和库要迁移,哪些暂时不能动,迁移后数据如何同步,一般靠谱的方案是从影响最小的模块和数据表开始改造,逐步上线。那么何种解决方案可以实现这样的改造？本文我们就基于mycat来实现下面的一个场景，用户表与订单表的数据迁移到MySQL，转账记录表则还保留在oracle。改造前结构是这样的：改造后：1.准备工作在具体测试前，我们先做一些准备工作。1.1 准备oracle数据库，这里我们用docker实现简单起见，我们直接从网上下载oracle的express版本。 docker create --name oraclexe11g01 -p 8080:8080 -p 1521:1521 martinsthiago/oraclexe-11g-fig docker start oraclexe11g01 `&lt;/pre&gt; 注意如果要使用GBK字符集，需要在客户端，服务器端做一些设置。 客户端 export NLS_LANG=AMERICAN_AMERICA.ZHS16GBK 服务器端 ALTER DATABASE CHARACTER SET ZHS16GBK; 1.2 创建表 创建用户表，订单表，转账记录表 &lt;pre&gt;`create table Z_USER ( userid INTEGER , uname VARCHAR2(50) ); CREATE TABLE Z_ORDERS ( USERID INTEGER , productId INTEGER , unitprice NUMBER(12, 5) , qty INTEGER , orderDate DATE , orderLineNr INTEGER , orderNr INTEGER ) ; create table Z_PAY ( payid INTEGER , orderNr INTEGER , paydate DATE , payamt NUMBER(15,2) , userid INTEGER ); `&lt;/pre&gt; 1.3 生成测试数据 这里我们通过ETL工具kettle来生成用户表，订单表，转账记录表的测试数据，具体过程略，以下就展示结果。 用户表生成10万条记录 [![2016-03-27_10-19-16](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/03/2016-03-27_10-19-16.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/03/2016-03-27_10-19-16.jpg) 订单表生成1万条记录 [![2016-03-27_9-40-56](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/03/2016-03-27_9-40-56.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/03/2016-03-27_9-40-56.jpg) 转账记录表生成9914条记录 [![2016-03-27_14-05-27](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/03/2016-03-27_14-05-27.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/03/2016-03-27_14-05-27.jpg) 2.Mycat配置 在假设的场景中，一共有10万用户数，所以我们根据用户ID进行分片，1到5万在第一个分片数据库，5万零1到10万在第二个分片数据库。 2.1配置schema.xml [![2016-03-27_13-21-38](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/03/2016-03-27_13-21-38.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/03/2016-03-27_13-21-38.jpg) 2.2配置rule.xml [![2016-03-27_14-08-32](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/03/2016-03-27_14-08-32.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/03/2016-03-27_14-08-32.jpg) 2.3配置autopartition-long-userid.txt &lt;pre&gt;`[root@localhost ~]# vi autopartition-long-userid.txt # range start-end ,data node index # K=1000,M=10000. 0-50000=0 50001-100000=1 `&lt;/pre&gt; 配置完成，启动mysql数据库容器，mycat容器。 3.转移测试 3.1转账记录表 根据设置，转账记录表还保留在oracle数据库，我们看看通过mycat数据是否可以查询及新增数据。在mycat 8066端,我们看到新增数据顺利插入至到dn3,也就是我们定义的oracle数据库。 &lt;pre&gt;`mysql&amp;gt; insert into Z_PAY( payid,orderNr, paydate,payamt,userid ) values (9915,5000686,&apos;2016-01-31&apos;,10.2 ,7029); Query OK, 1 row affected (0.01 sec) OK! mysql&amp;gt; explain insert into Z_PAY( payid,orderNr, paydate,payamt,userid ) values (9915,5000686,&apos;2016-01-31&apos;,10.2 ,7029); +-----------+---------------------------------------------------------------------------------------------------------+ | DATA_NODE | SQL | +-----------+---------------------------------------------------------------------------------------------------------+ | dn3 | insert into Z_PAY( payid,orderNr, paydate,payamt,userid ) values (9915,5000686,&apos;2016-01-31&apos;,10.2 ,7029) | +-----------+---------------------------------------------------------------------------------------------------------+ 1 row in set (0.01 sec) mysql&amp;gt; `&lt;/pre&gt; 在oracle sqlplus端,我们可以查询到刚才新增的一条数据。 &lt;pre&gt;`SQL&amp;gt; select * from z_pay where payid&amp;gt;=9910; PAYID ORDERNR PAYDATE PAYAMT USERID ---------- ---------- ------------ ---------- ---------- 9910 5005517 27-JAN-07 11.78 55819 9911 5008604 28-JAN-07 29.24 87168 9912 5003118 29-JAN-07 37.24 31233 9913 5004690 30-JAN-07 3.58 47576 9914 5000686 31-JAN-07 6.2 7029 9915 5000686 31-JAN-16 10.2 7029 6 rows selected. SQL&amp;gt; `&lt;/pre&gt; 3.2用户表 用户表的转移，我们这里也通过ETL工具kettle来实现,需要说明的是kettle可以自动在mysql创建数据库表，下面是结果展示。 [![2016-03-27_12-46-19](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/03/2016-03-27_12-46-19.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/03/2016-03-27_12-46-19.jpg) 这里要注意的是，连接的mysql实际上是连接的是mycat的8066端口。 [![2016-03-27_13-03-15](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/03/2016-03-27_13-03-15.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/03/2016-03-27_13-03-15.jpg) 这样在转移的过程中，mycat就已经帮我们进行了数据分片。转移完成后，在mycat8066端，我们看到用户数据分别在dn1,dn2 &lt;pre&gt;`mysql&amp;gt; explain select count(*) from z_user; +-----------+-------------------------------------------------+ | DATA_NODE | SQL | +-----------+-------------------------------------------------+ | dn1 | SELECT COUNT(*) AS COUNT0 FROM z_user LIMIT 100 | | dn2 | SELECT COUNT(*) AS COUNT0 FROM z_user LIMIT 100 | +-----------+-------------------------------------------------+ 2 rows in set (0.00 sec) mysql&amp;gt; select count(*) from z_user; +--------+ | COUNT0 | +--------+ | 100000 | +--------+ 1 row in set (0.03 sec) `&lt;/pre&gt; 进入mysql的3306，我们看到,用户数据确实根据我们预先定义的分别进入了两个数据库。 &lt;pre&gt;`mysql&amp;gt; select count(*),min(userid),max(userid) from db1.z_user; +----------+-------------+-------------+ | count(*) | min(userid) | max(userid) | +----------+-------------+-------------+ | 50000 | 1 | 50000 | +----------+-------------+-------------+ 1 row in set (0.05 sec) mysql&amp;gt; select count(*),min(userid),max(userid) from db2.z_user; +----------+-------------+-------------+ | count(*) | min(userid) | max(userid) | +----------+-------------+-------------+ | 50000 | 50001 | 100000 | +----------+-------------+-------------+ 1 row in set (0.04 sec) mysql&amp;gt; `&lt;/pre&gt; 3.3 订单表 同样订单表的转移，我们这里也通过ETL工具kettle来实现,下面是结果展示。 [![2016-03-27_13-08-56](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/03/2016-03-27_13-08-56.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/03/2016-03-27_13-08-56.jpg) &lt;pre&gt;`mysql&amp;gt; select count(*) from z_orders; +--------+ | COUNT0 | +--------+ | 10000 | +--------+ 1 row in set (0.01 sec) mysql&amp;gt; explain select count(*) from z_orders; +-----------+---------------------------------------------------+ | DATA_NODE | SQL | +-----------+---------------------------------------------------+ | dn1 | SELECT COUNT(*) AS COUNT0 FROM z_orders LIMIT 100 | | dn2 | SELECT COUNT(*) AS COUNT0 FROM z_orders LIMIT 100 | +-----------+---------------------------------------------------+ 2 rows in set (0.00 sec) `&lt;/pre&gt; 进入mysql的3306，我们看到,订单数据确实根据我们预先定义的分别进入了两个数据库。 &lt;pre&gt;`mysql&amp;gt; select count(*),min(userid),max(userid) from db1.z_orders; +----------+-------------+-------------+ | count(*) | min(userid) | max(userid) | +----------+-------------+-------------+ | 4968 | 3 | 49997 | +----------+-------------+-------------+ 1 row in set (0.01 sec) mysql&amp;gt; select count(*),min(userid),max(userid) from db2.z_orders; +----------+-------------+-------------+ | count(*) | min(userid) | max(userid) | +----------+-------------+-------------+ | 5032 | 50027 | 99989 | +----------+-------------+-------------+ 1 row in set (0.01 sec) 小结，通过以上实验，我们看到通过mycat这个中间件的穿针引线，我们就有可能在完全不中断业务的情况下，顺利的进行数据的迁移及分片工作，当然在实际情况下，场景比以上演示的复杂的多。我们还是可以通过逐步改造,逐步上线的方式完成整个数据的迁移工作。 PS,1.mycat连接oracle需要复制ojdbc14.jar到mycat的lib目录，2.kettle连接oracle需要复制ojdbc14.jar到kettle的data-integration/lib目录,3.kettle连接mysql同样需要复制mysql-connector-java-5.1.38-bin.jar到kettle的data-integration/lib目录。","tags":[{"name":"Oracle","slug":"Oracle","permalink":"http://blog.yaodataking.com/tags/Oracle/"},{"name":"Mycat","slug":"Mycat","permalink":"http://blog.yaodataking.com/tags/Mycat/"},{"name":"Kettle","slug":"Kettle","permalink":"http://blog.yaodataking.com/tags/Kettle/"},{"name":"Mysql","slug":"Mysql","permalink":"http://blog.yaodataking.com/tags/Mysql/"}]},{"title":"Mycat环境下的mysql数据迁移实验","date":"2016-03-19T16:36:49.000Z","path":"2016/03/20/mycat-mysql-data-sync/","text":"我们知道在生产环境下，数据库服务器如果出现磁盘IO、内存、CPU等性能的瓶颈，除了做一些性能优化外，选择一些数据迁移至新服务器也是不错的解决方案。特别在以Mycat为中间件的环境中，有些场景的数据迁移可能会非常的简单。我们今天就来做这样一个场景，travelrecord表原来定义为10个分片，现在由于原服务器性能的原因将这10个分片中的2个分片转移到第二台MySQL上。大概思路是这样的，先导出第一台服务器2个分片的数据，然后导入到第二台服务器，同时配置这两台主从复制，主要同步这2个分片的数据，然后修改mycat配置，把这2个分片配置在第二台服务器，重新reload配置，确定写往2个分片的数据都写在了第二台服务器，就可以停止主从复制，数据迁移完成。这种方法应该说是一种比较快的数据迁移做法，基本上业务中断在数据的导出和导入。下面我们就具体通过docker容器方式来实现。1.准备工作1.1启动第一台mysql容器 docker create --name mysqlsrv103 -v /mysql/data/mysql103:/var/lib/mysql -v /mysql/103:/etc/mysql/conf.d -e MYSQL_ROOT_PASSWORD=123456 -p 3336:3306 mysql:latest docker start mysqlsrv103 `&lt;/pre&gt; 1.2创建数据库 &lt;pre&gt;`docker exec -it mysqlsrv103 /bin/bash mysql -uroot -p mysql&amp;gt;create database db1; mysql&amp;gt;create database db2; mysql&amp;gt;create database db3; mysql&amp;gt;create database db4; mysql&amp;gt;create database db5; mysql&amp;gt;create database db6; mysql&amp;gt;create database db7; mysql&amp;gt;create database db8; mysql&amp;gt;create database db9; mysql&amp;gt;create database db10; `&lt;/pre&gt; 1.3 配置mycat schema.xml配置 [![2016-03-19_23-40-56](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/03/2016-03-19_23-40-56.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/03/2016-03-19_23-40-56.jpg) rule.xml配置 [![2016-03-19_23-44-49](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/03/2016-03-19_23-44-49.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/03/2016-03-19_23-44-49.jpg) autopartition-long.txt文件设置 &lt;pre&gt;`# range start-end ,data node index # K=1000,M=10000. 0-100M=0 100M1-200M=1 200M1-300M=2 300M1-400M=3 400M1-500M=4 500M1-600M=5 600M1-700M=6 700M1-800M=7 800M1-900M=8 900M1-1000M=9 1000M1-2000M=10 `&lt;/pre&gt; 启动mycat容器 &lt;pre&gt;`docker create --name mycat03 -v /mysql/conf03:/opt/mycat/conf -p 8066:8066 -p 9066:9066 mycat:1.5 docker start mycat03 docker exec -it mycat03 /bin/bash `&lt;/pre&gt; 进入mycat8066端口，创建travelrecord表 &lt;pre&gt;`mysql -utest -p -h127.0.0.1 -P8066 -DTESTDB create table travelrecord (id bigint not null primary key,user_id varchar(100),traveldate DATE, fee decimal,days int); `&lt;/pre&gt; 我们可以使用以下命令生成一些测试数据 ./test_stand_insert_perf.sh jdbc:mysql://localhost:8066/TESTDB test test 100 &quot;0-2000M&quot; 好，准备工作完成。接下来我们将启动和配置第二台服务器。 2.配置第二台mysql容器 &lt;pre&gt;`docker create --name mysqlsrv104 -v /mysql/data/mysql104:/var/lib/mysql -v /mysql/104:/etc/mysql/conf.d -e MYSQL_ROOT_PASSWORD=123456 -p 3346:3306 mysql:latest docker start mysqlsrv104 `&lt;/pre&gt; 导出第一台mysql容器db9,db10数据库 mysqldump -uroot -p -h127.0.0.1 -P3336 db9 &amp;gt; /root/db9.sql mysqldump -uroot -p -h127.0.0.1 -P3336 db10 &amp;gt; /root/db9.sql 导入到第二台mysql容器 create database db9; create database db10; source /root/db9.sql; source /root/db10.sql; 3.主从配置 修改第一台mysql参数,加入需要同步的两个分片数据库db9和db10 &lt;pre&gt;` [mysqld] log-bin=mysql-bin server-id=103 lower_case_table_names=1 binlog-do-db=db9 binlog-do-db=db10 `&lt;/pre&gt; 修改第二台mysql参数，加入需要同步的两个分片数据库db9和db10 &lt;pre&gt;` [mysqld] log-bin=mysql-bin server-id=104 lower_case_table_names=1 replicate-do-db=db9 replicate-do-db=db10 `&lt;/pre&gt; 重启容器使配置生效。 进入第一台mysql，show master status; &lt;pre&gt;`mysql&amp;gt; show master status; +------------------+----------+--------------+------------------+-------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set | +------------------+----------+--------------+------------------+-------------------+ | mysql-bin.000005 | 154 | db9,db10 | | | +------------------+----------+--------------+------------------+-------------------+ 1 row in set (0.00 sec) `&lt;/pre&gt; 进入第二台mysql,设置master参数，启动slave。 &lt;pre&gt;` change master to master_host=&apos;172.17.0.2&apos;,master_user=&apos;root&apos;,master_password=&apos;123456&apos;,master_log_file=&apos;mysql-bin.000005&apos;,master_log_pos=154; start slave; `&lt;/pre&gt; 查看状态,如果Slave_IO_Running和Slave_SQL_Running都显示Yes,说明主从配置成功。 &lt;pre&gt;` mysql&amp;gt; show slave status\\G; *************************** 1\\. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: 172.17.0.2 Master_User: root Master_Port: 3306 Connect_Retry: 60 Master_Log_File: mysql-bin.000005 Read_Master_Log_Pos: 154 Relay_Log_File: a26491a4abb2-relay-bin.000002 Relay_Log_Pos: 320 Relay_Master_Log_File: mysql-bin.000005 Slave_IO_Running: Yes Slave_SQL_Running: Yes Replicate_Do_DB: db9,db10 Replicate_Ignore_DB: Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: Replicate_Wild_Ignore_Table: Last_Errno: 0 Last_Error: Skip_Counter: 0 Exec_Master_Log_Pos: 154 Relay_Log_Space: 534 Until_Condition: None Until_Log_File: Until_Log_Pos: 0 Master_SSL_Allowed: No Master_SSL_CA_File: Master_SSL_CA_Path: Master_SSL_Cert: Master_SSL_Cipher: Master_SSL_Key: Seconds_Behind_Master: 0 Master_SSL_Verify_Server_Cert: No Last_IO_Errno: 0 Last_IO_Error: Last_SQL_Errno: 0 Last_SQL_Error: Replicate_Ignore_Server_Ids: Master_Server_Id: 103 Master_UUID: 5a5e5ab5-ed6c-11e5-938d-0242ac110002 Master_Info_File: /var/lib/mysql/master.info SQL_Delay: 0 SQL_Remaining_Delay: NULL Slave_SQL_Running_State: Slave has read all relay log; waiting for more updates Master_Retry_Count: 86400 Master_Bind: Last_IO_Error_Timestamp: Last_SQL_Error_Timestamp: Master_SSL_Crl: Master_SSL_Crlpath: Retrieved_Gtid_Set: Executed_Gtid_Set: Auto_Position: 0 Replicate_Rewrite_DB: Channel_Name: Master_TLS_Version: 1 row in set (0.00 sec) `&lt;/pre&gt; 4\\. 修改mycat配置 把db9,db10更改至第二台mysql容器。 [![2016-03-20_0-05-38](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/03/2016-03-20_0-05-38.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/03/2016-03-20_0-05-38.jpg) &lt;pre&gt;` mysql&amp;gt; reload @@config_all; Query OK, 1 row affected (0.15 sec) Reload config success `&lt;/pre&gt; 进入mycat 8066端口，手工插入一条数据 &lt;pre&gt;` mysql -utest -p -h127.0.0.0 -P8066 -DTESTDB insert into travelrecord (id,user_id,traveldate,fee,days) values(9800001,&apos;huang&apos;,&apos;2014-06-05&apos;,720.5,3); 查看log，我们看到数据已经写往第二台mysql容器。至此我们确认数据迁移成功，此时可以停止主从复制了，第一台mysql容器的二个分片数据库也可以放心的删除了。","tags":[{"name":"Mycat","slug":"Mycat","permalink":"http://blog.yaodataking.com/tags/Mycat/"},{"name":"Mysql","slug":"Mysql","permalink":"http://blog.yaodataking.com/tags/Mysql/"}]},{"title":"Kubernetes入门实战(5)：Kubernetes集群网络之flannel网络方案","date":"2016-03-13T11:24:58.000Z","path":"2016/03/13/kubernetes-cluster-flannel/","text":"flannel 是 CoreOS 团队针对 Kubernetes 设计的一个覆盖网络 (overlay network) 工具，其目的在于帮助每一个使用 Kuberentes 的 CoreOS 主机拥有一个完整的子网。Kubernetes 会为每一个 POD 分配一个独立的 IP 地址，这样便于同一个 POD 中的 Containers 彼此连接，而之前的 CoreOS 并不具备这种能力。为了解决这一问题，flannel 通过在集群中创建一个覆盖网格网络 (overlay mesh network) 为主机设定一个子网。具体flannel介绍及原理参见官网。下面我们实战配置及测试。注：本文安装配置是在我的上篇博文Kubernetes集群初探的基础上。1. etcd设置首先我们要对etcd做一些更改1.1设置flanel网络段 #etcdctl set /coreos.com/network/config ‘{ “Network”: “10.2.0.0/16” }’1.2修改配置文件在配置文件里/etc/etcd/etcd.conf把ETCD_LISTEN_CLIENT_URLS=”http://localhost:2379&quot;中的locahost改为0.0.0.0 2.flannel安装配置(每台Node节点都要配置)2.1下载 #wget https://github.com/coreos/flannel/releases/download/v0.5.5/flannel-0.5.5-linux-amd64.tar.gz2.2解压 #tar -xzvf flannel-0.5.5-linux-amd64.tar.gz2.3安装直接复制解压出来的两个文件到可执行目录 #cp flannel-0.5.5/flanneld /usr/bin #cp flannel-0.5.5/mk-docker-opts.sh /usr/bin2.4配置编辑/etc/sysconfig/flanneld # Flanneld configuration options # etcd url location FLANNEL_ETCD=&quot;http://centos-master:2379&quot; # etcs config key FLANNEL_ETCD_KEY=&quot;/coreos.com/network&quot; # Any additonal options #FLANNEL_OPTIONS= `&lt;/pre&gt; 编辑服务文件/usr/lib/systemd/system/flanneld.service &lt;pre&gt;&lt;/code&gt;[Unit] Description=Flanneld overlay address etcd agent After=network.target Before=docker.service [Service] Type=notify EnvironmentFile=-/etc/sysconfig/flanneld EnvironmentFile=-/etc/sysconfig/docker-network ExecStart=/usr/bin/flanneld \\ -etcd-endpoints=${FLANNEL_ETCD} \\ $FLANNEL_OPTIONS [Install] RequiredBy=docker.service WantedBy=multi-user.target &lt;/code&gt;&lt;/pre&gt; 2.5暂停docker服务 #systemctl stop docker 2.6执行以下脚本 #systemctl start flanneld #mk-docker-opts.sh -i #source /run/flannel/subnet.env #ifconfig docker0 ${FLANNEL_SUBNET} 2.7重启docker服务 #systemctl restart docker 检查网络配置，我们看到多了flannel0， 在centos-minion01上 &lt;pre&gt;`[root@centos-minion01 ~]# ip a 1: lo: mtu 65536 qdisc noqueue state UNKNOWN link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: eno16777736: mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 00:0c:29:66:c0:bf brd ff:ff:ff:ff:ff:ff inet 192.168.199.52/24 brd 192.168.199.255 scope global eno16777736 valid_lft forever preferred_lft forever inet6 fe80::20c:29ff:fe66:c0bf/64 scope link valid_lft forever preferred_lft forever 3: docker0: mtu 1500 qdisc noqueue state DOWN link/ether 02:42:c8:3e:ab:b3 brd ff:ff:ff:ff:ff:ff inet 10.2.35.1/24 brd 10.2.35.255 scope global docker0 valid_lft forever preferred_lft forever 4: flannel0: mtu 1472 qdisc pfifo_fast state UNKNOWN qlen 500 link/none inet 10.2.35.0/16 scope global flannel0 valid_lft forever preferred_lft forever `&lt;/pre&gt; 在centos-minion02上 &lt;pre&gt;`[root@centos-minion02 ~]# ip a 1: lo: mtu 65536 qdisc noqueue state UNKNOWN link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: eno16777736: mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 00:0c:29:6a:0c:48 brd ff:ff:ff:ff:ff:ff inet 192.168.199.53/24 brd 192.168.199.255 scope global eno16777736 valid_lft forever preferred_lft forever inet6 fe80::20c:29ff:fe6a:c48/64 scope link valid_lft forever preferred_lft forever 3: docker0: mtu 1500 qdisc noqueue state DOWN link/ether 02:42:2e:ab:fa:29 brd ff:ff:ff:ff:ff:ff inet 10.2.52.1/24 brd 10.2.52.255 scope global docker0 valid_lft forever preferred_lft forever inet6 fe80::42:2eff:feab:fa29/64 scope link valid_lft forever preferred_lft forever 6: flannel0: mtu 1472 qdisc pfifo_fast state UNKNOWN qlen 500 link/none inet 10.2.52.0/16 scope global flannel0 valid_lft forever preferred_lft forever `&lt;/pre&gt; 查看etcd上的路由表。 &lt;pre&gt;`[root@centos-master ~]# etcdctl ls /coreos.com/network/subnets /coreos.com/network/subnets/10.2.35.0-24 /coreos.com/network/subnets/10.2.52.0-24 [root@centos-master ~]# etcdctl get /coreos.com/network/subnets/10.2.35.0-24 {&quot;PublicIP&quot;:&quot;192.168.199.52&quot;} [root@centos-master ~]# etcdctl get /coreos.com/network/subnets/10.2.52.0-24 {&quot;PublicIP&quot;:&quot;192.168.199.53&quot;} `&lt;/pre&gt; 3\\. 测试验证 3.1 启动两个pod 我们在centos-master上制作两个pod文件。第二个文件把01改为02 &lt;pre&gt;`[root@centos-master mysqlpod]# cat mysqlpod01.yaml apiVersion: v1 kind: Pod metadata: name: mysql01 labels: name: mysql01 spec: containers: - name: mysql01 image: mysql env: - name: MYSQL_ROOT_PASSWORD value: p123456 ports: - containerPort: 3306 `&lt;/pre&gt; 启动这两个pod。 kubectl create -f mysqlpod01.yaml kubectl create -f mysqlpod02.yaml &lt;pre&gt;`[root@centos-master ~]# kubectl get pods -o wide NAME READY STATUS RESTARTS AGE NODE mysql01 1/1 Running 0 10m centos-minion02 mysql02 1/1 Running 0 7m centos-minion01 `&lt;/pre&gt; 我们看到分别启动在两台Node上。下面我们测试他们的容器能不能互联。 在centos-minion01上进入mysql02容器，获得IP地址为10.2.35.2，同时我们在centos-minion02上也获得mysql01容器的IP地址为10.2.52.2。 &lt;pre&gt;`[root@centos-minion01 ~]# docker exec -it 309728d3a3f4 /bin/bash root@mysql02:/# ip a 1: lo: mtu 65536 qdisc noqueue state UNKNOWN group default link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 5: eth0@if6: mtu 1500 qdisc noqueue state UP group default link/ether 02:42:0a:02:23:02 brd ff:ff:ff:ff:ff:ff inet 10.2.35.2/24 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::42:aff:fe02:2302/64 scope link tentative dadfailed valid_lft forever preferred_lft forever root@mysql02:/# mysql -uroot -p -h10.2.52.2 Enter password: Welcome to the MySQL monitor. Commands end with ; or \\g. Your MySQL connection id is 2 Server version: 5.7.10 MySQL Community Server (GPL) Copyright (c) 2000, 2015, Oracle and/or its affiliates. All rights reserved. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type &apos;help;&apos; or &apos;\\h&apos; for help. Type &apos;\\c&apos; to clear the current input statement. mysql&amp;gt; `&lt;/pre&gt; 我们看到连接没有问题。同时在centos-minion02的上，我们也试着进入mysql01容器，连接在centos-minion01节点上的mysql02容器。 &lt;pre&gt;`[root@centos-minion02 ~]# docker exec -it 98fd9272ad7a /bin/bash root@mysql01:/# ip a 1: lo: mtu 65536 qdisc noqueue state UNKNOWN group default link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 7: eth0@if8: mtu 1500 qdisc noqueue state UP group default link/ether 02:42:0a:02:34:02 brd ff:ff:ff:ff:ff:ff inet 10.2.52.2/24 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::42:aff:fe02:3402/64 scope link tentative dadfailed valid_lft forever preferred_lft forever root@mysql01:/# mysql -uroot -p -h10.2.35.2 Enter password: Welcome to the MySQL monitor. Commands end with ; or \\g. Your MySQL connection id is 2 Server version: 5.7.10 MySQL Community Server (GPL) Copyright (c) 2000, 2015, Oracle and/or its affiliates. All rights reserved. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type &apos;help;&apos; or &apos;\\h&apos; for help. Type &apos;\\c&apos; to clear the current input statement. mysql&amp;gt; 同样也可以顺利连接。 注：Flannel不需要在Master节点上部署，因为master节点不参与负载。Flannel不仅控制了Docker引擎子网的分配也控制了容器的IP分配。参考：Kubernetes权威指南","tags":[{"name":"flannel","slug":"flannel","permalink":"http://blog.yaodataking.com/tags/flannel/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://blog.yaodataking.com/tags/Kubernetes/"}]},{"title":"Kubernetes入门实战(4)：Kubernetes 集群网络之直接路由方案","date":"2016-03-13T06:15:30.000Z","path":"2016/03/13/kubernetes-cluster-quagga/","text":"上文我们已经初步搭建了Kubernetes两个Node节点的集群，并且成功地在启动mysql pod的Node节点上连接，但是在另一个Node节点上我们还是无法连接到。本文我们就用直接路由加Quagga的方式实现不同Node节点间的pod互联。 环境及准备工作同上文1. 修改docker0地址我们重新规划docker0的网址，分别使用10.1.10.1及10.1.20.1网段。在centos-minion01上，修改docker0网段地址。ifconfig docker0 10.1.10.1/24还要修改docker配置文件 # /etc/sysconfig/docker # Modify these options if you want to change the way the docker daemon runs OPTIONS=&apos;--bip=10.1.10.1/24&apos; `&lt;/pre&gt; 在centos-minion02上，修改docker0网段地址。 ifconfig docker0 10.1.20.1/24 还要修改docker配置文件 &lt;pre&gt;`# /etc/sysconfig/docker # Modify these options if you want to change the way the docker daemon runs OPTIONS=&apos;--bip=10.1.20.1/24&apos; `&lt;/pre&gt; 重启docker服务。 2\\. 启动mysql POD和services &lt;pre&gt;`[root@centos-master mysqlpod]# kubectl get pods NAME READY STATUS RESTARTS AGE mysql 1/1 Running 0 31m [root@centos-master mysqlpod]# kubectl get services NAME CLUSTER_IP EXTERNAL_IP PORT(S) SELECTOR AGE kubernetes 10.254.0.1 443/TCP 15d mysql 10.254.145.138 3306/TCP name=mysql 51m `&lt;/pre&gt; 我们看到mysql pod启动在centos-minion02上。 &lt;pre&gt;`[root@centos-master mysqlpod]# kubectl describe pods mysql|grep centos-minion Node: centos-minion02/192.168.199.53 `&lt;/pre&gt; 此时在centos-minion01上连接mysql，不能连接。 &lt;pre&gt;`[root@centos-minion01 ~]# mysql -uroot -p -h10.254.145.138 `&lt;/pre&gt; 3\\. 添加路由 在centos-minion01上, 添加到centos-minion02上docker0的静态路由地址。 route add -net 10.1.20.0 netmask 255.255.255.0 gw 192.168.199.52 在centos-minion02上, 添加到centos-minion01上docker0的静态路由地址。 route add -net 10.1.10.0 netmask 255.255.255.0 gw 192.168.199.53 再次连接mysql(使用service IP)，我们看到连接已经成功。 &lt;pre&gt;`[root@centos-minion01 ~]# mysql -uroot -p -h10.254.145.138 Enter password: Welcome to the MySQL monitor. Commands end with ; or \\g. Your MySQL connection id is 5 Server version: 5.7.10 MySQL Community Server (GPL) Copyright (c) 2000, 2016, Oracle and/or its affiliates. All rights reserved. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type &apos;help;&apos; or &apos;\\h&apos; for help. Type &apos;\\c&apos; to clear the current input statement. mysql&amp;gt; exit Bye `&lt;/pre&gt; 使用pod的内部IP，我们看到连接也成功。 &lt;pre&gt;`[root@centos-minion01 ~]# mysql -uroot -p -h10.1.20.2 Enter password: Welcome to the MySQL monitor. Commands end with ; or \\g. Your MySQL connection id is 6 Server version: 5.7.10 MySQL Community Server (GPL) Copyright (c) 2000, 2016, Oracle and/or its affiliates. All rights reserved. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type &apos;help;&apos; or &apos;\\h&apos; for help. Type &apos;\\c&apos; to clear the current input statement. mysql&amp;gt; exit Bye [root@centos-minion01 ~]# `&lt;/pre&gt; 4\\. 使用Quagga动态添加路由 为了减少手工添加路由，可以使用Quagga实现路由规则的动态添加。为简单起见，我们使用docker镜像。 #docker pull index.alauda.cn/georce/router 在每个Node上启动Quagga容器，Quagga需要以--privileged特权模式运行，并且指定--net=host，表示直接使用物理机的网络。 #docker run -itd --name=router --privileged --net=host index.alauda.cn/georce/router 启动成功后，Quagga会相互学习来完成到其他机器的docker0路由规则的添加。 &lt;pre&gt;`[root@centos-minion02 ~]# route -n Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface 0.0.0.0 192.168.199.1 0.0.0.0 UG 0 0 0 eno16777736 10.1.10.0 192.168.199.52 255.255.255.0 UG 20 0 0 eno16777736 10.1.20.0 0.0.0.0 255.255.255.0 U 0 0 0 docker0 169.254.0.0 0.0.0.0 255.255.0.0 U 1002 0 0 eno16777736 192.168.199.0 0.0.0.0 255.255.255.0 U 0 0 0 eno16777736 `&lt;/pre&gt; 我们看到已经可以在centos-minion02上ping通centos-minion01的docker0 &lt;pre&gt;`[root@centos-minion02 ~]# ping 10.1.10.1 PING 10.1.10.1 (10.1.10.1) 56(84) bytes of data. 64 bytes from 10.1.10.1: icmp_seq=1 ttl=64 time=1.20 ms 64 bytes from 10.1.10.1: icmp_seq=2 ttl=64 time=0.204 ms 64 bytes from 10.1.10.1: icmp_seq=3 ttl=64 time=0.236 ms 64 bytes from 10.1.10.1: icmp_seq=4 ttl=64 time=0.234 ms 64 bytes from 10.1.10.1: icmp_seq=5 ttl=64 time=0.423 ms","tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://blog.yaodataking.com/tags/Kubernetes/"},{"name":"Quagga","slug":"Quagga","permalink":"http://blog.yaodataking.com/tags/Quagga/"}]},{"title":"基于Docker容器的MyCat高可用方案(1)HAproxy","date":"2016-03-11T14:20:49.000Z","path":"2016/03/11/mycat-haproxy-docker/","text":"Mycat是一个彻底开源的新颖的数据库中间件产品。它的出现将彻底结束数据库的瓶颈问题，从而使数据库的高可用，高负载成为可能。在基于Mycat的MySQL主从读写分离及自动切换的docker实现一文中，我们已经实现基于Mycat的Mysql高可用，但是Mycat本身也存在稳定性和单点问题，所以本文我们通过HAproxy实现MyCat的高可用。架构图如下：本文所有组件都采用docker镜像和容器，为简单起见，都运行在一台宿主机上，系统为centos 71.Mysql配置创建并启动容器 docker create --name mysqlsrv101 -v /mysql/data/mysql101:/var/lib/mysql -v /mysql/101:/etc/mysql/conf.d -e MYSQL_ROOT_PASSWORD=123456 -p 3306:3306 mysql:latest docker create --name mysqlsrv102 -v /mysql/data/mysql102:/var/lib/mysql -v /mysql/102:/etc/mysql/conf.d -e MYSQL_ROOT_PASSWORD=123456 -p 3316:3306 mysql:latest docker start mysqlsrv101 docker start mysqlsrv102 `&lt;/pre&gt; mysql配置文件参见[基于Mycat的MySQL主从读写分离及自动切换的docker实现](http://blog.yaodataking.com/2016/01/mycat_mysql_docker_sample1.html)一文 2.Mycat配置 创建并启动容器,两个mycat容器共用同一配置文件。 &lt;pre&gt;`docker create --name mycat01 -v /mysql/mycatconf:/opt/mycat/conf -p 8066:8066 -p 9066:9066 mycat:1.5 docker create --name mycat02 -v /mysql/mycatconf:/opt/mycat/conf -p 8067:8066 -p 9067:9066 mycat:1.5 docker start mycat01 docker start mycat02 `&lt;/pre&gt; mycat的配置文件参见[基于Mycat的MySQL主从读写分离及自动切换的docker实现](http://blog.yaodataking.com/2016/01/mycat_mysql_docker_sample1.html)一文 3.HAproxy配置 3.1下载HAproxy镜像 &lt;pre&gt;`[root@localhost ~]# docker pull haproxy Using default tag: latest latest: Pulling from library/haproxy 73e8d4f6bf84: Pull complete 040bf8e08425: Pull complete 4c4d55db13de: Pull complete 03fd84c9b2d1: Pull complete 31209e66fb19: Pull complete e89816114fc3: Pull complete 3de0fe50637a: Pull complete 7bcd41b9d648: Pull complete Digest: sha256:6d1f490cd1e3c95bd38c525ea221d91e8470461602e0a56e9dde58567d99cbb8 Status: Downloaded newer image for haproxy:latest `&lt;/pre&gt; 3.2 HAproxy配置 编辑haproxy配置文件/mysql/haproxy/haproxy.cfg,由于在docker中运行，需要把daemon注释掉。 &lt;pre&gt;`global #daemon #remark this option while in docker maxconn 256 defaults mode http timeout connect 5000ms timeout client 50000ms timeout server 50000ms listen mycat bind 0.0.0.0:8068 mode tcp balance roundrobin server mycat1 172.17.0.4:8066 weight 1 check inter 1s rise 2 fall 2 server mycat2 172.17.0.5:8066 weight 1 check inter 1s rise 2 fall 2 listen stats #monitor mode http bind 0.0.0.0:8888 stats enable stats uri /dbs stats realm Global\\ statistics stats auth admin:admin `&lt;/pre&gt; 创建并启动HAproxy容器 &lt;pre&gt;`docker create --name myhaproxy01 -v /mysql/haproxy/haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg:ro -p 8068:8068 -p 8888:8888 haproxy docker start myhaproxy01 `&lt;/pre&gt; 4.高可用测试 使用mysql客户端连接haproxy的端口8068，我们看到连接正常，并且正常返回查询数据。 &lt;pre&gt;`[root@localhost ~]# mysql -utest -p -h127.0.0.1 -P8068 -DTESTDB Enter password: Reading table information for completion of table and column names You can turn off this feature to get a quicker startup with -A Welcome to the MySQL monitor. Commands end with ; or \\g. Your MySQL connection id is 1612 Server version: 5.5.8-mycat-1.5-alpha-20160108213035 MyCat Server (OpenCloundDB) Copyright (c) 2000, 2015, Oracle and/or its affiliates. All rights reserved. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type &apos;help;&apos; or &apos;\\h&apos; for help. Type &apos;\\c&apos; to clear the current input statement. mysql&amp;amp;gt; select count(*) from person; +--------+ | COUNT0 | +--------+ | 100000 | +--------+ 1 row in set (0.04 sec) mysql&amp;amp;gt; `&lt;/pre&gt; 查看haproxy的监控端口8888，我们看到mycat的两台容器都正常运行。 [![2016-03-11_21-43-44](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/03/2016-03-11_21-43-44.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/03/2016-03-11_21-43-44.jpg) 接下来我们模拟其中一台mycat突然宕机，数据连接情况。 &lt;pre&gt;`[root@localhost ~]# docker stop mycat02 mycat02 [root@localhost ~]# mysql -utest -p -h127.0.0.1 -P8068 -DTESTDB Enter password: Reading table information for completion of table and column names You can turn off this feature to get a quicker startup with -A Welcome to the MySQL monitor. Commands end with ; or \\g. Your MySQL connection id is 1616 Server version: 5.5.8-mycat-1.5-alpha-20160108213035 MyCat Server (OpenCloundDB) Copyright (c) 2000, 2015, Oracle and/or its affiliates. All rights reserved. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type &apos;help;&apos; or &apos;\\h&apos; for help. Type &apos;\\c&apos; to clear the current input statement. mysql&amp;amp;gt; select count(*) from person; +--------+ | COUNT0 | +--------+ | 100000 | +--------+ 1 row in set (0.04 sec) mysql&amp;amp;gt; 我们看到虽然刚才停掉了mycat02,但是mysql还是正常连接，并且返回查询数据。查看haproxy的监控端口8888，我们看到mycat服务器一台正常，一台宕机。","tags":[{"name":"Docker","slug":"Docker","permalink":"http://blog.yaodataking.com/tags/Docker/"},{"name":"Mycat","slug":"Mycat","permalink":"http://blog.yaodataking.com/tags/Mycat/"},{"name":"Mysql","slug":"Mysql","permalink":"http://blog.yaodataking.com/tags/Mysql/"},{"name":"HAproxy","slug":"HAproxy","permalink":"http://blog.yaodataking.com/tags/HAproxy/"}]},{"title":"Kettle数据同步初探","date":"2016-03-10T12:09:45.000Z","path":"2016/03/10/kettle-synchronize/","text":"Kettle是一款国外开源的ETL工具，数据抽取高效稳定。1.数据同步方法根据需求不同我们可以采取以下方案。1.1触发器：在数据库建立增删改的触发器。触发器将变更放到一张临时表里。优点：实时同步缺点：影响到业务系统，因为需要在业务系统建立触发器1.2日志：通过分析源数据库日志，来获得源数据库中的变化的数据。优点：不影响业务系统缺点：有一定得延时，对于没有提供日志分析接口的数据源，开发的难度比较大1.3时间戳：在要同步的源表里有时间戳字段，每当数据发生变化，时间戳会记录发生变化的时间优点：基本不影响业务系统缺点：要求源表必须有时间戳这一列1.4数据比较：通过比较两边数据源数据，来完成数据同步。一般用于实时性要求不高的场景。优点：基本不影响业务系统缺点：效率低1.5全表拷贝：定时清空目的数据源，将源数据源的数据全盘拷贝到目的数据源。一般用于数据量不大，实时性要求不高的场景。优点：基本不影响业务系统，开发、部署都很简单缺点：效率低 现在我们就数据比较法来演示kettle数据同步性能调优过程。环境：虚拟机UBUNTU 15.10，内存4GB，数据库POSTGRES。 2.准备工作2.1安装postgres数据库我们以Docker容器方式安装postgres数据库下载postgres 9.5.0 的镜像docker pull postgres:9.5.0创建容器mypostgres01,作为主数据库 docker create --name mypostgres01 -p 5432:5432 -e POSTGRES_PASSWORD=123456 postgres:9.5.0 `&lt;/pre&gt; 创建容器mypostgres02,作为备数据库 &lt;pre&gt;`docker create --name mypostgres02 -p 5433:5432 -e POSTGRES_PASSWORD=123456 postgres:9.5.0 `&lt;/pre&gt; 分别启动容器 docker start mypostgres01 docker start mypostgres02 &lt;pre&gt;`alex@ubuntu:~$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 62a6761dcbb8 postgres:9.5.0 &quot;/docker-entrypoint.s&quot; 15 seconds ago Up 10 seconds 0.0.0.0:5433-&amp;amp;gt;5432/tcp mypostgres02 762bf5d078f0 postgres:9.5.0 &quot;/docker-entrypoint.s&quot; 20 seconds ago Up 15 seconds 0.0.0.0:5432-&amp;amp;gt;5432/tcp mypostgres01 `&lt;/pre&gt; 2.2创建测试数据库 使用pgAdmin3 分别建立数据库kettle [![2016-03-10_8-25-32](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/03/2016-03-10_8-25-32.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/03/2016-03-10_8-25-32.jpg) 2.3生成示例数据 我们使用kettle的转换文件在两个数据库分别生成100万条orders数据。orders数据表结构如下。 &lt;pre&gt;`CREATE TABLE public.orders ( customerid integer, productid integer, discountpct smallint, nrproducts smallint, orderdate timestamp without time zone, orderlinenr bigint, ordernr integer, serial integer NOT NULL DEFAULT nextval(&apos;orders_serial_seq&apos;::regclass), CONSTRAINT orders_pkey PRIMARY KEY (serial) ) 具体方法可参见kettle自带的sample转换，转换文件路径如下\\samples\\transformations\\data-generator\\generate order data.ktr3.数据同步3.1 制作转换文件，分别添加两个表输入mypostgres01和mypostgres02，然后再添加合并记录和数据同步。数据库连接设置合并记录数据同步高级设置运行结果3.2 优化转换文件。我们在合并前增加排序步骤。运行结果我们看到通过排序，数据同步完成时间由原来的3分多钟缩短到1分多钟，系统的性能大大提高。","tags":[{"name":"postgresql","slug":"postgresql","permalink":"http://blog.yaodataking.com/tags/postgresql/"},{"name":"Docker","slug":"Docker","permalink":"http://blog.yaodataking.com/tags/Docker/"},{"name":"ETL","slug":"ETL","permalink":"http://blog.yaodataking.com/tags/ETL/"},{"name":"Kettle","slug":"Kettle","permalink":"http://blog.yaodataking.com/tags/Kettle/"}]},{"title":"Kubernetes入门实战(3)：运维之扩容与升级","date":"2016-03-05T01:07:04.000Z","path":"2016/03/05/kubernetes-ops1/","text":"本文我们将对Kubernetes的常用运维操作扩容与升级做简单说明。 1.Node的扩容Node的扩容简单言之就是增加新的Node节点。在节点上安装Kubelet，Kube-proxy及Docker, 并修改参数使其指向Master地址。基于Kuberlet的自动注册机制，新的Node将会自动加入现有的Kubernetes集群中。 2.Pod的动态扩容和缩放在实际运维过程中，我们常常需要对某个服务动态扩容以满足突增的流量，或者动态减少服务实例节约服务器资源。下面我们将动态增加redis-slave的pod副本由2个增加为3个。 #kubectl scale rc redis-slave –replicas=3 [root@localhost ~]# kubectl get pods NAME READY STATUS RESTARTS AGE frontend-ob32o 1/1 Running 1 30d frontend-zcyms 1/1 Running 1 30d frontend-zllt8 1/1 Running 1 30d redis-master-gjkqh 1/1 Running 1 30d redis-slave-geq4m 1/1 Running 1 30d redis-slave-o692g 1/1 Running 1 30d [root@localhost ~]# kubectl scale rc redis-slave --replicas=3 scaled [root@localhost ~]# kubectl get pods NAME READY STATUS RESTARTS AGE frontend-ob32o 1/1 Running 1 30d frontend-zcyms 1/1 Running 1 30d frontend-zllt8 1/1 Running 1 30d redis-master-gjkqh 1/1 Running 1 30d redis-slave-geq4m 1/1 Running 1 30d redis-slave-o692g 1/1 Running 1 30d redis-slave-qozrp 0/1 Running 0 6s `&lt;/pre&gt; 同样的我们也可以减少pod的副本，以下我们将redis-slave由3个副本减为1个。 &lt;pre&gt;`[root@localhost ~]# kubectl get pods NAME READY STATUS RESTARTS AGE frontend-ob32o 1/1 Running 1 30d frontend-zcyms 1/1 Running 1 30d frontend-zllt8 1/1 Running 1 30d redis-master-gjkqh 1/1 Running 1 30d redis-slave-geq4m 1/1 Running 1 30d redis-slave-o692g 1/1 Running 1 30d redis-slave-qozrp 1/1 Running 0 2m [root@localhost ~]# kubectl scale rc redis-slave --replicas=1 scaled [root@localhost ~]# kubectl get pods NAME READY STATUS RESTARTS AGE frontend-ob32o 1/1 Running 1 30d frontend-zcyms 1/1 Running 1 30d frontend-zllt8 1/1 Running 1 30d redis-master-gjkqh 1/1 Running 1 30d redis-slave-qozrp 1/1 Running 0 2m `&lt;/pre&gt; ## 3.应用的滚动升级 在实际运维过程中，如何不停止服务而进行升级将变得越来越常见，Kubernetes提供了Rolling-update的功能来解决上述场景。 我们假设PHP的image有一个新的v2版本，我们需要将现有PHP服务滚动升级为v2。 ### 3.1制作新镜像 简单起见，我们通过docker commit来制作一个新镜像，首先用原镜像启动一个新容器，你可以在容器里修改，然后退出。 &lt;pre&gt;`docker run -it docker.io/kubeguide/guestbook-php-frontend /bin/bash root@1b605f52b5f1:/var/www/html# exit exit `&lt;/pre&gt; 好，现在我们用docker commit来保存刚才我们编辑过的容器,我们把它命名为guestbook-php-frontend:v2 &lt;pre&gt;`[root@localhost ~]# docker commit 1b605f52b5f1 guestbook-php-frontend:v2 103579c2c2774b62017de760475e7a1ae3addacc9f1c88ea59ec1f3280ae93fb [root@localhost ~]# docker images REPOSITORY TAG IMAGE ID CREATED VIRTUAL SIZE guestbook-php-frontend v2 103579c2c277 5 seconds ago 509.6 MB docker.io/kubeguide/guestbook-php-frontend latest 38658844a359 5 months ago 509.6 MB docker.io/kubeguide/redis-master latest 423e126c2ad4 6 months ago 419.1 MB docker.io/kubeguide/guestbook-redis-slave latest 5429ea4e7990 6 months ago 109.5 MB gcr.io/google_containers/pause 0.8.0 2c40b0526b63 11 months ago 241.7 kB `&lt;/pre&gt; 我们看到新增加了一个镜像guestbook-php-frontend，tag为v2，接下来我们将通过两种方法来演示滚动升级。 ### 3.2通过配置文件 创建 frontend-controller-v2.yaml &lt;pre&gt;`apiVersion: v1 kind: ReplicationController metadata: name: frontend-v2 labels: name: frontend spec: replicas: 3 selector: name: frontend-v2 template: metadata: labels: name: frontend-v2 spec: containers: - name: php-redis image: guestbook-php-frontend:v2 env: - name: GET_HOSTS_FROM value: env ports: - containerPort: 80 `&lt;/pre&gt; Kubectl的执行过程如下： &lt;pre&gt;`[root@localhost ~]# kubectl rolling-update frontend -f frontend-controller-v2.yaml Creating frontend-v2 At beginning of loop: frontend replicas: 2, frontend-v2 replicas: 1 Updating frontend replicas: 2, frontend-v2 replicas: 1 At end of loop: frontend replicas: 2, frontend-v2 replicas: 1 At beginning of loop: frontend replicas: 1, frontend-v2 replicas: 2 Updating frontend replicas: 1, frontend-v2 replicas: 2 At end of loop: frontend replicas: 1, frontend-v2 replicas: 2 At beginning of loop: frontend replicas: 0, frontend-v2 replicas: 3 Updating frontend replicas: 0, frontend-v2 replicas: 3 At end of loop: frontend replicas: 0, frontend-v2 replicas: 3 Update succeeded. Deleting frontend frontend-v2 `&lt;/pre&gt; 查看pods创建过程, 我们看到新的POD副本从1开始增加，旧的POD副本从3逐步减少，最终旧的POD副本被删除。这样就完成了应用的升级。 &lt;pre&gt;`[root@localhost ~]# kubectl get rc CONTROLLER CONTAINER(S) IMAGE(S) SELECTOR REPLICAS frontend php-redis docker.io/kubeguide/guestbook-php-frontend name=frontend 3 frontend-v2 php-redis guestbook-php-frontend:v2 name=frontend-v2 1 [root@localhost ~]# kubectl get rc CONTROLLER CONTAINER(S) IMAGE(S) SELECTOR REPLICAS frontend php-redis docker.io/kubeguide/guestbook-php-frontend name=frontend 2 frontend-v2 php-redis guestbook-php-frontend:v2 name=frontend-v2 2 [root@localhost ~]# kubectl get rc CONTROLLER CONTAINER(S) IMAGE(S) SELECTOR REPLICAS frontend php-redis docker.io/kubeguide/guestbook-php-frontend name=frontend 1 frontend-v2 php-redis guestbook-php-frontend:v2 name=frontend-v2 3 [root@localhost ~]# kubectl get rc CONTROLLER CONTAINER(S) IMAGE(S) SELECTOR REPLICAS frontend-v2 php-redis guestbook-php-frontend:v2 name=frontend-v2 3 `&lt;/pre&gt; ### 3.3通过新版镜像 另一种方法是不使用配置文件，直接用kubectl rolling-update 加上--image参数指定新版镜像名称来滚动升级 kubectl rolling-update frontend --image=guestbook-php-frontend:v2 &lt;pre&gt;`[root@localhost ~]# kubectl rolling-update frontend --image=guestbook-php-frontend:v2 Creating frontend-cdb7f26e49a90eae43e257284310b1cf At beginning of loop: frontend replicas: 2, frontend-cdb7f26e49a90eae43e257284310b1cf replicas: 1 Updating frontend replicas: 2, frontend-cdb7f26e49a90eae43e257284310b1cf replicas: 1 At end of loop: frontend replicas: 2, frontend-cdb7f26e49a90eae43e257284310b1cf replicas: 1 At beginning of loop: frontend replicas: 1, frontend-cdb7f26e49a90eae43e257284310b1cf replicas: 2 Updating frontend replicas: 1, frontend-cdb7f26e49a90eae43e257284310b1cf replicas: 2 At end of loop: frontend replicas: 1, frontend-cdb7f26e49a90eae43e257284310b1cf replicas: 2 At beginning of loop: frontend replicas: 0, frontend-cdb7f26e49a90eae43e257284310b1cf replicas: 3 Updating frontend replicas: 0, frontend-cdb7f26e49a90eae43e257284310b1cf replicas: 3 At end of loop: frontend replicas: 0, frontend-cdb7f26e49a90eae43e257284310b1cf replicas: 3 Update succeeded. Deleting old controller: frontend Renaming frontend-cdb7f26e49a90eae43e257284310b1cf to frontend frontend `&lt;/pre&gt; 更新完成，查看RC，我们看到与配置文件不同，Kubectl给rc增加了一个key为&quot;deployment&quot;的label，当然这个名字可以通过--deployment-label-key参数修改。 &lt;pre&gt;`[root@localhost ~]# kubectl get rc CONTROLLER CONTAINER(S) IMAGE(S) SELECTOR REPLICAS frontend php-redis guestbook-php-frontend:v2 deployment=cdb7f26e49a90eae43e257284310b1cf,name=frontend 3 如果在更新过程中发现配置错误，可以通过执行kubectl rolling-update –rollback完成回滚kubectl rolling-update frontend –image=guestbook-php-frontend:v2 –rollback","tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://blog.yaodataking.com/tags/Kubernetes/"}]},{"title":"PostgreSQL 集群性能审计工具pgCluu","date":"2016-03-04T03:11:11.000Z","path":"2016/03/04/postgresql-pgcluu/","text":"pgCluu 是一个对 PostgreSQL 集群性能进行完整审计的工具，该工具分为两部分：1. collector 用于从 PostgreSQL 集群中获取统计数据，使用 psql 和 sar 工具。2. grapher 用于生成 HTML 报表和图表。 语法： PostgreSQL数据库和系统指标收集器 pgcluu_collectd [options] output_dir 报表生成器 pgcluu [options] -o report_dir input_dir `&lt;/pre&gt; 1.安装 好，下面我们介绍如何安装和使用它。 我们可以从pgCluu的[github](https://github.com/darold/pgcluu/releases)上下载。目前gpCluu版本更新到了2.4，下载后解压 tar -xzvf pgcluu-2.4.tar.gz 检查并编译，可执行文件默认会安装至 /usr/local/bin/目录下。 &lt;pre&gt;` [root@anzhy pgcluu-2.4]# perl Makefile.PL Checking if your kit is complete... Looks good Writing Makefile for pgCluu [root@anzhy pgcluu-2.4]# make cp pgcluu_collectd blib/script/pgcluu_collectd /usr/bin/perl -MExtUtils::MY -e &apos;MY-&amp;gt;fixin(shift)&apos; -- blib/script/pgcluu_collectd cp pgcluu blib/script/pgcluu /usr/bin/perl -MExtUtils::MY -e &apos;MY-&amp;gt;fixin(shift)&apos; -- blib/script/pgcluu Manifying blib/man1/pgcluu.1 [root@anzhy pgcluu-2.4]# sudo make install Installing /usr/local/share/man/man1/pgcluu.1 Installing /usr/local/bin/pgcluu_collectd Installing /usr/local/bin/pgcluu Appending installation info to /usr/lib64/perl5/perllocal.pod `&lt;/pre&gt; 2.配置 2.1 启动postgres 如果还没有启动postgres，我们先启动它，我们使用postgres用户 su - postgres pg_ctl -D /usr/local/pgsql/data start 2.2 执行收集器 我们先建立临时目录 mkdir /tmp/stat_db1/ pgcluu_collectd -D -i 60 /tmp/stat_db1/ 过几分钟我们停止收集 pgcluu_collectd -k &lt;pre&gt;&lt;/code&gt; [postgres@anzhy ~]$ mkdir /tmp/stat_db1/ [postgres@anzhy ~]$ pgcluu_collectd -D -i 60 /tmp/stat_db1/ [postgres@anzhy ~]$ LOG: Detach from terminal with pid: 2704 [postgres@anzhy ~]$ pgcluu_collectd -k OK: pgcluu_collectd exited with value 0 [postgres@anzhy ~]$ ll /tmp/stat_db1/ total 280 -rw-rw-r--. 1 postgres postgres 12554 Mar 4 09:19 pg_class_size.csv -rw-rw-r--. 1 postgres postgres 331 Mar 4 09:19 pg_database_size.csv -rw-rw-r--. 1 postgres postgres 0 Mar 4 09:19 pg_db_role_setting.csv -rw-rw-r--. 1 postgres postgres 4476 Mar 4 09:19 pg_hba.conf -rw-rw-r--. 1 postgres postgres 1636 Mar 4 09:19 pg_ident.conf -rw-rw-r--. 1 postgres postgres 128800 Mar 4 09:19 pg_settings.csv -rw-rw-r--. 1 postgres postgres 90 Mar 4 09:19 pg_stat_bgwriter.csv -rw-rw-r--. 1 postgres postgres 0 Mar 4 09:19 pg_stat_connections.csv -rw-rw-r--. 1 postgres postgres 342 Mar 4 09:19 pg_stat_database_conflicts.csv -rw-rw-r--. 1 postgres postgres 781 Mar 4 09:19 pg_stat_database.csv -rw-rw-r--. 1 postgres postgres 295 Mar 4 09:19 pg_statio_user_indexes.csv -rw-rw-r--. 1 postgres postgres 0 Mar 4 09:19 pg_statio_user_sequences.csv -rw-rw-r--. 1 postgres postgres 470 Mar 4 09:19 pg_statio_user_tables.csv -rw-rw-r--. 1 postgres postgres 1415 Mar 4 09:19 pg_stat_locks.csv -rw-rw-r--. 1 postgres postgres 0 Mar 4 09:19 pg_stat_missing_fkindexes.csv -rw-rw-r--. 1 postgres postgres 0 Mar 4 09:19 pg_stat_redundant_indexes.csv -rw-rw-r--. 1 postgres postgres 0 Mar 4 09:19 pg_stat_replication.csv -rw-rw-r--. 1 postgres postgres 0 Mar 4 09:19 pg_stat_unused_indexes.csv -rw-rw-r--. 1 postgres postgres 0 Mar 4 09:19 pg_stat_user_functions.csv -rw-rw-r--. 1 postgres postgres 328 Mar 4 09:19 pg_stat_user_indexes.csv -rw-rw-r--. 1 postgres postgres 966 Mar 4 09:19 pg_stat_user_tables.csv -rw-rw-r--. 1 postgres postgres 0 Mar 4 09:19 pg_stat_xact_user_functions.csv -rw-rw-r--. 1 postgres postgres 383 Mar 4 09:19 pg_stat_xact_user_tables.csv -rw-rw-r--. 1 postgres postgres 140 Mar 4 09:19 pg_tablespace_size.csv -rw-rw-r--. 1 postgres postgres 58 Mar 4 09:19 pg_xlog_stat.csv -rw-rw-r--. 1 postgres postgres 20543 Mar 4 09:19 postgresql.conf -rw-rw-r--. 1 postgres postgres 27402 Mar 4 09:19 sar_stats.dat -rw-rw-r--. 1 postgres postgres 21605 Mar 4 09:19 sysinfo.txt &lt;/code&gt;&lt;/pre&gt; 收集的数据保存在csv及txt文件中。 2.3 执行报表生成器 接下来我们通过报表生成器生成我们可以看的html网页 mkdir /tmp/report_db1/ pgcluu -o /tmp/report_db1/ /tmp/stat_db1/ &lt;pre&gt;` [postgres@anzhy ~]$ mkdir /tmp/report_db1/ [postgres@anzhy ~]$ pgcluu -o /tmp/report_db1/ /tmp/stat_db1/ [postgres@anzhy ~]$ ll /tmp/report_db1 total 1760 -rw-rw-r--. 1 postgres postgres 102595 Mar 4 09:21 bootstrap.min.css -rw-rw-r--. 1 postgres postgres 27834 Mar 4 09:21 bootstrap.min.js -rw-rw-r--. 1 postgres postgres 128934 Mar 4 09:21 cluster.html -rw-rw-r--. 1 postgres postgres 41925 Mar 4 09:21 database-all.html -rw-rw-r--. 1 postgres postgres 71078 Mar 4 09:21 database-pgbench.html -rw-rw-r--. 1 postgres postgres 71186 Mar 4 09:21 database-plpython_testdb.html -rw-rw-r--. 1 postgres postgres 74045 Mar 4 09:21 database-postgres.html -rw-rw-r--. 1 postgres postgres 70967 Mar 4 09:21 database-test.html -rw-rw-r--. 1 postgres postgres 71153 Mar 4 09:21 database-trigger_testdb.html -rw-rw-r--. 1 postgres postgres 125286 Mar 4 09:21 font-awesome.min.css -rw-rw-r--. 1 postgres postgres 80174 Mar 4 09:21 index.html -rw-rw-r--. 1 postgres postgres 104684 Mar 4 09:21 jquery.min.js -rw-rw-r--. 1 postgres postgres 43879 Mar 4 09:21 network-device0.html -rw-rw-r--. 1 postgres postgres 43873 Mar 4 09:21 network-device1.html -rw-rw-r--. 1 postgres postgres 43120 Mar 4 09:21 pgbench-index-size.html -rw-rw-r--. 1 postgres postgres 41934 Mar 4 09:21 pgbench-table-size.html -rw-rw-r--. 1 postgres postgres 3782 Mar 4 09:21 pgcluu.css -rw-rw-r--. 1 postgres postgres 104288 Mar 4 09:21 pgcluu.js -rw-rw-r--. 1 postgres postgres 42871 Mar 4 09:21 plpython_testdb-index-size.html -rw-rw-r--. 1 postgres postgres 41609 Mar 4 09:21 plpython_testdb-table-size.html -rw-rw-r--. 1 postgres postgres 42850 Mar 4 09:21 postgres-index-size.html -rw-rw-r--. 1 postgres postgres 41588 Mar 4 09:21 postgres-table-size.html -rw-rw-r--. 1 postgres postgres 17899 Mar 4 09:21 sorttable.js -rw-rw-r--. 1 postgres postgres 53244 Mar 4 09:21 system-device0.html -rw-rw-r--. 1 postgres postgres 75758 Mar 4 09:21 system.html -rw-rw-r--. 1 postgres postgres 42838 Mar 4 09:21 test-index-size.html -rw-rw-r--. 1 postgres postgres 41654 Mar 4 09:21 test-table-size.html -rw-rw-r--. 1 postgres postgres 42868 Mar 4 09:21 trigger_testdb-index-size.html -rw-rw-r--. 1 postgres postgres 41673 Mar 4 09:21 trigger_testdb-table-size.html 3.结果展示3.1 index 网页3.2 系统信息3.3 数据库信息 更多结果参见pgCluu官网Example 本文参考pgCluu官网","tags":[{"name":"postgresql","slug":"postgresql","permalink":"http://blog.yaodataking.com/tags/postgresql/"},{"name":"pgCluu","slug":"pgCluu","permalink":"http://blog.yaodataking.com/tags/pgCluu/"}]},{"title":"Kubernetes入门实战(2)：Kubernetes集群初探","date":"2016-02-27T12:18:55.000Z","path":"2016/02/27/kubernetes-cluster-1/","text":"上文我们在一台虚机上演示了Kubernetes基于redis和docker的guestbook留言簿案例，本文我们将通过配置Kubernetes集群的方式继续深入研究。集群组件安装如下配置。 IPNAMEComponent192.168.199.51centos-masteretcd,kube-apiserver,kube-controller-manager,kube-scheduler192.168.199.52centos-minion01kube-proxy,kubelet,docker 192.168.199.53centos-minion02kube-proxy,kubelet,docker 主机环境：centos 7，三台虚机。1.准备工作以下工作在每台虚机执行。1.1 停止防火墙 #systemctl disable firewalld #systemctl stop firewalld 1.2 修改iptables把icmp-host-prohibited两条注释掉 # sample configuration for iptables service # you can edit this manually or use system-config-firewall # please do not ask us to add additional ports/services to this default configuration *filter :INPUT ACCEPT [0:0] :FORWARD ACCEPT [0:0] :OUTPUT ACCEPT [0:0] -A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT -A INPUT -p icmp -j ACCEPT -A INPUT -i lo -j ACCEPT -A INPUT -p tcp -m state --state NEW -m tcp --dport 22 -j ACCEPT #-A INPUT -j REJECT --reject-with icmp-host-prohibited #-A FORWARD -j REJECT --reject-with icmp-host-prohibited COMMIT `&lt;/pre&gt; 重启iptables #systemctl restart iptables 1.3 使用阿里镜像 #wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo 1.4 更新主机列表 #echo &quot;192.168.199.51 centos-master 192.168.199.52 centos-minion01 192.168.199.53 centos-minion02&quot; &amp;gt;&amp;gt; /etc/hosts 2.安装配置kubernetes master 2.1 在centos-master上安装 #yum install kubernetes-master #yum install etcd 2.2配置 Kubernetes services #vi /etc/kubernetes/config &lt;pre&gt;`### # kubernetes system config # # The following values are used to configure various aspects of all # kubernetes services, including # # kube-apiserver.service # kube-controller-manager.service # kube-scheduler.service # kubelet.service # kube-proxy.service # logging to stderr means we get it in the systemd journal KUBE_LOGTOSTDERR=&quot;--logtostderr=true&quot; # journal message level, 0 is debug KUBE_LOG_LEVEL=&quot;--v=0&quot; # Should this cluster be allowed to run privileged docker containers KUBE_ALLOW_PRIV=&quot;--allow-privileged=false&quot; # How the controller-manager, scheduler, and proxy find the apiserver KUBE_MASTER=&quot;--master=http://centos-master:8080&quot; `&lt;/pre&gt; 2.3配置Kubernetes API server #vi /etc/kubernetes/apiserver &lt;pre&gt;`### # kubernetes system config # # The following values are used to configure the kube-apiserver # # The address on the local server to listen to. KUBE_API_ADDRESS=&quot;--insecure-bind-address=0.0.0.0&quot; # The port on the local server to listen on. KUBE_API_PORT=&quot;--insecure-port=8080&quot; # Port minions listen on #KUBELET_PORT=&quot;--kubelet-port=10250&quot; # Comma separated list of nodes in the etcd cluster KUBE_ETCD_SERVERS=&quot;--etcd-servers=http://127.0.0.1:2379&quot; # Address range to use for services KUBE_SERVICE_ADDRESSES=&quot;--service-cluster-ip-range=10.254.0.0/16&quot; # default admission control policies KUBE_ADMISSION_CONTROL=&quot;--admission-control=NamespaceLifecycle,NamespaceExists,LimitRanger,SecurityContextDeny,ResourceQuota&quot; # Add your own! KUBE_API_ARGS=&quot;&quot; `&lt;/pre&gt; 2.4 启动服务 &lt;pre&gt;`for SERVICES in etcd kube-apiserver kube-controller-manager kube-scheduler; do systemctl restart $SERVICES systemctl enable $SERVICES systemctl status $SERVICES done `&lt;/pre&gt; 2.5 停止服务 &lt;pre&gt;`for SERVICES in etcd kube-apiserver kube-controller-manager kube-scheduler; do systemctl stop $SERVICES done `&lt;/pre&gt; 3.安装配置kubernetes node 3.1 在centos-minion01及centos-minion02上安装 #yum install kubernetes-node #vi /etc/kubernetes/config &lt;pre&gt;`### # kubernetes system config # # The following values are used to configure various aspects of all # kubernetes services, including # # kube-apiserver.service # kube-controller-manager.service # kube-scheduler.service # kubelet.service # kube-proxy.service # logging to stderr means we get it in the systemd journal KUBE_LOGTOSTDERR=&quot;--logtostderr=true&quot; # journal message level, 0 is debug KUBE_LOG_LEVEL=&quot;--v=0&quot; # Should this cluster be allowed to run privileged docker containers KUBE_ALLOW_PRIV=&quot;--allow-privileged=false&quot; # How the controller-manager, scheduler, and proxy find the apiserver KUBE_MASTER=&quot;--master=http://centos-master:8080&quot; `&lt;/pre&gt; 3.2 配置 kubelet文件 vi /etc/kubernetes/kubelet centos-minion01 &lt;pre&gt;`### # kubernetes kubelet (minion) config # The address for the info server to serve on (set to 0.0.0.0 or &quot;&quot; for all interfaces) KUBELET_ADDRESS=&quot;--address=0.0.0.0&quot; # The port for the info server to serve on KUBELET_PORT=&quot;--port=10250&quot; # You may leave this blank to use the actual hostname KUBELET_HOSTNAME=&quot;--hostname-override=centos-minion01&quot; # location of the api-server KUBELET_API_SERVER=&quot;--api-servers=http://centos-master:8080&quot; # pod infrastructure container KUBELET_POD_INFRA_CONTAINER=&quot;--pod-infra-container-image=registry.access.redhat.com/rhel7/pod-infrastructure:latest&quot; # Add your own! KUBELET_ARGS=&quot;&quot; `&lt;/pre&gt; centos-minion02 &lt;pre&gt;` ### # kubernetes kubelet (minion) config # The address for the info server to serve on (set to 0.0.0.0 or &quot;&quot; for all interfaces) KUBELET_ADDRESS=&quot;--address=0.0.0.0&quot; # The port for the info server to serve on KUBELET_PORT=&quot;--port=10250&quot; # You may leave this blank to use the actual hostname KUBELET_HOSTNAME=&quot;--hostname-override=centos-minion02&quot; # location of the api-server KUBELET_API_SERVER=&quot;--api-servers=http://centos-master:8080&quot; # pod infrastructure container KUBELET_POD_INFRA_CONTAINER=&quot;--pod-infra-container-image=registry.access.redhat.com/rhel7/pod-infrastructure:latest&quot; # Add your own! KUBELET_ARGS=&quot;&quot; `&lt;/pre&gt; 3.3 配置config文件 &lt;pre&gt;`### # kubernetes system config # # The following values are used to configure various aspects of all # kubernetes services, including # # kube-apiserver.service # kube-controller-manager.service # kube-scheduler.service # kubelet.service # kube-proxy.service # logging to stderr means we get it in the systemd journal KUBE_LOGTOSTDERR=&quot;--logtostderr=true&quot; # journal message level, 0 is debug KUBE_LOG_LEVEL=&quot;--v=0&quot; # Should this cluster be allowed to run privileged docker containers KUBE_ALLOW_PRIV=&quot;--allow-privileged=false&quot; # How the controller-manager, scheduler, and proxy find the apiserver KUBE_MASTER=&quot;--master=http://centos-master:8080&quot; `&lt;/pre&gt; 3.4 启动服务 &lt;pre&gt;`for SERVICES in kube-proxy kubelet docker; do systemctl restart $SERVICES systemctl enable $SERVICES systemctl status $SERVICES done `&lt;/pre&gt; 在centos-minion01上启动 &lt;pre&gt;`[root@centos-minion01 ~]# for SERVICES in kube-proxy kubelet docker; do &amp;gt; systemctl restart $SERVICES &amp;gt; systemctl enable $SERVICES &amp;gt; systemctl status $SERVICES &amp;gt; done ● kube-proxy.service - Kubernetes Kube-Proxy Server Loaded: loaded (/usr/lib/systemd/system/kube-proxy.service; enabled; vendor preset: disabled) Active: active (running) since Sat 2016-02-27 07:35:45 CST; 227ms ago Docs: https://github.com/GoogleCloudPlatform/kubernetes Main PID: 3682 (kube-proxy) CGroup: /system.slice/kube-proxy.service └─3682 /usr/bin/kube-proxy --logtostderr=true --v=0 --master=http://centos-master:8080 Feb 27 07:35:45 centos-minion01 systemd[1]: Started Kubernetes Kube-Proxy Server. Feb 27 07:35:45 centos-minion01 systemd[1]: Starting Kubernetes Kube-Proxy Server... Feb 27 07:35:45 centos-minion01 kube-proxy[3682]: E0227 07:35:45.735033 3682 proxier.go:193] Error re...ory Feb 27 07:35:45 centos-minion01 kube-proxy[3682]: Try `iptables -h&apos; or &apos;iptables --help&apos; for more information. Feb 27 07:35:45 centos-minion01 kube-proxy[3682]: E0227 07:35:45.738008 3682 proxier.go:197] Error re...ory Feb 27 07:35:45 centos-minion01 kube-proxy[3682]: Try `iptables -h&apos; or &apos;iptables --help&apos; for more information. Hint: Some lines were ellipsized, use -l to show in full. ● kubelet.service - Kubernetes Kubelet Server Loaded: loaded (/usr/lib/systemd/system/kubelet.service; enabled; vendor preset: disabled) Active: active (running) since Sat 2016-02-27 07:35:46 CST; 201ms ago Docs: https://github.com/GoogleCloudPlatform/kubernetes Main PID: 3816 (kubelet) CGroup: /system.slice/kubelet.service └─3816 /usr/bin/kubelet --logtostderr=true --v=0 --api-servers=http://centos-master:8080 --addre... Feb 27 07:35:46 centos-minion01 systemd[1]: Started Kubernetes Kubelet Server. Feb 27 07:35:46 centos-minion01 systemd[1]: Starting Kubernetes Kubelet Server... ● docker.service - Docker Application Container Engine Loaded: loaded (/usr/lib/systemd/system/docker.service; enabled; vendor preset: disabled) Active: active (running) since Sat 2016-02-27 07:35:46 CST; 188ms ago Docs: http://docs.docker.com Main PID: 3875 (docker) CGroup: /system.slice/docker.service └─3875 /usr/bin/docker daemon --selinux-enabled Feb 27 07:35:46 centos-minion01 docker[3875]: time=&quot;2016-02-27T07:35:46.806484947+08:00&quot; level=info msg...dge&quot; Feb 27 07:35:46 centos-minion01 docker[3875]: time=&quot;2016-02-27T07:35:46.806514467+08:00&quot; level=info msg...dge&quot; Feb 27 07:35:46 centos-minion01 docker[3875]: time=&quot;2016-02-27T07:35:46.815103655+08:00&quot; level=warning ...s 1&quot; Feb 27 07:35:46 centos-minion01 docker[3875]: time=&quot;2016-02-27T07:35:46.820513002+08:00&quot; level=info msg...lse&quot; Feb 27 07:35:46 centos-minion01 docker[3875]: time=&quot;2016-02-27T07:35:46.877030818+08:00&quot; level=info msg...rt.&quot; Feb 27 07:35:46 centos-minion01 docker[3875]: .... Feb 27 07:35:46 centos-minion01 docker[3875]: time=&quot;2016-02-27T07:35:46.880893846+08:00&quot; level=info msg...ne.&quot; Feb 27 07:35:46 centos-minion01 docker[3875]: time=&quot;2016-02-27T07:35:46.880919835+08:00&quot; level=info msg...ion&quot; Feb 27 07:35:46 centos-minion01 docker[3875]: time=&quot;2016-02-27T07:35:46.880937118+08:00&quot; level=info msg...ntos Feb 27 07:35:46 centos-minion01 systemd[1]: Started Docker Application Container Engine. Hint: Some lines were ellipsized, use -l to show in full. [root@centos-minion01 ~]# `&lt;/pre&gt; 在centos-minion02上启动 &lt;pre&gt;`[root@centos-minion02 kubernetes]# for SERVICES in kube-proxy kubelet docker; do &amp;gt; systemctl restart $SERVICES &amp;gt; systemctl enable $SERVICES &amp;gt; systemctl status $SERVICES &amp;gt; done ● kube-proxy.service - Kubernetes Kube-Proxy Server Loaded: loaded (/usr/lib/systemd/system/kube-proxy.service; enabled; vendor preset: disabled) Active: active (running) since Sat 2016-02-27 07:32:22 CST; 221ms ago Docs: https://github.com/GoogleCloudPlatform/kubernetes Main PID: 3138 (kube-proxy) CGroup: /system.slice/kube-proxy.service └─3138 /usr/bin/kube-proxy --logtostderr=true --v=0 --master=http://centos-master:8080 Feb 27 07:32:22 centos-minion02 systemd[1]: Started Kubernetes Kube-Proxy Server. Feb 27 07:32:22 centos-minion02 systemd[1]: Starting Kubernetes Kube-Proxy Server... Feb 27 07:32:22 centos-minion02 kube-proxy[3138]: E0227 07:32:22.774533 3138 server.go:324] Not tryi...und Feb 27 07:32:22 centos-minion02 kube-proxy[3138]: E0227 07:32:22.857247 3138 proxier.go:193] Error r...ory Feb 27 07:32:22 centos-minion02 kube-proxy[3138]: Try `iptables -h&apos; or &apos;iptables --help&apos; for more infor...on. Feb 27 07:32:22 centos-minion02 kube-proxy[3138]: E0227 07:32:22.859129 3138 proxier.go:197] Error r...ory Feb 27 07:32:22 centos-minion02 kube-proxy[3138]: Try `iptables -h&apos; or &apos;iptables --help&apos; for more infor...on. Hint: Some lines were ellipsized, use -l to show in full. ● kubelet.service - Kubernetes Kubelet Server Loaded: loaded (/usr/lib/systemd/system/kubelet.service; enabled; vendor preset: disabled) Active: active (running) since Sat 2016-02-27 07:32:23 CST; 201ms ago Docs: https://github.com/GoogleCloudPlatform/kubernetes Main PID: 3279 (kubelet) CGroup: /system.slice/kubelet.service └─3279 /usr/bin/kubelet --logtostderr=true --v=0 --api-servers=http://centos-master:8080 --addr... Feb 27 07:32:23 centos-minion02 systemd[1]: Started Kubernetes Kubelet Server. Feb 27 07:32:23 centos-minion02 systemd[1]: Starting Kubernetes Kubelet Server... ● docker.service - Docker Application Container Engine Loaded: loaded (/usr/lib/systemd/system/docker.service; enabled; vendor preset: disabled) Active: active (running) since Sat 2016-02-27 07:32:23 CST; 186ms ago Docs: http://docs.docker.com Main PID: 3338 (docker) CGroup: /system.slice/docker.service └─3338 /usr/bin/docker daemon --selinux-enabled Feb 27 07:32:23 centos-minion02 docker[3338]: time=&quot;2016-02-27T07:32:23.772348922+08:00&quot; level=info ms...r\\&quot;&quot; Feb 27 07:32:23 centos-minion02 docker[3338]: time=&quot;2016-02-27T07:32:23.779372746+08:00&quot; level=info ms...dge&quot; Feb 27 07:32:23 centos-minion02 docker[3338]: time=&quot;2016-02-27T07:32:23.779397422+08:00&quot; level=info ms...dge&quot; Feb 27 07:32:23 centos-minion02 docker[3338]: time=&quot;2016-02-27T07:32:23.787899136+08:00&quot; level=warning...s 1&quot; Feb 27 07:32:23 centos-minion02 docker[3338]: time=&quot;2016-02-27T07:32:23.791497828+08:00&quot; level=info ms...lse&quot; Feb 27 07:32:23 centos-minion02 docker[3338]: time=&quot;2016-02-27T07:32:23.843065746+08:00&quot; level=info ms...rt.&quot; Feb 27 07:32:23 centos-minion02 docker[3338]: time=&quot;2016-02-27T07:32:23.843241402+08:00&quot; level=info ms...ne.&quot; Feb 27 07:32:23 centos-minion02 docker[3338]: time=&quot;2016-02-27T07:32:23.843258440+08:00&quot; level=info ms...ion&quot; Feb 27 07:32:23 centos-minion02 docker[3338]: time=&quot;2016-02-27T07:32:23.843271897+08:00&quot; level=info ms...ntos Feb 27 07:32:23 centos-minion02 systemd[1]: Started Docker Application Container Engine. Hint: Some lines were ellipsized, use -l to show in full. [root@centos-minion02 kubernetes]# `&lt;/pre&gt; 3.5 停止服务 &lt;pre&gt;`for SERVICES in kube-proxy kubelet docker; do systemctl stop $SERVICES done `&lt;/pre&gt; 4\\. 检查及确认状态 #kubectl get nodes #kubectl cluster-info 我们看到2个节点都正常启动。 &lt;pre&gt;`[root@centos-master ~]# kubectl get nodes NAME LABELS STATUS AGE centos-minion01 kubernetes.io/hostname=centos-minion01 Ready 1m centos-minion02 kubernetes.io/hostname=centos-minion02 Ready 51s [root@centos-master ~]# kubectl cluster-info Kubernetes master is running at http://localhost:8080 [root@centos-master ~]# `&lt;/pre&gt; 5.创建MYSQL POD 5.1 建立工作目录并查看API版本 在kubernetes master节点 #mkdir mysqlpod #cd mysqlpod #kubectl api-versions 我们看到API版本为1,所以设置文件时用v1就可以了。 &lt;pre&gt;` [root@centos-master mysqlpod]# kubectl api-versions Available Server Api Versions: v1 `&lt;/pre&gt; 5.2 编写mysql的pod文件 #vi mysqlpod.yaml &lt;pre&gt;`apiVersion: v1 kind: Pod metadata: name: mysql labels: name: mysql spec: containers: - name: mysql image: mysql env: - name: MYSQL_ROOT_PASSWORD value: 123456 ports: - containerPort: 3306 `&lt;/pre&gt; 5.3 启动POD #kubectl create -f mysqlpod.yaml #kubectl get pods #kubectl describe pods mysql &lt;pre&gt;`Events: FirstSeen LastSeen Count From SubobjectPath Reason Message ───────── ──────── ───── ──── ───────────── ────── ─────── 49s 49s 1 {scheduler } Scheduled Successfully assigned mysql to centos-minion02 49s 49s 1 {kubelet centos-minion02} implicitly required container POD Pulled Container image &quot;registry.access.redhat.com/rhel7/pod-infrastructure:latest&quot; already present on machine 49s 49s 1 {kubelet centos-minion02} implicitly required container POD Created Created with docker id 31f65f03a960 48s 48s 1 {kubelet centos-minion02} implicitly required container POD Started Started with docker id 31f65f03a960 48s 48s 1 {kubelet centos-minion02} spec.containers{mysql} Pulled Container image &quot;mysql&quot; already present on machine 48s 48s 1 {kubelet centos-minion02} spec.containers{mysql} Created Created with docker id aa39d65008dc 47s 47s 1 {kubelet centos-minion02} spec.containers{mysql} Started Started with docker id aa39d65008dc `&lt;/pre&gt; 我们看到这个POD启动在centos-minion02虚机上，首先它启动了一个叫pod-infrastructure的容器，然后去找本机是否有mysql镜像，没有就去下载，已有的话就直接创建一个mysql容器。 在centos-minion02上看启动的容器。 #docker ps &lt;pre&gt;`[root@centos-minion02 ~]# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES aa39d65008dc mysql &quot;/entrypoint.sh mysql&quot; About an hour ago Up About an hour k8s_mysql.1431d49_mysql_default_c935d35d-dce2-11e5-9ab1-000c29beeacc_95c7a9b7 31f65f03a960 registry.access.redhat.com/rhel7/pod-infrastructure:latest &quot;/pod&quot; About an hour ago Up About an hour k8s_POD.36d00adb_mysql_default_c935d35d-dce2-11e5-9ab1-000c29beeacc_1cf5c985 `&lt;/pre&gt; 我们看到启动两个容器，一个是mysql，一个是pod-infrastructure。 5.4 编写mysql的服务文件 #vi mysqlservice.yaml &lt;pre&gt;`apiVersion: v1 kind: Service metadata: labels: name: mysql name: mysql spec: ports: - port: 3306 selector: name: mysql `&lt;/pre&gt; 5.4 启动服务 #kubectl create -f mysqlservice.yaml &lt;pre&gt;`[root@centos-master mysqlpod]# kubectl get services NAME CLUSTER_IP EXTERNAL_IP PORT(S) SELECTOR AGE kubernetes 10.254.0.1 443/TCP 18h mysql 10.254.62.21 3306/TCP name=mysql 9s [root@centos-master mysqlpod]# `&lt;/pre&gt; 5.5 mysql登录 在centos-minion02上连接mysql的POD，我们看到连接正常。 &lt;pre&gt;`[root@centos-minion02 ~]# mysql -uroot -p -h10.254.62.21 Enter password: Welcome to the MySQL monitor. Commands end with ; or \\g. Your MySQL connection id is 3 Server version: 5.7.10 MySQL Community Server (GPL) Copyright (c) 2000, 2016, Oracle and/or its affiliates. All rights reserved. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type &apos;help;&apos; or &apos;\\h&apos; for help. Type &apos;\\c&apos; to clear the current input statement. mysql&amp;gt; show databases; +--------------------+ | Database | +--------------------+ | information_schema | | mysql | | performance_schema | | sys | +--------------------+ 4 rows in set (0.01 sec) mysql&amp;gt; 参考：Kubernetes权威指南","tags":[{"name":"Docker","slug":"Docker","permalink":"http://blog.yaodataking.com/tags/Docker/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://blog.yaodataking.com/tags/Kubernetes/"},{"name":"Mysql","slug":"Mysql","permalink":"http://blog.yaodataking.com/tags/Mysql/"}]},{"title":"基于Docker容器的Mycat性能测试案例","date":"2016-02-25T14:53:16.000Z","path":"2016/02/25/mycat-performance-test/","text":"本文我们将基于Docker容器的Mycat做性能测试。测试要求是：创建一个 person表，主键为Id，hash方式分片，主键自增（采用数据库方式），当自增的step分别为10,100,1万的三种情况下，对此表做性能测试。person表结构如下Id，主键，Mycat自增主键name,字符串，16字节最长school,毕业学校，数字，1-1000范围，是学校编号age，年龄，18-60addr,地址，32字节，建议为 gz-tianhe（城市-地区两级 枚举的仿真数据）zcode,邮编，birth，生日，为日期类型， 1980到2010年之间随机的日期score，得分，0-100分测试环境我们这里采用一台虚机，三个Docker容器，虚机上运行测试工具，一个Docker容器运行Myat Server，二个Docker容器运行MySQL测试工具我们采用Mycat标准测试工具。 1.准备工作1.1 启动容器 #docker start mysqlsrv101 #docker start mysqlsrv102 #docker start mycat关于这三个容器的安装详见我的另一篇博文基于Mycat的MySQL主从读写分离及自动切换的docker实现。 1.2 测试工具下载我们从Mycat的github下载,目前最新版1.51.3 person表按要求定义如下 create table person ( Id bigint not null primary key AUTO_INCREMENT, name varchar(16) , school int, age int, addr varchar(32), zcode int, birth date, score int ) AUTO_INCREMENT = 1; `&lt;/pre&gt; 1.4 mycat_sequence表定义如下，mycat_sequence表是用来存放全局序列号的 &lt;pre&gt;` CREATE TABLE mycat_sequence( name VARCHAR(50) NOT NULL, current_value INT NOT NULL, increment INT NOT NULL DEFAULT 100, PRIMARY KEY(name) ) ENGINE=InnoDB; `&lt;/pre&gt; 2\\. 配置文件修改 在mycat容器conf目录下 2.1修改schema.xml,增加person表，自动增长设为true，增加mycat_sequence表为存放全局序列号. [![2016-02-25_22-41-21](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/02/2016-02-25_22-41-21.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/02/2016-02-25_22-41-21.jpg) 2.2修改rule.xml ，这里我们对addr使用一致性hash算法分区。 [![2016-02-25_22-42-59](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/02/2016-02-25_22-42-59.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/02/2016-02-25_22-42-59.jpg) [![2016-02-25_22-43-40](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/02/2016-02-25_22-43-40.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/02/2016-02-25_22-43-40.jpg) 2.3修改server.xml,在system下添加,1表示使用数据库方式，0表示本地文件方式。这里我们采用数据库方式。 [![2016-02-25_22-45-42](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/02/2016-02-25_22-45-42.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/02/2016-02-25_22-45-42.jpg) 2.4 修改sequence_db_conf.properties,添加要给序号的表, 并指定分区。 &lt;pre&gt;` #sequence stored in datanode PERSON=dn1 `&lt;/pre&gt; 2.5 在测试工具bin目录下，建立insert_person.sql文件 &lt;pre&gt;` total=100000 sql=insert into person(Id,name,school,age,addr,zcode,birth,score) values(next value for MYCATSEQ_PERSON,&apos;${char([a-f,0-9]8:16)}&apos;,&apos;${int(1-1000)}&apos;,&apos;${int(18-60)}&apos;,&apos;${enum(gz-tianhe,sh-huangpu,sz-baoan)}&apos;,&apos;${int(100000-90000)}&apos;,&apos;${date(yyyyMMdd-[1980-2010]y)}&apos;,&apos;${int(0-100)}&apos;) `&lt;/pre&gt; [![2016-02-25_22-18-19](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/02/2016-02-25_22-18-19.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/02/2016-02-25_22-18-19.jpg) 以上工作完成，我们进入mycat 9066管理端，做 reload @@config_all;使刚才配置生效。 [![2016-02-24_16-19-35](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/02/2016-02-24_16-19-35.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/02/2016-02-24_16-19-35.jpg) 3 其他设置 3.1进入mycat 8066 端口，执行1.3,1.4步骤语句创建person表及mycat_sequence表. 3.2进入mysql101客户端,执行创建三个函数。 &lt;pre&gt;` DROP FUNCTION IF EXISTS `mycat_seq_currval`; DELIMITER ;; CREATE FUNCTION `mycat_seq_currval`(seq_name VARCHAR(50)) RETURNS varchar(64) CHARSET utf8 DETERMINISTIC BEGIN DECLARE retval VARCHAR(64); SET retval=&quot;-999999999,null&quot;; SELECT concat(CAST(current_value AS CHAR),&quot;,&quot;,CAST(increment AS CHAR)) INTO retval FROM mycat_sequence WHERE name = seq_name; RETURN retval; END ;; DELIMITER; DROP FUNCTION IF EXISTS `mycat_seq_setval`; DELIMITER;; CREATE FUNCTION `mycat_seq_setval`(seq_name VARCHAR(50),value INTEGER) RETURNS varchar(64) CHARSET utf8 DETERMINISTIC BEGIN UPDATE mycat_sequence SET current_value = value WHERE name = seq_name; RETURN mycat_seq_currval(seq_name); END;; DELIMITER; DROP FUNCTION IF EXISTS `mycat_seq_nextval`; DELIMITER ;; CREATE FUNCTION `mycat_seq_nextval`(seq_name VARCHAR(50)) RETURNS varchar(64) CHARSET utf8 DETERMINISTIC BEGIN UPDATE mycat_sequence SET current_value = current_value + increment WHERE name = seq_name; RETURN mycat_seq_currval(seq_name); END ;; DELIMITER; `&lt;/pre&gt; 4.测试步骤 4.1我们先开始测试步长10 进入mycat 8066 端口, 执行下面语句 &lt;pre&gt;` DELETE FROM mycat_sequence; INSERT INTO mycat_sequence(name,current_value,increment) VALUES (&apos;PERSON&apos;, 0, 10); `&lt;/pre&gt; 在mycat测试工具bin目录,执行 &lt;pre&gt;` ./test_stand_insert_perf.sh jdbc:mysql://localhost:8066/TESTDB test test 50 &quot;file=person-insert.sql&quot; `&lt;/pre&gt; 最后结果如下 [![2016-02-25_20-44-29](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/02/2016-02-25_20-44-29.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/02/2016-02-25_20-44-29.jpg) 我们看到10万笔记录，失败了300笔，时间耗时很长，tps很差。 4.2接下来我们先开始测试步长100 进入mycat 8066 端口, 执行下面语句 &lt;pre&gt;` DELETE FROM mycat_sequence; INSERT INTO mycat_sequence(name,current_value,increment) VALUES (&apos;PERSON&apos;, 0, 100); `&lt;/pre&gt; 清空person已有的数据 &lt;pre&gt;` drop table person; create table person ( Id bigint not null primary key AUTO_INCREMENT, name varchar(16) , school int, age int, addr varchar(32), zcode int, birth date, score int ) AUTO_INCREMENT = 1; `&lt;/pre&gt; [![2016-02-25_20-20-31](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/02/2016-02-25_20-20-31.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/02/2016-02-25_20-20-31.jpg) 在mycat测试工具bin目录,执行 &lt;pre&gt;` ./test_stand_insert_perf.sh jdbc:mysql://localhost:8066/TESTDB test test 50 &quot;file=person-insert.sql&quot; `&lt;/pre&gt; 最后结果如下 [![2016-02-25_20-50-59](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/02/2016-02-25_20-50-59.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/02/2016-02-25_20-50-59.jpg) 我们看到10万笔记录，失败了200笔，耗时缩短了，tps增加了。 4.3接下来我们先开始测试步长10000 进入mycat 8066 端口, 执行下面语句 &lt;pre&gt;` DELETE FROM mycat_sequence; INSERT INTO mycat_sequence(name,current_value,increment) VALUES (&apos;PERSON&apos;, 0, 10000); `&lt;/pre&gt; 清空person已有的数据 &lt;pre&gt;` drop table person; create table person ( Id bigint not null primary key AUTO_INCREMENT, name varchar(16) , school int, age int, addr varchar(32), zcode int, birth date, score int ) AUTO_INCREMENT = 1; `&lt;/pre&gt; 在mycat测试工具bin目录,执行 &lt;pre&gt;` ./test_stand_insert_perf.sh jdbc:mysql://localhost:8066/TESTDB test test 50 &quot;file=person-insert.sql&quot; 最后结果如下我们看到10万笔记录，全部成功插入，耗时很短，tps显著增加。结论，从以上三种步长测试结果看，明显步长越大，成功率越高，效果越佳。 参考：Mycat权威指南","tags":[{"name":"Docker","slug":"Docker","permalink":"http://blog.yaodataking.com/tags/Docker/"},{"name":"Mycat","slug":"Mycat","permalink":"http://blog.yaodataking.com/tags/Mycat/"},{"name":"Mysql","slug":"Mysql","permalink":"http://blog.yaodataking.com/tags/Mysql/"}]},{"title":"PostgreSQL压力测试工具pgbench入门","date":"2016-02-11T07:35:55.000Z","path":"2016/02/11/postgresql-pgbench/","text":"pgbench 是一个简单的测试PostgreSQL性能的程序。它可以运行在多个并发数据库会话中，并不停地执行同一顺序SQL命令，然后计算平均事务速度，也就是tps。pgbench 默认使用TPC-B方法来测试五个包含 SELECT、UPDATE、和 INSERT命令的脚本。当然，我们也可以使用自己的脚本来测试。本文将使用pgbench 默认提供的脚本来测试。1.环境及安装环境centos 6.5 ,内存 4G，postgesql 版本 9.3.4进入postgres中的pgbench源代码目录 #cd /home/src/pgsql/contrib/pgbench #make all #make install2.初始化数据启动数据库 #cd /usr/local/pgsql #pg_ctl -D data start #createdb pgbench初始化测试数据 #pgbench -i pgbench3. 测试3.1 模拟1个客户端 #pgbench -M simple -c 1 -T 100 -r pgbench starting vacuum...end. transaction type: TPC-B (sort of) scaling factor: 1 query mode: simple number of clients: 1 number of threads: 1 duration: 100 s number of transactions actually processed: 53477 tps = 534.764738 (including connections establishing) tps = 534.785402 (excluding connections establishing) statement latencies in milliseconds: 0.004993 \\set nbranches 1 * :scale 0.000899 \\set ntellers 10 * :scale 0.000661 \\set naccounts 100000 * :scale 0.000827 \\setrandom aid 1 :naccounts 0.000522 \\setrandom bid 1 :nbranches 0.000544 \\setrandom tid 1 :ntellers 0.000577 \\setrandom delta -5000 5000 0.076682 BEGIN; 0.250163 UPDATE pgbench_accounts SET abalance = abalance + :delta WHERE aid = :aid; 0.155333 SELECT abalance FROM pgbench_accounts WHERE aid = :aid; 0.171949 UPDATE pgbench_tellers SET tbalance = tbalance + :delta WHERE tid = :tid; 0.152673 UPDATE pgbench_branches SET bbalance = bbalance + :delta WHERE bid = :bid; 0.130653 INSERT INTO pgbench_history (tid, bid, aid, delta, mtime) VALUES (:tid, :bid, :aid, :delta, CURRENT_TIMESTAMP); 0.909966 END; `&lt;/pre&gt; 3.2 模拟50个客户端 #pgbench -M simple -c 50 -T 100 -r pgbench &lt;pre&gt;` starting vacuum...end. transaction type: TPC-B (sort of) scaling factor: 1 query mode: simple number of clients: 50 number of threads: 1 duration: 100 s number of transactions actually processed: 36036 tps = 359.955238 (including connections establishing) tps = 360.320538 (excluding connections establishing) statement latencies in milliseconds: 0.003963 \\set nbranches 1 * :scale 0.001221 \\set ntellers 10 * :scale 0.000811 \\set naccounts 100000 * :scale 0.001040 \\setrandom aid 1 :naccounts 0.000667 \\setrandom bid 1 :nbranches 0.000662 \\setrandom tid 1 :ntellers 0.000673 \\setrandom delta -5000 5000 0.585555 BEGIN; 1.312489 UPDATE pgbench_accounts SET abalance = abalance + :delta WHERE aid = :aid; 1.089996 SELECT abalance FROM pgbench_accounts WHERE aid = :aid; 109.413961 UPDATE pgbench_tellers SET tbalance = tbalance + :delta WHERE tid = :tid; 23.461208 UPDATE pgbench_branches SET bbalance = bbalance + :delta WHERE bid = :bid; 0.577524 INSERT INTO pgbench_history (tid, bid, aid, delta, mtime) VALUES (:tid, :bid, :aid, :delta, CURRENT_TIMESTAMP); 2.212504 END; `&lt;/pre&gt; 3.3 模拟100个客户端（数据库默认设置最大连接数) #pgbench -M simple -c 100 -T 100 -r pgbench &lt;pre&gt;` starting vacuum...end. transaction type: TPC-B (sort of) scaling factor: 1 query mode: simple number of clients: 100 number of threads: 1 duration: 100 s number of transactions actually processed: 33268 tps = 331.936164 (including connections establishing) tps = 333.080359 (excluding connections establishing) statement latencies in milliseconds: 0.003908 \\set nbranches 1 * :scale 0.001205 \\set ntellers 10 * :scale 0.000798 \\set naccounts 100000 * :scale 0.001023 \\setrandom aid 1 :naccounts 0.000649 \\setrandom bid 1 :nbranches 0.000658 \\setrandom tid 1 :ntellers 0.000684 \\setrandom delta -5000 5000 0.909069 BEGIN; 1.845696 UPDATE pgbench_accounts SET abalance = abalance + :delta WHERE aid = :aid; 1.291101 SELECT abalance FROM pgbench_accounts WHERE aid = :aid; 266.673504 UPDATE pgbench_tellers SET tbalance = tbalance + :delta WHERE tid = :tid; 26.131598 UPDATE pgbench_branches SET bbalance = bbalance + :delta WHERE bid = :bid; 0.605307 INSERT INTO pgbench_history (tid, bid, aid, delta, mtime) VALUES (:tid, :bid, :aid, :delta, CURRENT_TIMESTAMP); 2.406303 END; `&lt;/pre&gt; 3.4 模拟100个客户端,使用扩张调用接口 #pgbench -M extended -c 100 -T 100 -r pgbench &lt;pre&gt;` starting vacuum...end. transaction type: TPC-B (sort of) scaling factor: 1 query mode: extended number of clients: 100 number of threads: 1 duration: 100 s number of transactions actually processed: 29687 tps = 295.368409 (including connections establishing) tps = 295.968232 (excluding connections establishing) statement latencies in milliseconds: 0.004401 \\set nbranches 1 * :scale 0.001297 \\set ntellers 10 * :scale 0.000929 \\set naccounts 100000 * :scale 0.001026 \\setrandom aid 1 :naccounts 0.000715 \\setrandom bid 1 :nbranches 0.000711 \\setrandom tid 1 :ntellers 0.000681 \\setrandom delta -5000 5000 0.955081 BEGIN; 2.016270 UPDATE pgbench_accounts SET abalance = abalance + :delta WHERE aid = :aid; 1.409169 SELECT abalance FROM pgbench_accounts WHERE aid = :aid; 300.175762 UPDATE pgbench_tellers SET tbalance = tbalance + :delta WHERE tid = :tid; 29.409881 UPDATE pgbench_branches SET bbalance = bbalance + :delta WHERE bid = :bid; 0.749236 INSERT INTO pgbench_history (tid, bid, aid, delta, mtime) VALUES (:tid, :bid, :aid, :delta, CURRENT_TIMESTAMP); 2.665060 END; `&lt;/pre&gt; 3.5 模拟100个客户端,使用绑定变量调用接口 #pgbench -M prepared -c 100 -T 100 -r pgbench &lt;pre&gt;` starting vacuum...end. transaction type: TPC-B (sort of) scaling factor: 1 query mode: prepared number of clients: 100 number of threads: 1 duration: 100 s number of transactions actually processed: 35649 tps = 354.816112 (including connections establishing) tps = 355.568016 (excluding connections establishing) statement latencies in milliseconds: 0.003984 \\set nbranches 1 * :scale 0.001321 \\set ntellers 10 * :scale 0.000875 \\set naccounts 100000 * :scale 0.001053 \\setrandom aid 1 :naccounts 0.000693 \\setrandom bid 1 :nbranches 0.000697 \\setrandom tid 1 :ntellers 0.000728 \\setrandom delta -5000 5000 0.849722 BEGIN; 1.347952 UPDATE pgbench_accounts SET abalance = abalance + :delta WHERE aid = :aid; 0.922049 SELECT abalance FROM pgbench_accounts WHERE aid = :aid; 250.321603 UPDATE pgbench_tellers SET tbalance = tbalance + :delta WHERE tid = :tid; 24.354052 UPDATE pgbench_branches SET bbalance = bbalance + :delta WHERE bid = :bid; 0.465177 INSERT INTO pgbench_history (tid, bid, aid, delta, mtime) VALUES (:tid, :bid, :aid, :delta, CURRENT_TIMESTAMP); 2.328246 END; 我们主要关心的是最后的输出报告中的TPS值，里面有两个,一个是包含连接开销(including),另一个是不包含连接开销的(excluding),这个值是反映的每秒处理的事务数，这里也列出了每个事务所消耗的平均时间。一般认为能将硬件用到极致，速度越快越好。参考：http://www.postgresql.org/docs/9.3/static/pgbench.html","tags":[{"name":"postgresql","slug":"postgresql","permalink":"http://blog.yaodataking.com/tags/postgresql/"},{"name":"pgbench","slug":"pgbench","permalink":"http://blog.yaodataking.com/tags/pgbench/"}]},{"title":"2015 阅读书单","date":"2016-02-06T14:47:58.000Z","path":"2016/02/06/reading-2015/","text":"《局外人》 阿尔贝·加缪主人公是一个非常冷静而又固执的人，他的超乎寻常的淡然处事态度处处显示与这个世界的格格不合。在这种状况下，一个偶然的或者说必然的枪杀事件导致道德和司法制度对他的审判是不公平的。直到今天，我们仍然可以看到这种现象。但是在我们为主人公惋惜或者谴责的时候，他却为他的这个结局感到幸福…… 《许倬云说历史：大国霸业的兴废》 许倬云虽是谈话录，确实真知灼见，对中国古代历史，近代历史娓娓道来，对罗马帝国的衰亡一语中的。展望未来，如果我们不从过去吸取经验，发展一些知所约束、知所节制的智慧，如果我们也没有领悟一些不要强求、不要执著的智慧，也许人类在世界上就难以存留更长的时间。 《宋史十二讲》 张全明 李文涛选取斧声烛影与宋初政治，武将与宋代社会，包拯与吏治——宋代的贪污与廉政，江南财富的聚集——宋代经济重心的南移等12个能够代表宋代特色的大问题，进 行细致入微、通俗深入的阐释，以使读者明了宋代在中国历史发展序列中的独特地位，改变认为中国历史只是王朝的循环更替、皇帝家史的变化的习惯看法。 《三国机密》马伯庸三国作为经典中的经典，一直被中国乃至世界读者奉若瑰宝，古今中外诸多专家学者投身到三国历史的研究中去，各类学术文献、艺术作品层出不穷，而马伯庸把自己的《三国机密》定位为“历史可能性小说”，“历史上的事件都是固态的，解读却有很多种途径，在不改变历史事件的前提之下，给予不同的解释与逻辑关系，用一些可能性来填充其间，就是这类小说的核心理念”是他对这一类型做出的解释。《三国机密》便是在尊重三国基本史实的基础上，对正史上未曾提及的事件进行了合理的演义和想象，添加了作者无穷智慧而写就的小说。“在真实历史的缝隙闪转腾挪”应该是对本书最好的评价。 《简单之美：软件开发实践者的思考》 倪健在IT技术领域，中国缺少技术大师，但中国不缺少热爱技术、热爱思考的人，本书的作者便是其中的一个典型代表。他用自己的思考和经验带领读者朋友一起去探讨软件开发的本质——大道至简。如果你也是一位热爱技术和思考的人，本书决不容错过。 《死于技术：索尼衰亡启示》 立石泰则为何以技术著称的索尼在互联网时代逐渐走向衰落？ 为什么索尼不同于苹果，未能不断推出让用户尖叫的产品？ 独霸主要利润来源的PC业务，如今为何遭受贱卖反成企业转型的障碍？ 从创业者盛田昭夫、井深大，到守业者出井、平井，究竟是谁在战略上下了错误的一招棋？ 索尼这个曾经的伟大公司，是否还能重整旗鼓，走出困境？ 互联网时代的到来，不仅改变了众多科技互联网巨头的竞争方向，也对企业管理提出了巨大挑战。索尼的企业战略没有跟上时代发展的步伐，其领导者遗失了洞悉未来的长远眼光。索尼在鼎盛时期曾经塑造的神话破灭了，如今正艰难且被动地面对全球信息交互时代的挑战和考验。 《饥饿的盛世》 张宏杰这是一本探讨乾隆执政得失的通俗历史读物，其震撼力不亚于《万历十五年》。为什么中国历史上的朝代就从来摆脱不了一治一乱，一盛一衰的发展规律。原因之一在于人治统治下的局限性，一个帝国统治绩效如何很大程度上依赖于帝国统治者的个人作为，他们既是辉煌成绩的创造者，也是王朝衰弱的追魁祸首。原因之二在于专制制度的封闭僵化，几千年来，中国专制制度的框架和运转规则却没有根本突破和进步。当这种僵化的体制适应于朝代时，如果又遇上了英明的帝国统治者，盛世可期；当如果不适应于朝代发展时，即使人治有为，帝国衰亡同样无法避免，这就导致了中国历史的一种封闭自我线性循环。 《春秋大义:中国传统语境下的皇权与学术》 熊逸春秋的小国民主是如何一步步走向战国 、 秦 、 汉的大国寡头乃至大国专制的 ；《春秋》 及其相关的经典是如何在秦 、 汉以来的两千年专制社会里发挥实际作用的 ； 我们现代人所认为的这些 “ 迂腐的书斋学问 ” 在当初是如何地在政治和社会生活中呼风唤雨的；一贯被当作中国精神的“春秋大义”到底是一种怎样的东西 ；道家的思想是否真的是我们一般认为的 “ 清静无为 ” ；一统天下的儒学究竟在怎样培养出忠臣孝子 ；我们对那段历史有着多少的误解和想当然。权力决定思想，有什么样的政治需要，就有什么样的思想来附和权力。儒家、道家、法家等学说经过历代统治者的改造，早已面目全非，与原来的教义相去甚远，成为统治者愚民的工具。各个学派的很多概念虽然延续下来，早已是旧瓶装新酒。对照现实，不少观点可以当作现实的镜像。如何看待中国的传统文化？熊逸则用广博的知识视野告诉我们，传统文化是中国走向现代化的最大阻碍之一。国学甚嚣尘上的今天，这本书提醒我们不要掉入祖先的陷阱。 《世界之战：科学与灵性如何确定未来》 伦纳德·蒙洛迪诺 迪帕克·乔普拉科学让人类走上解开自然之谜的道路，让人类懂得了如何利用自然资源，如何开发新的技术，如何利用推理和观察而不是偏见来揭示事物的本质；而灵性则让人们走上了内心超然境界的自我探索之旅。科学认为世界是实实在在的，是人类智力可以探索的；而灵性则认为宇宙是一种智慧设计，处处充满能量。按照迪帕克的观点，灵性最大的优势是能够回答科学无法解答的问题，特别是意识领域的问题。我们的生活离不开科学，但同时我们也需要认识并接受思想、感觉、情感及其他认知模式的有效性。如果我们能将人类本质所含有的灵性、丰富性及完整性融入科学之中，肯定会大大加速人类的进一步的提升和完善。《世界之战》无疑就是实现这一目标的绝佳方式。《世界之战》延续了《时间简史》和《大设计》的叙述风格，语言通俗，打开了大众的视野，使之能够到达知识和灵性领域的最前沿。 《大目标：我们与这个世界的政治协商》 任冲昊 王巍 周小路 白熊站在21世纪的第二个十年里，回望100年前的时候，世界正在经历一场大变革：第一次世界大战刚刚结束，第二次工业革命的成果正在显现……百年之后的现在，世界仍然在酝酿一场大变革！站在世界变革的风口浪尖，我们必须要有一种力量，一种应变大变局的力量。本书给我们的便是这种力量！书中从政治、经济、军事、人口、历史等诸多方面，为我们呈现了世界各国一路走来的风风雨雨。本书另外分析了中国将来的道路，走工业化输出。 《融合时代》 刘积仁 史蒂夫。佩珀马斯特3年以后，你在哪里，在做什么？这取决于你对当下潮流的判断和抉择。《融合时代》教你把握移动互联社会的9大趋势，抢占无限商机，享受创意生活，人类社会的发展与技术的进步密不可分，技术推动社会的变革，社会的发展需求又不断推动新技术的产生。如今，社会发展和技术变革的速度已经越来越快，并不断颠覆我们的想象。原有的平衡和秩序在不断被打破，我们也因此而进入了一个更加复杂、却也更加精彩的世界。在新的世界里，所有的边界都在被逐渐打破，产生新的聚合，而所有的企业都将不断面临全新的竞争格局和挑战。《融合时代》通过对这九大趋势的分析，可以帮助你以更加明晰的态度面对瞬息万变的世界，帮助个人创造更加美好的生活，帮助企业更好地应对高速变化的世界的挑战。 《罗素姆万能机器人》卡雷尔·恰佩克（Karel Capek）R.U.R是一部发表于1920年的科幻片剧本，共3幕。剧作者是以童话寓言闻名于世的捷克作家卡雷尔·恰佩克（Karel Čapek）。R.U.R是Rossum’s Universal Robots的缩写，意为罗素姆万能机器人。剧本中一位名叫罗素姆的哲学家研制出一种机器人，被资本家大批制造来充当劳动力。这些大批制造的机器人外貌与人类相差无几，可以自行思考，而一场机器人灭绝人类的叛变计划正在进行。人形奴隶、自我意识觉醒、反抗人类、机器人的爱情、人类的灭顶之灾……在这本机器人的创世纪里，几乎包含了后来机器人题材、科幻作品讨论的所有主题。该剧于1921年在布拉格演出，轰动了欧洲。卡雷尔·恰佩克作品中创造了“robot”（机器人）一词。这个词源于捷克语的“robota”，意思是“劳役、苦力”，为书中那些机器劳力命名。从此“robot”一词被欧美国家吸收，成为“机器人”的专有名词。 《中国3D打印的未来》 罗军中国从20世纪90年代初开始涉足3D打印技术，并取得了巨大进展，但与国外同行相比仍存在一定差距。特别是中国3D打印企业普遍存在“小而散”、各自为政的现象，如何发挥整合优势、抱团发展是目前亟需解决的问题。如果能够加强同行合作，抱团发展，形成合力，相信3D打印会成为唯一一项中国有可能赶超世界先进水平的技术。 《免费：难以抗拒的力量》 克里斯•安德森免费已经成为了一种商业策略，而最终可能变成任何企业生存的要件。 《智能时代》霍金斯 布拉克斯莉新技术催生新机会，新机会成就新的王者！智能时代大踏步来临，你准备好了吗！ 《众包 2：群体创造的力量》 凡卡·雷马斯瓦米 弗朗西斯·古雅尔在这个网络信息时代，没有企业敢于忽视众包和群体创造的存在！ 《众筹：传统融资模式颠覆与创新》 盛佳 柯斌 杨倩回眸众筹历史，描述众筹的当下图景，理性分析众筹模式的革命性，勾勒出在社交网站上玩转众筹的模式，并深入解读中美众筹业不同的发展机遇与监管规则，解密推动众筹成为主流筹资方式的动力所在。 《帝国的兴衰》 段鸣镝中央电视台历时六年，首次运用最新的经济学理论和考古学发现，破解秦汉帝国兴衰之迷，探索以小农经济为主体的中华帝国固有的又无法解决的悖论，诠释中国两千年王朝更替历史周期律。兴亡谁人定？盛衰岂无凭？透过历史的重重暮霭，展现在我们面前的是亘古不变的历史规律，它也是经济规律和政治规律，它像一条斩之不去的绳索，始终在左右着中国两千年的帝国命运！历史是一个小女孩，谁都可以打扮。统治阶层为了更好的进行统治，往往按照便于统治的角度书写历史，使历史成为了统治者的当代史，随着政权的分崩离析，历史又被新赋予新的含义，在王朝的更迭过程中，史书也被修来该去，但是历史留给人们的普遍哲理将永远存在，源远流长。书中以不同以往的观点对大汉王朝的兴盛衰亡进行分析，土地的兼并，资源不断的被垄断利益集团控制，民不聊生，生存难以为继。揭竿而起，群雄逐鹿，问鼎天下，均田地，抑赋税，兴百年，患不均，两级分化，揭竿再起。历史不断重复，分分合合，从零始，归为零。书中试图找到可以为统治者借鉴的历史法则，人治与法治，终归是人治，我们能够从历史中学到的就是什么也学不到！ 《黄金大崩溃：黄金是否依然是财富的避风港》约尼·雅各布 (Yoni Jacobs)黄金，拥有永恒的魅力，长期以来被人们视为安全与可靠的投资品。听起来就像曾经的房地产市场与.com公司在泡沫被刺破前的样子。当前，黄金的泡沫正在逐渐膨胀。在《黄金大崩溃：黄金是否依然是财富的避风港》一书中作者约尼·雅各布为我们整理出一幅清晰而深刻的图景，向我们揭示了黄金泡沫终将不可避免的崩溃。而本书的观点有助于读者们在泡沫崩溃前防患于未然，甚至从中获利。本书通过对经济史的回顾，以及财务与金融理论的分析，严谨地论证了黄金是如何开始受人关注，并被过度高估的。揭示了人们是如何突入这一轮泡沫的，而这轮泡沫又终将去向何方。 《大数据营销：定位客户》 麦德奇(Dimitri Maex) 保罗B.布朗(Paul B. Brown)你对数据的处理方式远比获取更多的数据来得重要。**营销业的未来属于两类人：技术员与魔法师。**技术员会确保数据数量丰富，时效最新；魔法师则会将数据变为可以获利的点子。两位作者创作了这部价值无可限量的作品，他们认为，每家有责任心的营销公司都会希望了解个中原委。你会发现，其实每个企业都只有两面而已：供应面与需求面。供应面是属于企业可以控制的一面。公司领导层会了解这方面的数据，比如购买一台新机器将提高多少生产力。 而另一方面，需求面是公司无法掌控的，决定权在客户手中。当然，你可以尝试所有方式去接近客户，但最终是由客户或消费者决定是否对你必须提供的产品感兴趣。需求面是人们尚未明确的领域，这方面孰因孰果始终不能确定。客户购买完全符合自己需求的产品是由于喜欢推出的广告，还是产品的价格有吸引力，或者是口碑的原因，甚至这些因素兼而有之，还是可能有上百种因素共同影响？要着手治理一家陷入混乱的企业，请先弄清楚该企业出了什么问题。假设一位顾客点击互联网上的广告后会立即购买，他买产品是不是因为受到一则横幅广告的影响？查明促使顾客购买的原因就是我每天都要做的工作。你会看到，我使用的是来自供应面的明确工具，它们经过了实验，经受了考验。我会将它们用于了解混乱的企业供应面。 这些工具可以给你提供帮助，套用本书副标题的话说就是，用一种能增加销售额和利润的方式发展你的企业。 这样的方式对公司最高层的管理人员至关重要。 《大国的衰落:金融危机启示录》 弗雷德里克·艾伦本书是弗雷德里克·艾伦在西方广为传诵的历史杰作，全书按时间顺序，以细腻的笔触追溯了那个令人心有余悸、百感交集的历史片 段。从1929年的大崩溃到1939年的战争阴影，期间穿插有美国经济、政治、文化、道德、风尚等方面的内容，尤为值得称道的是，作者花费较大笔墨描述了 “胡佛的困境”、 “罗斯福的新政”等内容，以史为鉴，这对于我们理解当下全球金融危机的影响和奥巴马的新政都有着不可小觑的启示作用。可以毫不夸张地说，本书绝对是关于美国大萧条那段历史中最为经典的作品。 《一个帝国的生与死》 夜狼啸西风还原水浒好汉的真实原型和命运，揭开北宋走向灭亡的悲剧真相！历史的迷人之处在于，你往往能够从中看到未来。这本书以水浒中的人物与故事为索引，以点代面地介绍了中华文明历史上最让我又爱又恨的朝代——宋朝。爱是因为宋朝无疑是中华文明的历史上的巅峰，虽然这样的巅峰非常狭义，但是至少，这是一个最多农民能吃饱饭的朝代。恨是因为国家积贫积弱，从一开始的偏安一隅，到最后被外族灭亡，太多唏嘘与无奈，这也许就是一个帝国的生与死吧。历史的兴亡，这里不谈，毕竟这不是一本专业的学术论述书籍，而且从一个一个水浒的故事出发，更带有种杂志专栏的感觉，有趣，可是在逻辑性和系统性上面就比较薄弱。书名也许在某种程度上说，起得太大了，但是从这些小事儿中，的确能够看得出由盛转衰，继而灭亡的轨迹，从这个角度看，说是一个帝国的生与死也未尝不可。总体来说，倘若你喜欢水浒，又想知道背后的故事，那么这本书是你一定不能错过的一本，借用罗辑思维的说辞：有种、有趣、有料。 《必然》 凯文.凯利读书笔记详见关于未来的畅想","tags":[{"name":"阅读书单","slug":"阅读书单","permalink":"http://blog.yaodataking.com/tags/阅读书单/"}]},{"title":"Kubernetes入门实战(1)：基于redis和docker的guestbook留言簿案例","date":"2016-02-04T14:14:38.000Z","path":"2016/02/04/kubernetes-guestbook/","text":"1.Kubernetes介绍1.1 简介Kubernetes是什么？首先，它是一个全新的基于容器技术的分布式架构领先方案。其次，它是一个开放的开发平台。最后，它是一个完备的分布式系统支撑平台。Kubernetes是Google团队发起的开源项目，它的目标是管理跨多个主机的容器，提供基本的部署，维护以及运用伸缩，主要实现语言为Go语言。Kubernetes特点是：•易学：轻量级，简单，容易理解•便携：支持公有云，私有云，混合云，以及多种云平台•可拓展：模块化，可插拔，支持钩子，可任意组合•自修复：自动重调度，自动重启，自动复制。Kubernets目前在https://github.com/kubernetes/kubernetes进行维护。 1.2 基本概念•Node(节点)：在Kubernetes中，节点是实际工作的点，较早版本称为Minion。节点可以是虚拟机或者物理机器，依赖于一个集群环境。每个节点都有一些必要的服务以运行Pod容器组，并且它们都可以通过主节点来管理。在Node上运行的服务进程包括docker daemon，Kubelet和 Kube-Proxy。•Pod(容器组)：是Kubernetes的基本操作单元，把相关的一个或多个容器构成一个Pod，通常Pod里的容器运行相同的应用。Pod包含的容器运行在同一个节点上，看作一个统一管理单元，共享相同的volumes和network namespace/IP和Port空间。•Pod的生命周期：Pod的生命周期是通过Replication Controller来管理的。在整个过程中，Pod处于4种状态之一：Pending, Running, Succeeded, Failed。•Replication Controller(RC)：用于定义Pod副本的数量。确保任何时候Kubernetes集群中有指定数量的Pod副本在运行， 如果少于指定数量的Pod副本，Replication Controller会启动新的Pod，反之会杀死多余的以保证数量不变。•Service(服务)：一个Service可以看作一组提供相同服务的Pod的对外访问接口。•Volume(存储卷)：Volume是Pod中能够被多个容器访问的共享目录。•Label(标签)：用于区分Pod、Service、Replication Controller的key/value键值对，Pod、Service、 Replication Controller可以有多个label，但是每个label的key只能对应一个value。Labels是Service和Replication Controller运行的基础，为了将访问Service的请求转发给后端提供服务的多个容器，正是通过标识容器的labels来选择正确的容器。同样，Replication Controller也使用labels来管理通过pod 模板创建的一组容器，这样Replication Controller可以更加容易，方便地管理多个容器，无论有多少容器。•Proxy(代理)：是为了解决外部网络能够访问跨机器集群中容器提供的应用服务而设计的。Proxy提供TCP/UDP sockets的proxy，每创建一种Service，Proxy主要从etcd获取Services和Endpoints的配置信息，或者也可以从file获取，然后根据配置信息在Minion上启动一个Proxy的进程并监听相应的服务端口，当外部请求发生时，Proxy会根据Load Balancer将请求分发到后端正确的容器处理。•Namespace(命名空间)：通过将系统内部的对象“分配”到不同的Namespace中，形成逻辑上的不同分组，便于在共享使用整个集群的资源同时还能分别管理。•Annotation(注解)：与Label类似，但Label定义的是对象的元数据，而Annotation则是用户任意定义的“附加”信息。 1.3 组件1.3.1 Master运行三个组件：•apiserver：作为kubernetes系统的入口，封装了核心对象的增删改查操作，以RESTFul接口方式提供给外部客户和内部组件调用。它维护的REST对象将持久化到etcd（一个分布式强一致性的key/value存储）。•scheduler：负责集群的资源调度，为新建的Pod分配机器。这部分工作分出来变成一个组件，意味着可以很方便地替换成其他的调度器。•controller-manager：负责执行各种控制器，目前有两类：(1)endpoint-controller：定期关联service和Pod(关联信息由endpoint对象维护)，保证service到Pod的映射总是最新的。(2)replication-controller：定期关联replicationController和Pod，保证replicationController定义的复制数量与实际运行Pod的数量总是一致的。 1.3.2 Worker运行两个组件：•kubelet：负责管控docker容器，如启动/停止、监控运行状态等。它会定期从etcd获取分配到本机的Pod，并根据Pod信息启动或停止相应的容器。同时，它也会接收apiserver的HTTP请求，汇报Pod的运行状态。•proxy：负责为Pod提供代理。它会定期从etcd获取所有的service，并根据service信息创建代理。当某个客户Pod要访问其他Pod时，访问请求会经过本机proxy做转发。 2.Kubernetes安装2.1关闭自带的防火墙服务#systemctl disable firewalld #systemctl stop firewalld 2.2安装安装etcd和Kubernetes软件，会自动安装docker #yum install -y etcd kubernetes 2.3修改docker配置文件#vi /etc/sysconfig/docker OPTIONS=&apos;--selinux-enabled=false --insecure-registry gcr.io&apos; `&lt;/pre&gt; ### 2.4修改apiserver配置文件 #vi /etc/kubernetes/apiserver 删除代码中的ServiceAccount &lt;pre&gt;` KUBE_ADMISSION_CONTROL=&quot;--admission_control=NamespaceLifecycle,NamespaceExists,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota&quot; `&lt;/pre&gt; ### 2.5启动所有服务 #systemctl start etcd #systemctl start docker #systemctl start kube-apiserver #systemctl start kube-controller-manager #systemctl start kube-scheduler #systemctl start kubelet #systemctl start kube-proxy ## 3.Guestbook部署 **•redis-master:** 用于前端Web应用进行“写”留言的操作，其中已经保存了一条内容为“Hello World！”的留言。 **•guestbook-redis-slave:** 用于前端Web应用进行“读”留言的操作，并与redis-master的数据保持同步。 **•guestbook-php-frontend:** PHP Web服务，在网页上展示留言内容，也提供一个文本输入框供访客添加留言。 ### 3.1 下载镜像 #docker pull kubeguide/redis-master #docker pull kubeguide/guestbook-redis-slave #docker pull kubeguide/guestbook-php-frontend [![2016-02-01_22-01-11](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/02/2016-02-01_22-01-11.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/02/2016-02-01_22-01-11.jpg) ### 3.2设置工作目录 #mkdir kube-guestbook #cd kube-guestbook ### 3.3创建redis-master Pod和服务 #vi redis-master-controller.yaml &lt;pre&gt;`apiVersion: v1 kind: ReplicationController metadata: name: redis-master labels: name: redis-master spec: replicas: 1 selector: name: redis-master template: metadata: labels: name: redis-master spec: containers: - name: master image: docker.io/kubeguide/redis-master ports: - containerPort: 6379`&lt;/pre&gt; 创建redis-master Pod #kubectl create -f redis-master-controller.yaml #kubectl get pods 一开始pod在pending状态 [![2016-02-03_19-38-36](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/02/2016-02-03_19-38-36.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/02/2016-02-03_19-38-36.jpg) 第一次启动容器时间比较久，如果没什么问题，状态会更新为Running。 [![2016-02-03_21-18-38](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/02/2016-02-03_21-18-38.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/02/2016-02-03_21-18-38.jpg) #vi redis-master-service.yaml &lt;pre&gt;`apiVersion: v1 kind: Service metadata: name: redis-master labels: name: redis-master spec: ports: # the port that this service should serve on - port: 6379 targetPort: 6379 selector: name: redis-master`&lt;/pre&gt; 创建redis-master服务 #kubectl create -f redis-master-service.yaml #kubectl get services [![2016-02-03_21-21-15](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/02/2016-02-03_21-21-15.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/02/2016-02-03_21-21-15.jpg) ### 3.4创建redis-slave Pod和服务 #vi redis-slave-controller.yaml &lt;pre&gt;`apiVersion: v1 kind: ReplicationController metadata: name: redis-slave labels: name: redis-slave spec: replicas: 2 selector: name: redis-slave template: metadata: labels: name: redis-slave spec: containers: - name: slave image: docker.io/kubeguide/guestbook-redis-slave env: - name: GET_HOSTS_FROM value: env ports: - containerPort: 6379`&lt;/pre&gt; 创建redis-slave Pod #kubectl create -f redis-slave-controller.yaml #kubectl get rc [![2016-02-03_21-23-31](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/02/2016-02-03_21-23-31.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/02/2016-02-03_21-23-31.jpg) #kubectl get pods [![2016-02-03_21-23-50](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/02/2016-02-03_21-23-50.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/02/2016-02-03_21-23-50.jpg) #vi redis-slave-service.yaml &lt;pre&gt;`apiVersion: v1 kind: Service metadata: name: redis-slave labels: name: redis-slave spec: ports: - port: 6379 selector: name: redis-slave`&lt;/pre&gt; 创建redis-slave服务 #kubectl create -f redis-slave-service.yaml #kubectl get services [![2016-02-03_21-25-17](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/02/2016-02-03_21-25-17.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/02/2016-02-03_21-25-17.jpg) ### 3.5创建frontend Pod和服务 #vi frontend-controller.yaml &lt;pre&gt;`apiVersion: v1 kind: ReplicationController metadata: name: frontend labels: name: frontend spec: replicas: 3 selector: name: frontend template: metadata: labels: name: frontend spec: containers: - name: php-redis image: docker.io/kubeguide/guestbook-php-frontend env: - name: GET_HOSTS_FROM value: env ports: - containerPort: 80`&lt;/pre&gt; 创建frontend Pod #kubectl create -f frontend-controller.yaml #kubectl get rc [![2016-02-03_21-27-09](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/02/2016-02-03_21-27-09.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/02/2016-02-03_21-27-09.jpg) #kubectl get pods [![2016-02-03_21-27-18](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/02/2016-02-03_21-27-18.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/02/2016-02-03_21-27-18.jpg) #vi frontend-service.yaml &lt;pre&gt;`apiVersion: v1 kind: Service metadata: name: frontend labels: name: frontend spec: type: NodePort ports: - port: 80 nodePort: 30001 selector: name: frontend 创建frontend服务 #kubectl create -f frontend-service.yaml #kubectl get services 3.6通过浏览器访问网页访问主机30001端口，我们看到网页已经默认有一条Hello World！我们再输入新的留言Hello Kubernetes！然后submit，看到下面多出来一条新的留言。 3.7停止Pod和服务#kubectl stop rc -l “name in (redis-master, redis-slave, frontend)” #kubectl delete service -l “name in (redis-master, redis-slave, frontend)” 参考：Guestbook ExampleKubernetes权威指南","tags":[{"name":"Docker","slug":"Docker","permalink":"http://blog.yaodataking.com/tags/Docker/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://blog.yaodataking.com/tags/Kubernetes/"},{"name":"Redis","slug":"Redis","permalink":"http://blog.yaodataking.com/tags/Redis/"}]},{"title":"Docker 快速搭建私有仓库","date":"2016-01-31T13:19:37.000Z","path":"2016/01/31/docker-registry/","text":"Docker Hub这样的公共仓库虽然给了用户很多便利，但是对大多数用户来说，建立本地私有仓库还是很有必要的。本文以centos 7 为例介绍如何快速建立本地私有仓库。1.准备环境准备centos 7 主机一台（IP:192.168.199.39)。2.安装Docker Engine增加docker 官方yum库 #sudo tee /etc/yum.repos.d/docker.repo &amp;lt;&amp;lt;-&apos;EOF&apos; [dockerrepo] name=Docker Repository baseurl=https://yum.dockerproject.org/repo/main/centos/$releasever/ enabled=1 gpgcheck=1 gpgkey=https://yum.dockerproject.org/gpg EOF `&lt;/pre&gt; #sudo yum install docker-engine 启动docker服务 #sudo service docker start 3.安装私有仓库 好了，现在我们可以下拉docker-registry镜像了，它是官方提供的工具，可以用于构建私有的镜像仓库。 #docker pull registry 建立本地私有仓库地址 #mkdir -pv /opt/data/registry 启动私有仓库容器 &lt;pre&gt;` docker run -d -p 5000:5000 --restart=always --name registry \\ -v /opt/data/registry:/var/lib/registry \\ registry `&lt;/pre&gt; 好了，其实现在已经搭建好私有仓库了，很简单吧。 4.测试本机上传镜像至私有仓库 我们先下拉一个镜像 #docker pull centos:7 打个标签（本机IP+PORT) #docker tag centos:7 127.0.0.1:5000/centos:7 上传 #docker push 127.0.0.1:5000/centos:7 在本机可以用下面命令访问私有仓库 #curl 127.0.0.1:5000/v1/search 也可以从浏览器访问 http://192.168.199.39:5000/v1/search 5.测试其它主机从私有仓库下载镜像 首先修改docker配置文件，重启docker服务 #vi /etc/sysconfig/docker &lt;pre&gt;` OPTIONS=&apos;--selinux-enabled --insecure-registry 192.168.199.39:5000&apos; #systemctl restart docker使用下面命令我们就可以下载镜像了。 #docker pull 192.168.199.39:5000/centos:7 参考：https://docs.docker.com/registry/deploying/https://docs.docker.com/engine/installation/centos/","tags":[{"name":"Docker","slug":"Docker","permalink":"http://blog.yaodataking.com/tags/Docker/"},{"name":"Registry","slug":"Registry","permalink":"http://blog.yaodataking.com/tags/Registry/"}]},{"title":"Docker管理工具Shipyard安装配置","date":"2016-01-22T15:45:16.000Z","path":"2016/01/22/docker-shipyard/","text":"Shipyard是一个基于Web的Docker管理工具，基于Docker Swarm，支持多主机，可以把多个Docker主机上的容器统一管理；可以查看镜像，下拉镜像；可以管理私有镜像仓库；并提供 RESTful API 等。本文将在两台docker主机上安装配置shipyard并分别在不同的Docker上发布两个MySQL实例，MySQL-dev与MySQL-Online。环境准备 centos 7 + docker 1.9.1,准备两台。主节点：docker41，IP:192.168.199.41；从节点：docker42，IP:192.168.199.42。1.Shipyard生态介绍shipyard是由shipyard控制器以及周围生态系统构成，都是以容器封装，以下按照启动顺序进行介绍。1)RethinkDB首先启动的就是RethinkDB容器，shipyard采用RethinkDB作为数据库来保存账户，引擎，服务键值以及元信息等信息。2)Discovery为了使用Swarm的选举机制，我们需要一个外部的密钥值存储容器，shipyard默认采用了etcd。3)shipyard_certs证书管理容器，实现证书验证功能4)Proxy默认情况下，Docker引擎只监听Socket，我们可以重新配置引擎使用TLS或者使用一个代理容器，转发请求从TCP到Docker监听的UNIX Socket。5)Swarm ManagerSwarm管理器6)Swarm AgentSwarm代理，运行在每个节点上。7)Controllershipyard控制器，Remote API的实现和web的实现。2.准备工作在两台主机上分别做几下两点。1)首先清除iptables #iptables -F2)设置daemon参数，重启docker #vi /usr/lib/systemd/system/docker.service ExecStart=/usr/bin/docker daemon -H unix:///var/run/docker.sock `&lt;/pre&gt; [![2016-01-22_19-22-23](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/01/2016-01-22_19-22-23.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/01/2016-01-22_19-22-23.jpg) 3.Shipyard主节点安装（IP:192.168.199.41) 1)下拉镜像 虽然官网提供了一键安装的命令，curl -sSL https://shipyard-project.com/deploy | bash -s , 但我们还是体验一下non-TLS手工安装，也就是说不安装shipyard_certs。所以我们先下拉以下几个镜像。 docker pull rethinkdb docker pull microbox/etcd docker pull shipyard/docker-proxy docker pull swarm:latest docker pull shipyard/shipyard [![2016-01-22_19-18-51](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/01/2016-01-22_19-18-51.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/01/2016-01-22_19-18-51.jpg) 如果在下拉镜像速度上有问题，建议使用国内的镜像代理。 2)启动容器shipyard-rethinkdb &lt;pre&gt;` docker run \\ -ti \\ -d \\ --restart=always \\ --name shipyard-rethinkdb \\ rethinkdb `&lt;/pre&gt; 3)启动容器shipyard-discovery &lt;pre&gt;` docker run \\ -ti \\ -d \\ -p 4001:4001 \\ -p 7001:7001 \\ --restart=always \\ --name shipyard-discovery \\ microbox/etcd -name discovery `&lt;/pre&gt; [![2016-01-22_19-25-11](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/01/2016-01-22_19-25-11.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/01/2016-01-22_19-25-11.jpg) 4)启动shipyard-proxy &lt;pre&gt;` docker run \\ -ti \\ -d \\ -p 2375:2375 \\ --hostname=$HOSTNAME \\ --restart=always \\ --name shipyard-proxy \\ -v /var/run/docker.sock:/var/run/docker.sock \\ -e PORT=2375 \\ shipyard/docker-proxy:latest `&lt;/pre&gt; [![2016-01-22_19-25-34](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/01/2016-01-22_19-25-34.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/01/2016-01-22_19-25-34.jpg) 5)启动shipyard-swarm-manager &lt;pre&gt;` docker run \\ -ti \\ -d \\ --restart=always \\ --name shipyard-swarm-manager \\ swarm:latest \\ manage --replication --addr 192.168.199.41:3375 --host tcp://0.0.0.0:3375 etcd://192.168.199.41:4001 `&lt;/pre&gt; [![2016-01-22_22-46-33](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/01/2016-01-22_22-46-33.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/01/2016-01-22_22-46-33.jpg) 6)启动shipyard-swarm-agent &lt;pre&gt;` docker run \\ -ti \\ -d \\ --restart=always \\ --name shipyard-swarm-agent \\ swarm:latest \\ join --addr 192.168.199.41:2375 etcd://192.168.199.41:4001 `&lt;/pre&gt; [![2016-01-22_19-26-28](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/01/2016-01-22_19-26-28.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/01/2016-01-22_19-26-28.jpg) 7)启动shipyard-controller &lt;pre&gt;` docker run \\ -ti \\ -d \\ --restart=always \\ --name shipyard-controller \\ --link shipyard-rethinkdb:rethinkdb \\ --link shipyard-swarm-manager:swarm \\ -p 8080:8080 \\ shipyard/shipyard:latest \\ server \\ -d tcp://swarm:3375 `&lt;/pre&gt; [![2016-01-22_19-26-40](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/01/2016-01-22_19-26-40.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/01/2016-01-22_19-26-40.jpg) 至此主节点安装完毕，我们可以查看log来检查是否安装成功。 8)Web界面访问 访问http://192.168.199.41:8080，输入默认用户名和密码：admin/shipyard 查看启动的容器 [![2016-01-22_19-27-55](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/01/2016-01-22_19-27-55.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/01/2016-01-22_19-27-55.jpg) 查看镜像 [![2016-01-22_19-28-11](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/01/2016-01-22_19-28-11.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/01/2016-01-22_19-28-11.jpg) 查看节点，此时只有docker41这个节点 [![2016-01-22_19-29-11](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/01/2016-01-22_19-29-11.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/01/2016-01-22_19-29-11.jpg) 4.添加从节点（IP:192.168.199.42) 1)下拉镜像 docker pull shipyard/docker-proxy docker pull swarm:latest 2)启动容器shipyard-proxy &lt;pre&gt;` docker run \\ -ti \\ -d \\ -p 2375:2375 \\ --hostname=$HOSTNAME \\ --restart=always \\ --name shipyard-proxy \\ -v /var/run/docker.sock:/var/run/docker.sock \\ -e PORT=2375 \\ shipyard/docker-proxy:latest `&lt;/pre&gt; 3)启动容器shipyard-swarm-manager &lt;pre&gt;` docker run \\ -ti \\ -d \\ --restart=always \\ --name shipyard-swarm-manager \\ swarm:latest \\ manage --replication --addr 192.168.199.42:3375 --host tcp://0.0.0.0:3375 etcd://192.168.199.41:4001 `&lt;/pre&gt; 4)启动容器shipyard-swarm-agent &lt;pre&gt;` docker run \\ -ti \\ -d \\ --restart=always \\ --name shipyard-swarm-agent \\ swarm:latest \\ join --addr 192.168.199.42:2375 etcd://192.168.199.41:4001 `&lt;/pre&gt; [![2016-01-22_22-49-38](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/01/2016-01-22_22-49-38.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/01/2016-01-22_22-49-38.jpg) 查看容器shipyard-swarm-manager的log，看到已经注册 [![2016-01-22_21-04-51](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/01/2016-01-22_21-04-51.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/01/2016-01-22_21-04-51.jpg) 5）Web界面访问。 还是访问http://192.168.199.41:8080，查看容器，已经多了节点docker42上的容器。 [![2016-01-22_21-06-18](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/01/2016-01-22_21-06-18.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/01/2016-01-22_21-06-18.jpg) 查看节点，多了 docker42 [![2016-01-22_21-05-59](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/01/2016-01-22_21-05-59.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/01/2016-01-22_21-05-59.jpg) 5.给节点设置label 为了分清节点的功能，我们分别给节点设置不同label,Shipyard好像取消了直接维护，我们只能在docker daemon设置。在docker41上修改。 vi /usr/lib/systemd/system/docker.service &lt;pre&gt;` ExecStart=/usr/bin/docker daemon -H unix:///var/run/docker.sock --label docker=dev `&lt;/pre&gt; 在docker42上修改。 &lt;pre&gt;` ExecStart=/usr/bin/docker daemon -H unix:///var/run/docker.sock --label docker=online `&lt;/pre&gt; 分别重启docker，我们再看node节点label已改变。 [![2016-01-22_22-07-02](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/01/2016-01-22_22-07-02.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/01/2016-01-22_22-07-02.jpg) 6.添加Mysql实例 1)添加Mysql-dev, 按图配置参数，端口，路径，容器名。swarm约束，我们选择标签为dev的那台。 [![2016-01-22_22-35-02](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/01/2016-01-22_22-35-02.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/01/2016-01-22_22-35-02.jpg) 查看容器详细信息。 [![2016-01-22_22-38-36](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/01/2016-01-22_22-38-36.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/01/2016-01-22_22-38-36.jpg) 2)添加Mysql-online，按图配置参数，端口，路径，容器名。swarm约束，我们选择标签为online的那台。 [![2016-01-22_22-37-33](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/01/2016-01-22_22-37-33.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/01/2016-01-22_22-37-33.jpg) 查看容器详细信息。 [![2016-01-22_22-39-26](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/01/2016-01-22_22-39-26.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/01/2016-01-22_22-39-26.jpg) 看到新增的两台容器，分别在不同的docker上。 [![2016-01-22_22-40-02](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/01/2016-01-22_22-40-02.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/01/2016-01-22_22-40-02.jpg) 以上，我们看到Shipyard确实可以很方便的统一管理位于不同主机的容器。 PS. 启动从节点的容器shipyard-swarm-manager时，出现ID duplicated错误的解决办法。 &lt;pre&gt;` ERRO[0008] ID duplicated. UGJV:S4FQ:NSHM:GL2I:OBON:UOJF:OZHB:E6FA:DVML:L67M:5ZLT:YM2G shared by 192.168.199.41:2375 and 192.168.199.42:2375 这个错误产生的原因，应该是安装docker后，我们两台主机是复制的，所以key值相同。解决办法是，删除/etc/docker/key.json这个文件，重启docker。 参考：https://docs.docker.com/swarm/multi-manager-setup/https://shipyard-project.com/deployhttps://shipyard-project.com/docs/deploy/manual/","tags":[{"name":"Docker","slug":"Docker","permalink":"http://blog.yaodataking.com/tags/Docker/"},{"name":"Shipyard","slug":"Shipyard","permalink":"http://blog.yaodataking.com/tags/Shipyard/"},{"name":"swarm","slug":"swarm","permalink":"http://blog.yaodataking.com/tags/swarm/"}]},{"title":"基于Mycat的MySQL主从读写分离及自动切换的docker实现(1)","date":"2016-01-17T08:32:21.000Z","path":"2016/01/17/mycat-mysql-docker-sample1/","text":"Mycat是一个彻底开源的新颖的数据库中间件产品。它的出现将彻底结束数据库的瓶颈问题，从而使数据库的高可用，高负载成为可能。本文将用Docker实现MySQL在主从配置（1主1从）情况下的读写分离及自动切换。1.MySQL主从服务器配置1)从Docker官方下拉MySQL的image #docker pull mysql:latest2)设置目录为了使MySql的数据保持在宿主机上，我们先建立几个目录。 #mkdir -pv /mysql/data建立主服务器的配置目录 #mkdir -pv /mysql/101建立从服务器的配置目录 #mkdir -pv /mysql/1023)设置主从服务器配置 #vi /mysql/101/101.cnf [mysqld] log-bin=mysql-bin server-id=101 `&lt;/pre&gt; #vi /mysql/102/102.cnf &lt;pre&gt;`[mysqld] log-bin=mysql-bin server-id=102 `&lt;/pre&gt; 3)创建主从服务器容器 &lt;pre&gt;`#docker create --name mysqlsrv101 -v /mysql/data/mysql101:/var/lib/mysql -v /mysql/101:/etc/mysql/conf.d -e MYSQL_ROOT_PASSWORD=123456 -p 3306:3306 mysql:latest #docker create --name mysqlsrv102 -v /mysql/data/mysql102:/var/lib/mysql -v /mysql/102:/etc/mysql/conf.d -e MYSQL_ROOT_PASSWORD=123456 -p 3316:3306 mysql:latest `&lt;/pre&gt; 启动容器 #docker start mysqlsrv101 #docker start mysqlsrv102 [![2016-01-17_19-07-02](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/01/2016-01-17_19-07-02.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/01/2016-01-17_19-07-02.jpg) 4)登录主服务器的mysql，查询master的状态 &lt;pre&gt;`mysql&amp;gt; show master status; +------------------+----------+--------------+------------------+-------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set | +------------------+----------+--------------+------------------+-------------------+ | mysql-bin.000003 | 154 | | | | +------------------+----------+--------------+------------------+-------------------+ 1 row in set (0.00 sec) `&lt;/pre&gt; 5)登录从服务器的mysql，设置与主服务器相关的配置参数 change master to master_host=&apos;172.17.0.2&apos;,master_user=&apos;root&apos;,master_password=&apos;123456&apos;,master_log_file=&apos;mysql- bin.000003&apos;,master_log_pos=154; start slave; [![2016-01-16_23-08-54](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/01/2016-01-16_23-08-54.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/01/2016-01-16_23-08-54.jpg) 检查从服务器复制功能状态。 &lt;pre&gt;`mysql&amp;gt; show slave status\\G *************************** 1\\. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: 172.17.0.2 Master_User: root Master_Port: 3306 Connect_Retry: 60 Master_Log_File: mysql-bin.000003 Read_Master_Log_Pos: 154 Relay_Log_File: 140ef1385e2b-relay-bin.000002 Relay_Log_Pos: 320 Relay_Master_Log_File: mysql-bin.000003 Slave_IO_Running: Yes Slave_SQL_Running: Yes Replicate_Do_DB: Replicate_Ignore_DB: Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: Replicate_Wild_Ignore_Table: Last_Errno: 0 Last_Error: Skip_Counter: 0 Exec_Master_Log_Pos: 154 Relay_Log_Space: 534 Until_Condition: None Until_Log_File: Until_Log_Pos: 0 Master_SSL_Allowed: No Master_SSL_CA_File: Master_SSL_CA_Path: Master_SSL_Cert: Master_SSL_Cipher: Master_SSL_Key: Seconds_Behind_Master: 0 Master_SSL_Verify_Server_Cert: No Last_IO_Errno: 0 Last_IO_Error: Last_SQL_Errno: 0 Last_SQL_Error: Replicate_Ignore_Server_Ids: Master_Server_Id: 101 Master_UUID: 6c303202-bbfd-11e5-8dd5-0242ac110002 Master_Info_File: /var/lib/mysql/master.info SQL_Delay: 0 SQL_Remaining_Delay: NULL Slave_SQL_Running_State: Slave has read all relay log; waiting for more updates Master_Retry_Count: 86400 Master_Bind: Last_IO_Error_Timestamp: Last_SQL_Error_Timestamp: Master_SSL_Crl: Master_SSL_Crlpath: Retrieved_Gtid_Set: Executed_Gtid_Set: Auto_Position: 0 Replicate_Rewrite_DB: Channel_Name: Master_TLS_Version: 1 row in set (0.00 sec) mysql&amp;gt; `&lt;/pre&gt; 2.MySQL主从服务器测试 进入主服务器(容器mysqlsrv101),创建几个数据库 create database db1; create database db2; create database db3; [![2016-01-16_23-14-36](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/01/2016-01-16_23-14-36.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/01/2016-01-16_23-14-36.jpg) 进入从服务器(容器mysqlsrv102),可查看到刚才主服务器创建的数据库。 [![2016-01-16_23-14-08](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/01/2016-01-16_23-14-08.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/01/2016-01-16_23-14-08.jpg) 至此MySQL主从服务器配置成功。 3.安装MyCat 1)Dockerfile文件 我们使用下面的Dockerfile自己build mycat镜像。 &lt;pre&gt;`from centos:7 MAINTAINER Alex Wu #install java8 ADD jdk-8u51-linux-x64.gz /usr/local RUN ln -s /usr/local/jdk1.8.0_51 /usr/local/java ENV JAVA_HOME /usr/local/java ENV PATH $PATH:$JAVA_HOME/bin #install mycat VOLUME /opt/mycat/conf ADD Mycat-server-1.5-alpha-20160108213035-linux.tar.gz /opt EXPOSE 8066 9066 CMD [&quot;/opt/mycat/bin/mycat&quot;,&quot;console&quot;] `&lt;/pre&gt; 2)启动Mycat容器 复制mycat配置文件至/mysql/mycatconf &lt;pre&gt;`#docker create --name mycat01 -v /mysql/mycatconf:/opt/mycat/conf -p 8066:8066 -p 9066:9066 mycat:1.5 #docker start mycat01 `&lt;/pre&gt; 4.测试读写分离 负载均衡类型，目前的取值有3种： 1\\. balance=&quot;0&quot;, 不开启读写分离机制，所有读操作都发送到当前可用的writeHost上。 2\\. balance=&quot;1&quot;，全部的readHost与stand by writeHost参与select语句的负载均衡，简单的说，当双主双从模式(M1-&amp;gt;S1，M2-&amp;gt;S2，并且M1与 M2互为主备)，正常情况下，M2,S1,S2都参与select语句的负载均衡。 3\\. balance=&quot;2&quot;，所有读操作都随机的在writeHost、readhost上分发。 4\\. balance=&quot;3&quot;，所有读请求随机的分发到wiriterHost对应的readhost执行，writerHost不负担读压力. writeType属性 负载均衡类型，目前的取值有3种： 1\\. writeType=&quot;0&quot;, 所有写操作发送到配置的第一个writeHost，第一个挂了切到还生存的第二个writeHost，重新启动后已切换后的为准，切换记录在配置文件中:dnindex.properties . 2\\. writeType=&quot;1&quot;，所有写操作都随机的发送到配置的writeHost。 3\\. writeType=&quot;2&quot;，没实现。 switchType属性 - 1 表示不自动切换,默认值,心跳语句为select user() - 2 基于MySQL主从同步的状态决定是否切换,心跳语句为show slave status - 3 基于MySQL galary cluster的切换机制,心跳语句为show status like ‘wsrep%’ 按如下修改schema.xml的balance,writeType,switchType的值。 [![2016-01-17_16-18-45](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/01/2016-01-17_16-18-45.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/01/2016-01-17_16-18-45.jpg) 打开log4j.xml的debug选项，重启mycat容器，进入8066端口 mysql -utest -p -h127.0.0.1 -P8066 -DTESTDB 插入三条记录 &lt;pre&gt;`mysql&amp;gt; insert into travelrecord (id,user_id,traveldate,fee,days) values(1,&apos;wang&apos;,&apos;2014-01-05&apos;,510.5,3); Query OK, 1 row affected, 1 warning (0.10 sec) mysql&amp;gt; insert into travelrecord (id,user_id,traveldate,fee,days) values(5000001,&apos;zhang&apos;,&apos;2015-01-05&apos;,1510.5,5); Query OK, 1 row affected, 1 warning (0.01 sec) mysql&amp;gt; insert into travelrecord (id,user_id,traveldate,fee,days) values(10000001,&apos;huang&apos;,&apos;2014-06-05&apos;,720.5,3); Query OK, 1 row affected, 1 warning (0.00 sec) `&lt;/pre&gt; 我们看mycat.log文件，看到数据写入了主服务器。 [![2016-01-17_16-04-27](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/01/2016-01-17_16-04-27.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/01/2016-01-17_16-04-27.jpg) 我们再使用查询语句查询。 &lt;pre&gt;`mysql&amp;gt; select * from travelrecord; +----------+---------+------------+------+------+ | id | user_id | traveldate | fee | days | +----------+---------+------------+------+------+ | 5000001 | zhang | 2015-01-05 | 1511 | 5 | | 1 | wang | 2014-01-05 | 511 | 3 | | 10000001 | huang | 2014-06-05 | 721 | 3 | +----------+---------+------------+------+------+ 3 rows in set (0.18 sec) `&lt;/pre&gt; [![2016-01-17_16-17-42](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/01/2016-01-17_16-17-42.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/01/2016-01-17_16-17-42.jpg) 我们看到数据是从服务器读取的，这证明了读写分离的配置是成功的。 4.测试自动切换 在MySQL主服务器突然宕机时，通过配置，Mycat会自动将连接转到MySQL从服务器上，在从服务器上读写数据。（此时MySQL主从服务器最好配置成主从互备) 按如下修改schema.xml的balance,writeType,switchType的值。 [![2016-01-18_21-21-03](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/01/2016-01-18_21-21-03.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/01/2016-01-18_21-21-03.jpg) 重启mycat容器，进入9066管理端口。 mysql -utest -p -h127.0.0.1 -P9066 ,我们看到心跳检测正常。 [![2016-01-18_20-56-41](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/01/2016-01-18_20-56-41.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/01/2016-01-18_20-56-41.jpg) 此时，我们模拟宕机，停止MySQL主服务器的容器。再从9066端口查看心跳，主服务器出现异常。 [![2016-01-18_21-09-14](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/01/2016-01-18_21-09-14.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/01/2016-01-18_21-09-14.jpg) 进入8066端口。 mysql -utest -p -h127.0.0.1 -P8066 -DTESTDB,我们先清空数据，然后再插入三条记录。 &lt;pre&gt;`mysql&amp;gt; insert into travelrecord (id,user_id,traveldate,fee,days) values(1,&apos;wang&apos;,&apos;2014-01-05&apos;,510.5,3); Query OK, 1 row affected, 1 warning (0.10 sec) mysql&amp;gt; insert into travelrecord (id,user_id,traveldate,fee,days) values(5000001,&apos;zhang&apos;,&apos;2015-01-05&apos;,1510.5,5); Query OK, 1 row affected, 1 warning (0.01 sec) mysql&amp;gt; insert into travelrecord (id,user_id,traveldate,fee,days) values(10000001,&apos;huang&apos;,&apos;2014-06-05&apos;,720.5,3); Query OK, 1 row affected, 1 warning (0.00 sec) `&lt;/pre&gt; 我们看mycat.log文件，看到数据写入了从服务器。 [![2016-01-18_21-08-08](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/01/2016-01-18_21-08-08.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/01/2016-01-18_21-08-08.jpg) 我们再使用查询语句查询。看到查询也是从服务器读取的。 [![2016-01-18_21-11-11](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/01/2016-01-18_21-11-11.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/01/2016-01-18_21-11-11.jpg) 查看dnindex.properties,看到localhost1值为1，也就是从服务器。 &lt;pre&gt;`[root@8872a66b639d conf]# cat dnindex.properties #update #Mon Jan 18 00:57:29 UTC 2016 localhost1=1 [root@8872a66b639d conf]# 这证明了自动切换的配置是成功的。","tags":[{"name":"Docker","slug":"Docker","permalink":"http://blog.yaodataking.com/tags/Docker/"},{"name":"Mycat","slug":"Mycat","permalink":"http://blog.yaodataking.com/tags/Mycat/"},{"name":"Mysql","slug":"Mysql","permalink":"http://blog.yaodataking.com/tags/Mysql/"}]},{"title":"Docker 容器网络机制初探(2)-Open vSwitch","date":"2016-01-16T08:30:52.000Z","path":"2016/01/16/docker-container-network-ovs/","text":"1.Open vSwitch简介Open vSwitch的目标，是做一个具有产品级质量的多层虚拟交换机。通过可编程扩展，可以实现大规模网络的自动化（配置、管理、维护）。它支持现有标准管理接口和协议（比如netFlow，sFlow，SPAN，RSPAN，CLI，LACP，802.1ag等，熟悉物理网络维护的管理员可以毫不费力地通过Open vSwitch转向虚拟网络管理）。相比于Linux bridge，Open vSwitch有以下好处。1).网络隔离2).QoS配置3).流量监控，Netflow，sFlow4).数据包分析，Packet Mirror。本文我们使用Open vSwitch的GRE通道简单实现下图的网络结构，并用tshark或tcpdump等工具分析网络的流向。2.Open vSwitch安装1)环境准备 centos 7 + docker 1.9.1(准备两台)2)安装组件在docker41主机上安装Linux网桥配置命令brctl #yum install bridge-utils安装编译所需组件 #yum install gcc make python-devel openssl-devel kernel-devel graphviz kernel-debug-devel autoconf automake rpm-build libtool3)下载 #wget http://openvswitch.org/releases/openvswitch-2.4.0.tar.gz #mkdir -p /root/rpmbuild/SOURCES #cp openvswitch-2.4.0.tar.gz /root/rpmbuild/SOURCES #tar -xzf openvswitch-2.4.0.tar.gz4)编译 #cd openvswitch-2.4.0 #rpmbuild -bb –without check rhel/openvswitch.spec编译完成，文件存放位置/root/rpmbuild/RPMS/x86_645)安装 #cd /root/rpmbuild/RPMS/x86_64 #yum install openvswitch-2.4.0-1.x86_64.rpm #systemctl start openvswitch.service #systemctl status openvswitch.service6)复制复制安装文件至另一台主机docker42，并启动服务 #scp openvswitch-2.4.0-1.x86_64.rpm root@192.168.199.42:/root/3.配置脚本在主机docker41上, #vi 41-ovs-docker.sh # Base on http://goldmann.pl/blog/2014/01/21/connecting-docker-containers-on-multiple-hosts/ # Edit this variable: the &apos;other&apos; host. REMOTE_IP=192.168.199.42 # Edit this variable: the bridge address on &apos;this&apos; host. BRIDGE_ADDRESS=172.17.41.1/24 ROUTE_ADDRESS=172.17.0.0/16 # Name of the bridge and GRE DOCKER_BRIDGE=docker0 OVS_BRIDGE=br0 GRE_TUNNEL=gre0 # Deactivate the bridge ip link set $DOCKER_BRIDGE down ip link set $OVS_BRIDGE down # Remove the docker0 bridge brctl delbr $DOCKER_BRIDGE # Delete the Open vSwitch bridge ovs-vsctl del-br $OVS_BRIDGE # Add the docker bridge brctl addbr $DOCKER_BRIDGE # Set up the IP for the docker bridge ip a add $BRIDGE_ADDRESS dev $DOCKER_BRIDGE # Add the br0 Open vSwitch bridge ovs-vsctl add-br $OVS_BRIDGE # Create the tunnel to the other host and attach it to the ovs bridge ovs-vsctl add-port $OVS_BRIDGE $GRE_TUNNEL -- set interface $GRE_TUNNEL type=gre options:remote_ip=$REMOTE_IP # Add the ovs bridge to docker bridge brctl addif $DOCKER_BRIDGE $OVS_BRIDGE # Activate the bridge ip link set $DOCKER_BRIDGE up ip link set $OVS_BRIDGE up #add route to docker bridge ip route add $ROUTE_ADDRESS dev $DOCKER_BRIDGE # Restart Docker daemon to use the new DOCKER_BRIDGE systemctl restart docker.service `&lt;/pre&gt; #chmode +x 41-ovs-docker.sh 同理在docker42上 vi 42-ovs-docker.sh &lt;pre&gt;` #Base on http://goldmann.pl/blog/2014/01/21/connecting-docker-containers-on-multiple-hosts/ # Edit this variable: the &apos;other&apos; host. REMOTE_IP=192.168.199.41 # Edit this variable: the bridge address on &apos;this&apos; host. BRIDGE_ADDRESS=172.17.42.1/24 ROUTE_ADDRESS=172.17.0.0/16 # Name of the bridge and GRE DOCKER_BRIDGE=docker0 OVS_BRIDGE=br0 GRE_TUNNEL=gre0 # Deactivate the bridge ip link set $DOCKER_BRIDGE down ip link set $OVS_BRIDGE down # Remove the docker0 bridge brctl delbr $DOCKER_BRIDGE # Delete the Open vSwitch bridge ovs-vsctl del-br $OVS_BRIDGE # Add the docker bridge brctl addbr $DOCKER_BRIDGE # Set up the IP for the docker bridge ip a add $BRIDGE_ADDRESS dev $DOCKER_BRIDGE # Add the br0 Open vSwitch bridge ovs-vsctl add-br $OVS_BRIDGE # Create the tunnel to the other host and attach it to the ovs bridge ovs-vsctl add-port $OVS_BRIDGE $GRE_TUNNEL -- set interface $GRE_TUNNEL type=gre options:remote_ip=$REMOTE_IP # Add the ovs bridge to docker bridge brctl addif $DOCKER_BRIDGE $OVS_BRIDGE # Activate the bridge ip link set $DOCKER_BRIDGE up ip link set $OVS_BRIDGE up #add route to docker bridge ip route add $ROUTE_ADDRESS dev $DOCKER_BRIDGE # Restart Docker daemon to use the new DOCKER_BRIDGE systemctl restart docker.service chmode +x 42-ovs-docker.sh4.测试在docker41主机执行 #./41-ovs-docker.sh在docker42主机执行 #./42-ovs-docker.sh1)docker0互ping在docker41主机ping主机docker42的docker0 #ping 172.17.42.1在docker42主机ping主机docker41的docker0 #ping 172.17.41.12)在主机docker41的一个容器内ping主机docker42的一个容器在主机docker41启动一个容器，获取ip地址172.17.41.2 #docker run -it –rm=true java:latest /bin/bash在主机docker42启动一个容器，获取IP地址172.17.42.2 #docker run -it –rm=true java:latest /bin/bash在容器172.17.41.2内ping172.17.42.2tshark分析 #tshark -i br0 -R ip proto gre #tshark -i eno16777736 ip proto gre在容器172.17.42.2内ping172.17.41.2tshark分析 #tshark -i br0 -R ip proto gre #tshark -i eno16777736 ip proto gre","tags":[{"name":"Container","slug":"Container","permalink":"http://blog.yaodataking.com/tags/Container/"},{"name":"Docker","slug":"Docker","permalink":"http://blog.yaodataking.com/tags/Docker/"},{"name":"open vswitch","slug":"open-vswitch","permalink":"http://blog.yaodataking.com/tags/open-vswitch/"}]},{"title":"postgresql trigger调用C语言函数实例","date":"2016-01-09T02:41:41.000Z","path":"2016/01/09/postgresql-trigger/","text":"postgresql 的一个触发器是一种声明，告诉数据库应该在执行特定的操作的时候执行特定的函数。本文我们将通过一个C语言编写的函数来演示触发器的机制。1.准备工作1)安装数据库，初始化并启动 #/usr/local/pgsql/bin/pg_ctl -D /usr/local/pgsql/data start2)建立测试数据库及表 #/usr/local/pgsql/bin/createdb trigger_testdb #/usr/local/pgsql/bin/psql trigger_testdb CREATE TABLE ttest ( x integer ); `&lt;/pre&gt; [![2016-01-09_9-39-44](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/01/2016-01-09_9-39-44.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/01/2016-01-09_9-39-44.jpg) 2.编写C函数并编译，代码如下 &lt;pre&gt;` #include &quot;postgres.h&quot; #include &quot;executor/spi.h&quot; /* this is what you need to work with SPI */ #include &quot;commands/trigger.h&quot; /* ... triggers ... */ #include &quot;utils/rel.h&quot; /* ... and relations */ #ifdef PG_MODULE_MAGIC PG_MODULE_MAGIC; #endif extern Datum trigf(PG_FUNCTION_ARGS); PG_FUNCTION_INFO_V1(trigf); Datum trigf(PG_FUNCTION_ARGS) { TriggerData *trigdata = (TriggerData *) fcinfo-&amp;gt;context; TupleDesc tupdesc; HeapTuple rettuple; char *when; bool checknull = false; bool isnull; int ret, i; /* make sure it&apos;s called as a trigger at all */ if (!CALLED_AS_TRIGGER(fcinfo)) elog(ERROR, &quot;trigf: not called by trigger manager&quot;); /* tuple to return to executor */ if (TRIGGER_FIRED_BY_UPDATE(trigdata-&amp;gt;tg_event)) rettuple = trigdata-&amp;gt;tg_newtuple; else rettuple = trigdata-&amp;gt;tg_trigtuple; /* check for null values */ if (!TRIGGER_FIRED_BY_DELETE(trigdata-&amp;gt;tg_event) &amp;amp;&amp;amp; TRIGGER_FIRED_BEFORE(trigdata-&amp;gt;tg_event)) checknull = true; if (TRIGGER_FIRED_BEFORE(trigdata-&amp;gt;tg_event)) when = &quot;before&quot;; else when = &quot;after &quot;; tupdesc = trigdata-&amp;gt;tg_relation-&amp;gt;rd_att; /* connect to SPI manager */ if ((ret = SPI_connect()) &amp;lt; 0) elog(ERROR, &quot;trigf (fired %s): SPI_connect returned %d&quot;, when, ret); /* get number of rows in table */ ret = SPI_exec(&quot;SELECT count(*) FROM ttest&quot;, 0); if (ret vals[0], SPI_tuptable-&amp;gt;tupdesc, 1, &amp;amp;isnull)); elog (INFO, &quot;trigf (fired %s): there are %d rows in ttest&quot;, when, i); SPI_finish(); if (checknull) { SPI_getbinval(rettuple, tupdesc, 1, &amp;amp;isnull); if (isnull) rettuple = NULL; } return PointerGetDatum(rettuple); } `&lt;/pre&gt; #gcc -fPIC -I /home/src/pgsql/src/include -shared -o trigger_testc.so trigger_testc.c 把编译好的so文件复制到lib目录下 #cp trigger_testc.so /usr/local/pgsql/lib 3.创建触发器 加载编译好的so文件到数据库 load &apos;trigger_testc&apos; 创建函数 &lt;pre&gt;` CREATE FUNCTION trigf() RETURNS trigger AS &apos;trigger_testc&apos; LANGUAGE C; `&lt;/pre&gt; 创建两个触发器，一个触发在INSERT/UPDATE/DELETE事务之前，一个触发在INSERT/UPDATE/DELETE事务之后。 触发后执行trigf函数，这个函数也是我们刚才编译的C语言函数。 &lt;pre&gt;` CREATE TRIGGER tbefore BEFORE INSERT OR UPDATE OR DELETE ON ttest FOR EACH ROW EXECUTE PROCEDURE trigf(); CREATE TRIGGER tafter AFTER INSERT OR UPDATE OR DELETE ON ttest FOR EACH ROW EXECUTE PROCEDURE trigf(); 4.测试1) 插入空值INSERT INTO ttest VALUES (NULL);2)插入第一条记录INSERT INTO ttest VALUES (1);3)插入第二条记录INSERT INTO ttest SELECT x * 2 FROM ttest;4)更新第二条记录UPDATE ttest SET x = NULL WHERE x = 2;UPDATE ttest SET x = 4 WHERE x = 2;5)删除记录DELETE FROM ttest;参考官网","tags":[{"name":"postgresql","slug":"postgresql","permalink":"http://blog.yaodataking.com/tags/postgresql/"},{"name":"trigger","slug":"trigger","permalink":"http://blog.yaodataking.com/tags/trigger/"}]},{"title":"Docker 容器网络机制初探(1)-Linux路由","date":"2016-01-08T14:22:00.000Z","path":"2016/01/08/docker-container-network/","text":"我们知道Docker利用容器来运行应用的，容器是从镜像创建的运行实例。它可以被启动、开始、停止、删除。每个容器都是相互隔离的、保证安全的平台。但是容器之间如何通信呢？本文我们使用Linux的路由简单实现下图的网络结构，并用tshark或tcpdump等工具分析网络的流向。1.环境准备 centos 7 + docker 1.9.1(准备两台)1)静态IP设置 #vi /etc/sysconfig/network-scripts/ifcfg-eno16777736BOOTPROTO=staticIPADDR=192.168.199.42NETMASK=255.255.255.0GATEWAY=192.168.199.1DNS1=114.114.114.114DNS2=114.114.115.115NM_CONTROLLED=no2)主机名设置 #hostnamectl set-hostname docker42 #hostname #cat /etc/hostname同理设置另外一台主机。2.docker0 IP设置在docker41主机上，查看docker的服务文件位置。 #systemctl status docker修改docker.service文件，加入–bip=172.41.1.1/16 #vi /usr/lib/systemd/system/docker.service [Unit] Description=Docker Application Container Engine Documentation=https://docs.docker.com After=network.target docker.socket Requires=docker.socket [Service] Type=notify ExecStart=/usr/bin/docker daemon --bip=172.41.1.1/16 -H fd:// MountFlags=slave LimitNOFILE=1048576 LimitNPROC=1048576 LimitCORE=infinity [Install] WantedBy=multi-user.target `&lt;/pre&gt; #systemctl daemon-reload #systemctl restart docker 同样修改docker42主机的docker.service文件 2.主机之间的docker0互通 在主机docker41上ping 主机docker42的IP 172.42.1.1，发现不通，原因是没有加入路由。 #route add -net 172.42.0.0/16 gw 192.168.199.42 [![2016-01-08_11-36-02](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/01/2016-01-08_11-36-02.jpg)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2016/01/2016-01-08_11-36-02.jpg) 发现可以ping通了。同样的在docker42主机加入下面路由 #route add -net 172.41.0.0/16 gw 192.168.199.41 如果要永久保存，编辑/etc/sysconfig/network-scripts下route开头文件，如果没有，可以新建。 #vi /etc/sysconfig/network-scripts/route-eno16777736 &lt;pre&gt;`172.42.0.0/16 via 192.168.199.42 3.主机之间容器互通在主机docker41上启动一容器 #docker run –rm=true -it java:latest /bin/baship addr 获得容器ip地址在主机docker42上ping 容器ip地址，发现目标主机禁止在主机docker41清空iptable #iptables -F #iptables -t nat -F这样再ping就通了。4.tshark分析ping网络流向tshark是wireshark的命令行工具，所以在centos可以直接安装 #yun install -y wireshark我们在docker42主机发送一个ping命令到docker41主机的容器172.41.0.1，然后我们在主机docker41和docker42分别执行下列两条语句监控icmp协议。 #tshark -i eno16777736 -f icmp #tshark -i docker0 -f icmp通过上四图我们可知，ping命令发出路径是主机docker42的docker0到en0，然后通过路由找到docker41的en0和docker0，结果返回的方向正好相反，从主机docker41的docker0到en0，然后返回到docker42的en0和docker0。5.tshark分析mysql的数据流向在主机docker41启动一个mysql容器 #docker run –name mysqlcontainer41 -e MYSQL_ROOT_PASSWORD=p123456 -d mysql:latest #docker exec -it mysqlcontainer41 /bin/baship addr查看IP地址在主机docker42启动一个mysql容器 #docker run –name mysqlcontainer42 -e MYSQL_ROOT_PASSWORD=p123456 -d mysql:latest #docker exec -it mysqlcontainer42 /bin/baship addr查看IP地址从主机docker42的容器去连接主机docker41的mysql容器 #docker exec -it mysqlcontainer42 /bin/bash #mysql -uroot -p -h172.41.0.1我们在主机docker41和docker42分别执行下列两条语句，分别监控docker0及en0的3306端口。 #tshark -i eno16777736 -f ‘tcp dst port 3306’ #tshark -i docker0 -f ‘tcp dst port 3306’我们执行一条select语句，select * from mysql.user;看看报文是怎么走向的。从上图我们可以知道，查询命令发出路径是主机docker42的docker0到en0，然后通过路由找到docker41的en0和docker0，结果返回的方向正好相反，从主机docker41的docker0到en0，然后返回到docker42的en0和docker0。同样的我们可以通过下面命令打印mysql查询语句。 #tshark -i eno16777736 -f tcp -R ‘mysql.query’ -T fields -e mysql.query","tags":[{"name":"Container","slug":"Container","permalink":"http://blog.yaodataking.com/tags/Container/"},{"name":"Docker","slug":"Docker","permalink":"http://blog.yaodataking.com/tags/Docker/"},{"name":"容器","slug":"容器","permalink":"http://blog.yaodataking.com/tags/容器/"}]},{"title":"Docker proxy关停测试","date":"2016-01-02T15:56:40.000Z","path":"2016/01/02/docker-proxy/","text":"正常情况下，当启动一个端口映射的容器时，docker –proxy进程就会起来，实现宿主机上0.0.0.0地址上对容器的访问代理，在docker-proxy加入Docker之后相当长的一段时间内。Docker爱好者普遍感受到，很多场景下，docker-proxy并非必需，甚至会带来一些其他的弊端。影响较大的场景主要有两种。第一，单个容器需要和宿主机有多个端口的映射。此场景下，若容器需要映射1000个端口甚至更多，那么宿主机上就会创建1000个甚至更多的 docker-proxy进程。据不完全测试，每一个docker-proxy占用的内存是4-10MB不等。如此一来，直接消耗至少4-10GB内存， 以及至少1000个进程，无论是从系统内存，还是从系统CPU资源来分析，这都会是很大的负担。第二，众多容器同时存在于宿主机的情况，单个容器映射端口极少。这种场景下，关于宿主机资源的消耗并没有如第一种场景下那样暴力，而且一种较为慢性的方式侵噬资源。如今，Docker Daemon引入- -userland-proxy这个flag，将以上场景的控制权完全交给了用户，由用户决定是否开启，也为用户的场景的proxy代理提供了灵活性。那么我们怎么通过- -userland-proxy这个flag来设置呢？不同版本位置稍有不同。本文通过centos 7来演示。首先我们要知道docker的配置文件安装在哪里。 #systemctl status docker(ubuntu 可以用 service docker status查看)可以看到centos 7中docker的服务文件在/usr/lib/systemd/system目录下。停止正在运行的容器，停止docker服务，编辑这个服务文件 #vi /usr/lib/systemd/system/docker.service在ExecStart=/usr/bin/docker daemon -H fd:// 后加上–userland-proxy=false编辑完成，重新启动docker服务先用ps –ef|grep docker 查看docker进程，只有docker daemon，那么docker-proxy会不会还会起来呢？我们启动一个mysql容器，再查看docker进程，发现–userland-proxy=false 这个参数起作用了，docker-proxy没有起来。那么访问数据会不会影响呢？我们用其他主机连上mysql数据库，发现一切正常。有兴趣的同学还可以测试是否访问速度是否加快了。","tags":[{"name":"Docker","slug":"Docker","permalink":"http://blog.yaodataking.com/tags/Docker/"},{"name":"userland-proxy","slug":"userland-proxy","permalink":"http://blog.yaodataking.com/tags/userland-proxy/"}]},{"title":"Docker Volume定义及实例应用","date":"2016-01-02T07:46:11.000Z","path":"2016/01/02/docker-volume/","text":"Volume真正的目的是将容器以及容器产生的数据分离开来。如果你删除容器的时候，Volume的数据不会自动删除，除非删除容器的时候加了-v选项。很多人都对Volume有一个误解，他们认为Volume是为了持久化，但其实容器已经持久化了，容器自创建后会一直存在，除非你删除它们。Volume可以使用以下两种方式创建,这两种方式有些细小而又重要的差别。1)在Dockerfile中指定VOLUME /some/dir2)执行docker run -v /some/dir命令来指定无论哪种方式都是做了同样的事情。它们告诉Docker在主机上创建一个目录（默认情况下是在/var/lib/docker下），然后将其挂载到指定的路径。当删除使用该Volume的容器时，Volume本身不会受到影响，它可以一直存在下去。如果在容器中不存在指定的路径，那么该目录将会被自动创建。下面我们将通过实例来加深理解。这是一个官方mysql镜像的Dockerfile, FROM debian:jessie # add our user and group first to make sure their IDs get assigned consistently, regardless of whatever dependencies get added RUN groupadd -r mysql &amp;amp;&amp;amp; useradd -r -g mysql mysql RUN mkdir /docker-entrypoint-initdb.d # FATAL ERROR: please install the following Perl modules before executing /usr/local/mysql/scripts/mysql_install_db: # File::Basename # File::Copy # Sys::Hostname # Data::Dumper RUN apt-get update &amp;amp;&amp;amp; apt-get install -y perl pwgen --no-install-recommends &amp;amp;&amp;amp; rm -rf /var/lib/apt/lists/* # gpg: key 5072E1F5: public key &quot;MySQL Release Engineering &quot; imported RUN apt-key adv --keyserver ha.pool.sks-keyservers.net --recv-keys A4A9406876FCBD3C456770C88C718D3B5072E1F5 ENV MYSQL_MAJOR 5.7 ENV MYSQL_VERSION 5.7.10-1debian8 RUN echo &quot;deb http://repo.mysql.com/apt/debian/ jessie mysql-${MYSQL_MAJOR}&quot; &amp;gt; /etc/apt/sources.list.d/mysql.list # the &quot;/var/lib/mysql&quot; stuff here is because the mysql-server postinst doesn&apos;t have an explicit way to disable the mysql_install_db codepath besides having a database already &quot;configured&quot; (ie, stuff in /var/lib/mysql/mysql) # also, we set debconf keys to make APT a little quieter RUN { \\ echo mysql-community-server mysql-community-server/data-dir select &apos;&apos;; \\ echo mysql-community-server mysql-community-server/root-pass password &apos;&apos;; \\ echo mysql-community-server mysql-community-server/re-root-pass password &apos;&apos;; \\ echo mysql-community-server mysql-community-server/remove-test-db select false; \\ } | debconf-set-selections \\ &amp;amp;&amp;amp; apt-get update &amp;amp;&amp;amp; apt-get install -y mysql-server=&quot;${MYSQL_VERSION}&quot; &amp;amp;&amp;amp; rm -rf /var/lib/apt/lists/* \\ &amp;amp;&amp;amp; rm -rf /var/lib/mysql &amp;amp;&amp;amp; mkdir -p /var/lib/mysql # comment out a few problematic configuration values # don&apos;t reverse lookup hostnames, they are usually another container RUN sed -Ei &apos;s/^(bind-address|log)/#&amp;amp;/&apos; /etc/mysql/my.cnf \\ &amp;amp;&amp;amp; echo &apos;skip-host-cache\\nskip-name-resolve&apos; | awk &apos;{ print } $1 == &quot;[mysqld]&quot; &amp;amp;&amp;amp; c == 0 { c = 1; system(&quot;cat&quot;) }&apos; /etc/mysql/my.cnf &amp;gt; /tmp/my.cnf \\ &amp;amp;&amp;amp; mv /tmp/my.cnf /etc/mysql/my.cnf VOLUME /var/lib/mysql COPY docker-entrypoint.sh /entrypoint.sh ENTRYPOINT [&quot;/entrypoint.sh&quot;] EXPOSE 3306 CMD [&quot;mysqld&quot;] 我们看到Dockerfile里指定了VOLUME /var/lib/mysql。1.我们通过以下命令启动一个容器docker run –name mysqlcontainer01 -e MYSQL_ROOT_PASSWORD=p123456 -d mysql:latest通过docker inspect mysqlcontainer01可以看到mount了一个目录在主机/var/lib/docker/volumes下我们可以看到这个目录其实就是容器中mysql创建的数据库目录/var/lib/mysql现在我们试图停止并删除容器，再看一下/var/lib/docker/volumes目录下是否还存在。我们看到启动容器时创建的目录还是存在的，而这个容器现在已经删除，通过这个Volume保证了容器中创建的数据还存在。 2.我们再通过以下命令创建一个容器docker run –name mysqlcontainer02 -v /data/mysqldb:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=p123456 -d mysql:latest事先，主机已经有一个mysql数据库目录/data/mysqldb我们看到已经有个数据库test目录及一个testfile文件，我们看容器启动后是否会挂载至容器中。我们看到确实已经有test数据库了。容器中查看/var/lib/mysql也可以看到数据库test目录及一个testfile文件我们再在容器中/var/lib/mysql 下新增一个文件testfile2然后在主机/data/mysqldb看到也有了这就证明通过volume挂载使得主机跟容器数据互通。","tags":[{"name":"Docker","slug":"Docker","permalink":"http://blog.yaodataking.com/tags/Docker/"}]},{"title":"Docker构建镜像实例","date":"2016-01-01T15:58:07.000Z","path":"2016/01/01/docker-myimagefirst/","text":"本文目标是使用centos7为基础镜像，构建一个包括java 8,tomcat7,mysql(mariadb),mycat的镜像，并验证从本机（非宿主机）能访问 8080 ，能用mysql客户端连接mycat的 8066,9066端口。1.首先从仓库下拉centos7镜像，为了加速下拉，我这里选择国内的daoclould作为代理镜像2.Dockfile from centos:7 MAINTAINER Alex Wu RUN yum -y install wget RUN wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo #install java8 #ADD jdk-8u51-linux-x64.gz /usr/local #RUN ln -s /usr/local/jdk1.8.0_51 /usr/local/java #ENV JAVA_HOME /usr/local/java #ENV PATH $PATH:$JAVA_HOME/bin RUN yum -y install java-1.8.0-openjdk #install tomcat ADD apache-tomcat-7.0.67.tar.gz /usr/local RUN ln -s /usr/local/apache-tomcat-7.0.67 /usr/local/tomcat #install mysql #ADD mariadb-5.5.36-linux-x86_64.tar.gz /usr/local #RUN groupadd -r mysql &amp;amp;&amp;amp; useradd -g mysql -r mysql #RUN ln -s /usr/local/mariadb-5.5.36-linux-x86_64 /usr/local/mysql #RUN chown -R mysql:mysql /usr/local/mysql/data #RUN /usr/local/mysql/scripts/mysql_install_db --user=mysql --datadir=/usr/local/mysql/data --basedir=/usr/local/mysql RUN yum -y install mariadb-server mariadb RUN /usr/bin/mysql_install_db --user=mysql --datadir=/var/lib/mysql #install mycat ADD Mycat-server-1.4-release-20151019230038-linux.tar.gz /opt COPY ./schema.xml /opt/mycat/conf ENV MYCAT_USER mycatuser ENV MYCAT_PASS mycatpass RUN sed -i &apos;s/user name=&quot;test&quot;/user name=\\&quot;&apos;&quot;$MYCAT_USER&quot;&apos;&quot;/&apos; /opt/mycat/conf/server.xml &amp;amp;&amp;amp; sed -i &apos;s/name=&quot;password&quot;&amp;gt;test/name=&quot;password&quot;&amp;gt;&apos;&quot;$MYCAT_PASS&quot;&apos;/&apos; /opt/mycat/conf/server.xml EXPOSE 8080 8066 9066 COPY ./start.sh . ENTRYPOINT ./start.sh `&lt;/pre&gt; ************************************************************************ 3.start脚本 ************************************************************************ &lt;pre&gt;` #!/bin/bash #/usr/local/mysql/bin/mysqld_safe --datadir=&apos;/usr/local/mysql/data&apos; &amp;amp; /usr/bin/mysqld_safe --datadir=/var/lib/mysql &amp;amp; /opt/mycat/bin/mycat start &amp;amp; /usr/local/tomcat/bin/catalina.sh run 4.build镜像 #docker build -t myimage/first .build 完毕，我们可以看到image #docker images5.启动容器docker create –name mycontainer01 -p 8080:8080 -p 8066:8066 -p 9066:9066 myimage/firstdocker start mycontainer01docker exec -it mycontainer01 /bin/bash我们可以看到容器已经起来。6.验证1) 访问8080端口2)其他主机mysql访问8066，9066端口mysql -umycatuser -pmycatpass -h192.168.199.111 –P8066mysql -umycatuser -pmycatpass -h192.168.199.111 –P9066","tags":[{"name":"Docker","slug":"Docker","permalink":"http://blog.yaodataking.com/tags/Docker/"}]},{"title":"postgresql使用UUID功能","date":"2016-01-01T08:51:06.000Z","path":"2016/01/01/postgresql-uuid/","text":"1.UUID介绍1.1什么是UUID?UUID是Universally Unique Identifier的缩写，它是在一定的范围内（从特定的名字空间到全球）唯一的机器生成的标识符。UUID具有以下涵义：1.1.1经由一定的算法机器生成为了保证UUID的唯一性，规范定义了包括网卡MAC地址、时间戳、名字空间（Namespace）、随机或伪随机数、时序等元素，以及从这些元素生成UUID的算法。UUID的复杂特性在保证了其唯一性的同时，意味着只能由计算机生成。1.1.2非人工指定，非人工识别UUID是不能人工指定的，除非你冒着UUID重复的风险。UUID的复杂性决定了“一般人“不能直接从一个UUID知道哪个对象和它关联。1.1.3在特定的范围内重复的可能性极小UUID的生成规范定义的算法主要目的就是要保证其唯一性。但这个唯一性是有限的，只在特定的范围内才能得到保证，这和UUID的类型有关,UUID是16字节128位长的数字，通常以36字节的字符串表示，示例如下：3F2504E0-4F89-11D3-9A0C-0305E82C3301其中的字母是16进制表示，大小写无关。GUID（Globally Unique Identifier）是UUID的别名；但在实际应用中，GUID通常是指微软实现的UUID。 1.2UUID的版本UUID具有多个版本，每个版本的算法不同，应用范围也不同。首先是一个特例－－Nil UUID－－通常我们不会用到它，它是由全为0的数字组成，如下：00000000-0000-0000-0000-0000000000001.2.1UUID版本1：基于时间的UUID基于时间的UUID通过计算当前时间戳、随机数和机器MAC地址得到。由于在算法中使用了MAC地址，这个版本的UUID可以保证在全球范围的唯一性。但与此同时，使用MAC地址会带来安全性问题，这就是这个版本UUID受到批评的地方。如果应用只是在局域网中使用，也可以使用退化的算法，以IP地址来代替MAC地址－－Java的UUID往往是这样实现的（当然也考虑了获取MAC的难度）。1.2.2UUID版本2：DCE安全的UUIDDCE（Distributed Computing Environment）安全的UUID和基于时间的UUID算法相同，但会把时间戳的前4位置换为POSIX的UID或GID。这个版本的UUID在实际中较少用到。1.2.3UUID版本3：基于名字的UUID（MD5）基于名字的UUID通过计算名字和名字空间的MD5散列值得到。这个版本的UUID保证了：相同名字空间中不同名字生成的UUID的唯一性；不同名字空间中的UUID的唯一性；相同名字空间中相同名字的UUID重复生成是相同的。1.2.4UUID版本4：随机UUID根据随机数，或者伪随机数生成UUID。这种UUID产生重复的概率是可以计算出来的，但随机的东西就像是买彩票：你指望它发财是不可能的，但狗屎运通常会在不经意中到来。1.2.5UUID版本5：基于名字的UUID（SHA1）和版本3的UUID算法类似，只是散列值计算使用SHA1（Secure Hash Algorithm 1）算法。 2.Postgresql的UUID实现默认安装的Postgresql是不带UUID函数的，但是我们可以用源代码编译安装它。这里我们假定postgresql原来已经安装，我们要增加UUID功能，如果是全新安装postgresql，安装时加上相应的选项即可。 2.1安装UUID库#yum install uuid-devel uuid #rpm -ql uuid我们可以知道UUID库安装在/usr/lib64,在后面安装postgresql动态库uuid-ossp.so时会用到 2.2编译postgresql的uuid动态库假定postgresql原来安装在/usr/local/pgsql目录。进入postgres源代码根目录(如何下载postgres源代码,可参见我的另一篇博文Postgresql搭建eclipse开发环境) #./configure –prefix=/usr/local/pgsql –with-ossp-uuid –with-libraries=/usr/lib64进入uuid-ossp编译 #cd contrib/uuid-ossp/ #make #make install查看动态库是否安装成功至/usr/local/pgsql #ll /usr/local/pgsql/lib/uuid* #ll /usr/local/pgsql/share/extension/ #chown postgres:postgres -R /usr/local/pgsql 2.3安装UUID函数启动postgres #pg_ctl -D /usr/local/pgsql/data -l /usr/local/pgsql/data/log start #psql使用下面sql语句安装UUID函数create extension “uuid-ossp”;select extname,extowner,extnamespace,extrelocatable,extversion from pg_extension;至此UUID函数成功安装。 3.测试我们分别测试uuid的不同版本select uuid_generate_v1();select uuid_generate_v1mc();select uuid_generate_v3(uuid_ns_url(), ‘http://blog.yaodataking.com‘);select uuid_generate_v4();select uuid_generate_v5(uuid_ns_url(), ‘http://blog.yaodataking.com‘);关于postgresql uuid的更多信息，我们可以参见官网说明http://www.postgresql.org/docs/current/static/uuid-ossp.html","tags":[{"name":"postgresql","slug":"postgresql","permalink":"http://blog.yaodataking.com/tags/postgresql/"},{"name":"uuid","slug":"uuid","permalink":"http://blog.yaodataking.com/tags/uuid/"}]},{"title":"Postgres-XL集群安装与配置","date":"2015-12-20T11:16:13.000Z","path":"2015/12/20/postgres-xl/","text":"1.Postgres-XL简介Postgres-XL是从Postgres-XC衍生而来的一款产品, 经过改良, 对MPP这块做了比较大的改进.所以它同样包含了以下组件。gtm：利用pg的MVCC全局地控制tuple，一个pgxc中只能有一个gtm。gtm_standby：gtm的备机。gtm_proxy：降低gtm压力，用多个代理来进行对coordinator进行操作。coordinator：协调节点，负责操作所有datanode，本身不存放数据。在coordinator上可以以distribute切片(分布)或者replication复制的方式进行创建表。datanode：存放数据的节点，如果数据在coordinator上是以切片方式建的表则数据只存放此表的一部分数据，如果是replication方式的表则存放全部数据。我们来看一下Postgres-XL和postgresql, Postgres-XC的对比 : PostgreSQL Postgres-XC Postgres-XL Open source SQL database for Enterprises Open source SQL database for Enterprises Open source SQL database for Enterprises - Large coverage of PostgreSQL support Large coverage of PostgreSQL support - Global MVCC Consistency Global MVCC Consistency - - MPP query support - - Performance improvements - - Multi-tenant security关于Postgres-XC，参见另一篇博文Postgres-XC集群安装与配置 2.安装环境准备一台Centos 6.5 GNOME 2.28.2从官网下载源码：http://sourceforge.net/projects/postgres-xl/官方文档:http://files.postgres-xl.org/documentation/index.html安装配置如下图： IP 角色 端口 Nodename 安装目录 192.168.199.151 GTM 6666 gtm /pgxl_data/gtm 192.168.199.151 Coordinator 1921 coord1 /pgxl_data/coordinator/cd1 192.168.199.151 Coordinator 1925 coord2 /pgxl_data/coordinator/cd2 192.168.199.151 Datanode 15431 db1 /pgxl_data/datanode/dn1 192.168.199.151 Datanode 15432 db2 /pgxl_data/datanode/dn2有条件的可以把这个角色分别安装在不同主机上。 3.安装配置1）创建用户 #groupadd pgxl #useradd pgxl -g pgxl #passwd pgxl 2)安装安装依赖库 #yum install -y flex bison readline-devel zlib-devel openjade docbook-style-dsssl #tar -xzvf postgres-xl-v9.2-src.tar.gz #cd postgres-xl #./configure –prefix=/usr/local/pgsql_xl #make #make install3）创建存放路径 #mkdir -p /pgxl_data/gtm #mkdir -p /pgxl_data/coordinator/cd1 #mkdir -p /pgxl_data/coordinator/cd2 #mkdir -p /pgxl_data/datanode/dn1 #mkdir -p /pgxl_data/datanode/dn2 #chown -R pgxl:pgxl /pgxl_data 4）配置环境变量 #su - pgxl #vi .bash_profileexport PGHOME=/usr/local/pgsql_xlexport LANG=en_US.utf8export LD_LIBRARY_PATH=$PGHOME/lib:/lib64:/usr/lib64:/usr/local/lib64:/lib:/usr/lib:/usr/local/libexport PATH=$PGHOME/bin:$PATH:.export MANPATH=$PGHOME/share/man:$MANPATH #source .bash_profile5）初始化节点 #initgtm -Z gtm -D /pgxl_data/gtm #initdb -D /pgxl_data/coordinator/cd1 –nodename coord1 #initdb -D /pgxl_data/coordinator/cd2 –nodename coord2 #initdb -D /pgxl_data/datanode/dn1 –nodename db1 #initdb -D /pgxl_data/datanode/dn2 –nodename db26)配置节点配置gtm节点vi /pgxl_data/gtm/gtm.confnodename = ‘gtm’listen_addresses = ‘*’port = 6666startup = ACT 配置 coordinator 节点vi /pgxl_data/coordinator/cd1/postgresql.conf #————————————listen_addresses = ‘*’port = 1921max_connections = 100 DATA NODES AND CONNECTION POOLING#———————————————-pooler_port = 6661max_pool_size = 100 GTM CONNECTION#—————————–gtm_host = ‘192.168.199.151’gtm_port = 6666pgxc_node_name = ‘coord1’ vi /pgxl_data/coordinator/cd1/pg_hba.conf IPv4 local connections:host all all 127.0.0.1/32 trusthost all all 192.168.199.151/32 trusthost all all 0.0.0.0/0 md5 vi /pgxl_data/coordinator/cd2/postgresql.conf #————————————listen_addresses = ‘*’port = 1925max_connections = 100 DATA NODES AND CONNECTION POOLING#———————————————-pooler_port = 6662max_pool_size = 100 GTM CONNECTION#—————————–gtm_host = ‘192.168.199.151’gtm_port = 6666pgxc_node_name = ‘coord2’ vi /pgxl_data/coordinator/cd2/pg_hba.conf IPv4 local connections:host all all 127.0.0.1/32 trusthost all all 192.168.199.151/32 trusthost all all 0.0.0.0/0 md5 配置 datanode 节点vi /pgxl_data/datanode/dn1/postgresql.conf #————————————listen_addresses = ‘*’port = 15431max_connections = 100 DATA NODES AND CONNECTION POOLING#———————————————-pooler_port = 6667 #min_pool_size = 1max_pool_size = 100 GTM CONNECTION#—————————–gtm_host = ‘192.168.199.151’gtm_port = 6666pgxc_node_name = ‘db1’ vi /pgxl_data/datanode/dn1/pg_hba.conf IPv4 local connections:host all all 127.0.0.1/32 trusthost all all 192.168.199.151/32 trusthost all all 0.0.0.0/0 md5 vi /pgxl_data/datanode/dn2/postgresql.conf #————————————listen_addresses = ‘*’port = 15432max_connections = 100 DATA NODES AND CONNECTION POOLING#———————————————-pooler_port = 6668 #min_pool_size = 1max_pool_size = 100 GTM CONNECTION#—————————–gtm_host = ‘192.168.199.151’gtm_port = 6666pgxc_node_name = ‘db2’ vi /pgxl_data/datanode/dn2/pg_hba.conf IPv4 local connections:host all all 127.0.0.1/32 trusthost all all 192.168.199.151/32 trusthost all all 0.0.0.0/0 md5 7）启动节点启动顺序gtm + (gtmstandby) + (gtmproxy) + datanode + coordinator启动gtm #gtm -D /pgxl_data/gtm &amp;启动数据节点 #postgres –datanode -D /pgxl_data/datanode/dn1 &amp; #postgres –datanode -D /pgxl_data/datanode/dn2 &amp;启动coordinator节点 #postgres –coordinator -D /pgxl_data/coordinator/cd1 &amp; #postgres –coordinator -D /pgxl_data/coordinator/cd2 &amp;8）注册节点在coord1上注册： #psql -p 1921 postgresselect * from pgxc_node;create node coord2 with(TYPE=coordinator,HOST=’192.168.199.151’,PORT=1925);create node db1 with(TYPE=datanode,HOST=’192.168.199.151’,PORT=15431,primary);create node db2 with(TYPE=datanode,HOST=’192.168.199.151’,PORT=15432);alter node coord1 with(TYPE=’coordinator’,HOST=’192.168.199.151’,PORT=1921);select pgxc_pool_reload();在coord2上注册： #psql -p 1925 postgresselect * from pgxc_node;create node coord1 with(TYPE=coordinator,HOST=’192.168.199.151’,PORT=1921);create node db1 with(TYPE=datanode,HOST=’192.168.199.151’,PORT=15431,primary);create node db2 with(TYPE=datanode,HOST=’192.168.199.151’,PORT=15432);alter node coord2 with(TYPE=’coordinator’,HOST=’192.168.199.151’,PORT=1925);select pgxc_pool_reload();在db1上注册： #psql -p 15431 postgresselect * from pgxc_node;create node coord1 with(TYPE=coordinator,HOST=’192.168.199.151’,PORT=1921);create node coord2 with(TYPE=coordinator,HOST=’192.168.199.151’,PORT=1925);create node db2 with(TYPE=datanode,HOST=’192.168.199.151’,PORT=15432);alter node db1 with(TYPE=’datanode’,HOST=’192.168.199.151’,PORT=15431,primary=true);select pgxc_pool_reload();在db2上注册： #psql -p 15432 postgresselect * from pgxc_node;create node coord1 with(TYPE=coordinator,HOST=’192.168.199.151’,PORT=1921);create node coord2 with(TYPE=coordinator,HOST=’192.168.199.151’,PORT=1925);create node db1 with(TYPE=datanode,HOST=’192.168.199.151’,PORT=15431,primary);alter node db2 with(TYPE=datanode,HOST=’192.168.199.151’,PORT=15432,primary=false);select pgxc_pool_reload();所有节点都应该如下所示9）停止节点停止顺序coordinator+datanode+(gtmproxy)+(gtmstandby)+gtm #pg_ctl stop -D /pgxl_data/coordinator/cd1 -Z coordinator -m fast #pg_ctl stop -D /pgxl_data/coordinator/cd2 -Z coordinator -m fast #pg_ctl stop -D /pgxl_data/datanode/dn1 -Z datanode -m fast #pg_ctl stop -D /pgxl_data/datanode/dn2 -Z datanode -m fast #gtm_ctl stop -D /pgxl_data/gtm -Z gtm -m fast 4.验证测试在coord1上创建一个数据库，并建立一个新表 #psql -p 1921 postgrescreate database pgxl_test;\\c pgxc_test;create table test_xl (id integer,name varchar(32));insert into test_xl select generate_series(1,100),’pgxl_test’;在coord2上查询，看是否能够查询到在coord1上新建的数据库和表在db1上查看在db2上查看这说明我们在coordinator上是以distribute切片方式建的表，数据分别放在datanode上。","tags":[{"name":"postgresql","slug":"postgresql","permalink":"http://blog.yaodataking.com/tags/postgresql/"}]},{"title":"Postgres-XC集群安装与配置","date":"2015-12-20T05:54:32.000Z","path":"2015/12/20/postgres-xc/","text":"1.Postgres-XC简介PostgreSQL-XC 是一种提供写可靠性，多主节点数据同步，数据传输的开源集群方案，它包括很多组件，这些 PostgreSQL-XC 组件可以分别安装在多台物理机器或者虚拟机上。组件：gtm：利用pg的MVCC全局地控制tuple，一个pgxc中只能有一个gtm。gtm_standby：gtm的备机。gtm_proxy：降低gtm压力，用多个代理来进行对coordinator进行操作。coordinator：协调节点，负责操作所有datanode，本身不存放数据。在coordinator上可以以distribute切片(分布)或者replication复制的方式进行创建表。datanode：存放数据的节点，如果数据在coordinator上是以切片方式建的表则数据只存放此表的一部分数据，如果是replication方式的表则存放全部数据。我们来看一下Postgres-XL和postgresql, Postgres-XC的对比 : PostgreSQL Postgres-XC Postgres-XL Open source SQL database for Enterprises Open source SQL database for Enterprises Open source SQL database for Enterprises - Large coverage of PostgreSQL support Large coverage of PostgreSQL support - Global MVCC Consistency Global MVCC Consistency - - MPP query support - - Performance improvements - - Multi-tenant security关于Postgres-XL，参见另一篇博文Postgres-XL集群安装与配置 2.安装环境准备一台Centos 6.5 GNOME 2.28.2从官网下载源码：http://sourceforge.net/projects/postgres-xc/安装配置如下图： IP 角色 端口 Nodename 安装目录 192.168.199.151 GTM 6666 gtm /pgxc_data/gtm 192.168.199.151 Coordinator 1921 coord1 /pgxc_data/coordinator/cd1 192.168.199.151 Coordinator 1925 coord2 /pgxc_data/coordinator/cd2 192.168.199.151 Datanode 15431 db1 /pgxc_data/datanode/dn1 192.168.199.151 Datanode 15432 db2 /pgxc_data/datanode/dn2有条件的可以把这个角色分别安装在不同主机上。 3.安装配置1）创建用户 #groupadd pgxc #useradd pgxc -g pgxc #passwd pgxc2)安装 #tar -xzvf pgxc-v1.0.4.tar.gz #cd postgres-xc-1.0.4 #./configure –prefix=/usr/local/pgsql_xc #make #make install3）创建存放路径 #mkdir -p /pgxc_data/gtm #mkdir -p /pgxc_data/coordinator/cd1 #mkdir -p /pgxc_data/coordinator/cd2 #mkdir -p /pgxc_data/datanode/dn1 #mkdir -p /pgxc_data/datanode/dn2 #chown -R pgxc:pgxc /pgxc_data4）配置环境变量 #su - pgxc #vi .bash_profileexport PGHOME=/usr/local/pgsql_xcexport LANG=en_US.utf8export LD_LIBRARY_PATH=$PGHOME/lib:/lib64:/usr/lib64:/usr/local/lib64:/lib:/usr/lib:/usr/local/libexport PATH=$PGHOME/bin:$PATH:.export MANPATH=$PGHOME/share/man:$MANPATH #source .bash_profile5）初始化节点 #initgtm -Z gtm -D /pgxc_data/gtm #initdb -D /pgxc_data/coordinator/cd1 –nodename coord1 #initdb -D /pgxc_data/coordinator/cd2 –nodename coord2 #initdb -D /pgxc_data/datanode/dn1 –nodename db1 #initdb -D /pgxc_data/datanode/dn2 –nodename db26)配置节点配置gtm节点vi /pgxc_data/gtm/gtm.confnodename = ‘gtm’listen_addresses = ‘*’port = 6666startup = ACT 配置 coordinator 节点vi /pgxc_data/coordinator/cd1/postgresql.conf #————————————listen_addresses = ‘*’port = 1921max_connections = 100 DATA NODES AND CONNECTION POOLING#———————————————-pooler_port = 6667 #min_pool_size = 1max_pool_size = 100 GTM CONNECTION#—————————–gtm_host = ‘192.168.199.151’gtm_port = 6666pgxc_node_name = ‘coord1’ vi /pgxc_data/coordinator/cd1/pg_hba.conf IPv4 local connections:host all all 127.0.0.1/32 trusthost all all 192.168.199.151/32 trusthost all all 0.0.0.0/0 md5 vi /pgxc_data/coordinator/cd2/postgresql.conf #————————————listen_addresses = ‘*’port = 1925max_connections = 100 DATA NODES AND CONNECTION POOLING#———————————————-pooler_port = 6668 #min_pool_size = 1max_pool_size = 100 GTM CONNECTION#—————————–gtm_host = ‘192.168.199.151’gtm_port = 6666pgxc_node_name = ‘coord2’ vi /pgxc_data/coordinator/cd2/pg_hba.conf IPv4 local connections:host all all 127.0.0.1/32 trusthost all all 192.168.199.151/32 trusthost all all 0.0.0.0/0 md5 配置 datanode 节点vi /pgxc_data/datanode/dn1/postgresql.conf #————————————listen_addresses = ‘*’port = 15431max_connections = 100 GTM CONNECTION#—————————–gtm_host = ‘192.168.199.151’gtm_port = 6666pgxc_node_name = ‘db1’ vi /pgxc_data/datanode/dn1/pg_hba.conf IPv4 local connections:host all all 127.0.0.1/32 trusthost all all 192.168.199.151/32 trusthost all all 0.0.0.0/0 md5 vi /pgxc_data/datanode/dn2/postgresql.conf #————————————listen_addresses = ‘*’port = 15432max_connections = 100 GTM CONNECTION#—————————–gtm_host = ‘192.168.199.151’gtm_port = 6666pgxc_node_name = ‘db2’ vi /pgxc_data/datanode/dn2/pg_hba.conf IPv4 local connections:host all all 127.0.0.1/32 trusthost all all 192.168.199.151/32 trusthost all all 0.0.0.0/0 md57）启动节点启动顺序gtm + (gtmstandby) + (gtmproxy) + datanode + coordinator启动gtm #gtm -D /pgxc_data/gtm &amp; #ps -ef|grep gtm #gtm_ctl status -z gtm -D /pgxc_data/gtm启动数据节点 #postgres -X -D /pgxc_data/datanode/dn1 &amp; #postgres -X -D /pgxc_data/datanode/dn2 &amp; #ps -ef|grep pgxc启动coordinator节点 #postgres -C -D /pgxc_data/coordinator/cd1 &amp; #postgres -C -D /pgxc_data/coordinator/cd2 &amp;8）注册节点在coord1上注册： #psql -p 1921 postgresselect * from pgxc_node;create node coord2 with(TYPE=coordinator,HOST=’192.168.199.151’,PORT=1925);create node db1 with(TYPE=datanode,HOST=’192.168.199.151’,PORT=15431,primary);create node db2 with(TYPE=datanode,HOST=’192.168.199.151’,PORT=15432);alter node coord1 with(TYPE=’coordinator’,HOST=’192.168.199.151’,PORT=1921);select pgxc_pool_reload();在coord2上注册： #psql -p 1925 postgresselect * from pgxc_node;create node coord1 with(TYPE=coordinator,HOST=’192.168.199.151’,PORT=1921);create node db1 with(TYPE=datanode,HOST=’192.168.199.151’,PORT=15431,primary);create node db2 with(TYPE=datanode,HOST=’192.168.199.151’,PORT=15432);alter node coord2 with(TYPE=’coordinator’,HOST=’192.168.199.151’,PORT=1925);select pgxc_pool_reload();9）停止节点停止顺序coordinator+datanode+(gtmproxy)+(gtmstandby)+gtm #pg_ctl stop -D /pgxc_data/datanode/dn1 -Z datanode -m fast #pg_ctl stop -D /pgxc_data/datanode/dn2 -Z datanode -m fast #pg_ctl stop -D /pgxc_data/coordinator/cd1 -Z coordinator -m fast #pg_ctl stop -D /pgxc_data/coordinator/cd2 -Z coordinator -m fast #gtm_ctl stop -D /pgxc_data/gtm -Z gtm -m fast 4.验证测试在coord1上创建一个数据库，并建立一个新表 #psql -p 1921 postgrescreate database pgxc_test;\\c pgxc_test;create table test_xc (id integer,name varchar(32));insert into test_xc select generate_series(1,100),’pgxc_test’;在coord2上查询，看是否能够查询到在coord1上新建的数据库和表在db1上查看在db2上查看这说明我们在coordinator上是以distribute切片方式建的表，数据分别放在datanode上。","tags":[{"name":"postgresql","slug":"postgresql","permalink":"http://blog.yaodataking.com/tags/postgresql/"}]},{"title":"《必然》：关于未来的畅想","date":"2015-12-11T17:56:25.000Z","path":"2015/12/12/the-inevitable/","text":"有幸成为了逻辑思维第一批五万本预定凯文·凯利的新书《必然》的读者。拿到书到现在也差不多四个礼拜了，觉得非常有必要做一下笔记。知道凯文·凯利（KK）当然是从他的《失控》开始的，后来又读了《科技想要什么》，都是脑洞大开的著作。而《必然》这本书虽然没有前两本震撼，但也令人对未来的趋势一起畅想，一起思索。在这本书中，凯文·凯利用十二个现在分词向我们描述了未来三十年的趋势：这就是形成（Becoming）、知化（Cognifying）、流动（Flowing）、屏读（Screening）、使用（Accessing）、共享（Sharing）、过滤（Filtering）、重混（Remixing）、互动（Interacting）、追踪（Tracking）、提问（Questioning），以及开始（Beginning）。纵观这十二个趋势，其实可以大致分为两类，第一类是技术工具的革新，有知化、流动、追踪、重混、过滤。第二类是生活方式上的变化，有屏读、使用、共享、互动、提问。而这些变革在未来的三十年中正在形成，并且现在刚刚开始。 好吧，让我们一起畅想，一起思索。关于知化，凯文·凯利描写道：●人工智能思想的到来加速了其他所有颠覆性趋势的进程，它在未来世界中的威力与曾经的“铀元素”相当。我们可以肯定地说，知化是必然的，因为它已经近在咫尺。●即将到来的人工智能更像亚马逊的网络服务——廉价、可靠、工业级的数字智能在一切事物背后运行，除了闪现在你眼前的短暂时刻，它近乎无影无形。这种常见的设施会根据你的需求提供你想要的智能水平。●近期的三大突破将开启人们期待已久的人工智能时代：廉价的并行计算；大数据；更好的算法。●我们想要的不是智能，而是人工智慧。与一般的智能不同，智慧是专注的、能衡量的、专门化的。它还能够用完全不同于人类认知的方式思考。●在一个联系超密集的世界中，不一样的思维是创新和财富的来源。仅仅聪明是不够的。商业动机会让与工业力量相关的人工智能无处不在，它们廉价而聪明，会被植入到所有我们制造的东西里。当我们开始发明新的智能种类和新的思维方式时，将获得更大的回报。●我根据人类和机器人的关系把工作分为四大类，希望能帮助我们更好地了解机器人将怎样取代人类：1.人类能从事但机器人表现更佳的工作；2.人类不能从事但机器人能从事的工作；3.人类想要从事却还不知道是什么的工作；4.（刚开始）只有人类能从事的工作。 关于流动，凯文·凯利从纸质书籍的四个方面（书页、版本、介质、完成度)的一成不变特性到电子书的这四个方面的流动不息特性比较，指出：●流动性带来了新的力量：重要的不是复制品的数量，而是可以通过其他媒体链接、处理、注释、标记、突出、翻译、强化一份复制品的方式的数量。同时，流动性进一步释放了创造力。●流动的四个阶段分别是：1.固定、罕见；2.免费、无所不在；3.流动、分享；4.开放、变化。●从停滞到流动，从一成不变到奔流不息，期间的变化并不意味着要放弃稳定性。这是一场对广阔边界的开垦，许多建立在可变性基础上的额外选项都会成为可能。●最重要的不同在于，一成不变不再是唯一的选项了。好东西不一定是静态不变的。换句话说，正确的不稳定现在成了好事。 关于过滤的必然性，凯文·凯利分析道：●在人们有限的一生中，没有人有足够的时间把每个选择的潜在影响都逐个审视一遍。我们需要一种对信息进行分类的方法，而唯一的选择就是寻求帮助来指导我们如何选择。●我们通过“守门人”来过滤信息；我们通过媒介来过滤信息；我们通过管理者过滤信息；我们通过品牌过滤；我们通过政府过滤；我们通过我们的文化环境过滤；我们通过我们的朋友过滤；我们通过自身来过滤。●巧妙的算法会对每个人的大量行为记录进行汇总分析，以期能够及时地预测我的行为。通过对10亿条过往购买记录的相关性计算，他们的预测会相当有先见之明。●未来的30年中，我们在教育、运输、医疗和零售领域都能看到大规模定制的出现。●就如何过滤和该过滤什么而言，我们还处在初级阶段。未来的发展空间要比单纯地“我们过滤和我们被过滤”要广阔得多。这些强大的计算技术可以并且也将运用到万物联网的各个领域。●在信息丰富的世界里，唯一稀缺的资源就是人类的注意力。●平台利用强大的计算能力将不断扩张的广告商们与不断增长的消费者群体进行匹配。它们的人工智能系统会寻求在最理想的位置、最理想的时间插放最理想的广告，并且以最理想的方式、最理想的频率做出反馈。 关于重混，凯文·凯利认为：●我们正处在一个盛产重混产品的时期。创新者将早期简单的媒介形式与后期复杂的媒介形式重新组合，产生出无数种新的媒介形式。新的媒介形式越多，我们就越能将它们重混成更多可能的更新型媒介形式。●新的媒介形式自身将会被重混、分解，并在未来的几十年里重组成数百种其他的新形式。●在未来，沉浸式环境和虚拟现实也必然会具备返回先前状态的功能。实际上，任何数字产品都将具备撤销和回放功能，就像它们都会具备重混的功能。●就可复制性而言，媒介的完美重复功用已经得到充分开发。但就可回放性而言，媒介的完美重复功用还未得到充分利用。●全球经济都在远离物质世界，向非实体的比特世界靠拢。同时，它也在远离所有权，向使用权靠拢；也在远离复制价值，向网络价值靠拢；同时奔向一个必定会到来的世界，那里持续不断发生着日益增多的重混。●但凡有价值的创作物，最终将不可避免地以某种形式转化成其他东西。人们当然永远可以获得1997年J·K·罗琳发表的那个版本的《哈利·波特》，但不可避免的是，在未来几十年里会出现1000本根据她的原版图书创作出来的同人小说。发明或作品本身越有魅力，也就越可能被其他人所转化，而且这一过程也越重要。●在未来的30年里，最重要的文化作品和最有影响力的媒介将是重混现象发生最频繁的地方。 关于追踪导致个人隐私的泄露，凯文·凯利认为隐私只能通过信任获得，而信任需要稳固的身份作基础。●在过去几年里，廉价的微型数字传感器能轻易记录各类不同的参数，以至于几乎人人都能测量上千种和自身有关的数据。这些涉及自身的实验已经开始改变我们对医疗、健康和人类行为的看法。●更微缩的芯片、更强劲的电池以及云端连接激励了一些自我追踪者尝试时间跨度很长的追踪，尤其在健康方面。●不久的将来，一个极其个人化的身体记录数据库（包括完整的基因序列）可以用来打造个人治疗方案和个性化医疗。●自我追踪的范畴远远大于健康。它涵盖了我们的整个生活。微型可穿戴的数字“眼睛”和“耳朵”能够记录我们一天中每分每秒的所见所闻，从而帮助我们记忆。●物联网的设计是用来追踪数据，这也是它所处的云端的本质属性。在未来5年中，我们预计云端中加入的340亿联网设备将会用来传输数据。云端的作用则是保存数据。任何接触云端的东西都能被追踪，也一定会被追踪。●所有先前无法测量的东西都被量化、数字化，并且可以被追踪。我们会持续追踪自己，我们和朋友之间也会互相追踪。企业和政府会对我们实行更多追踪。50年后，无处不在的追踪行为将成为常规。●无处不在的监督是必然的。因为我们无法让这个机制停止追踪，我们只能让人们之间的关系更对称。●互相监督的社会中会出现一种权利意识，即每个人都有权获取关于自己的数据，并从中受益。但是每种权利都伴随着义务，因此每个人都有义务尊重信息的完整，负责任地分享信息并接受他人监督。 关于屏读，凯文·凯利说道，其实今天，我们中的大部分人都变成了屏幕之民。屏幕之民倾向于忽略书籍中的经典逻辑，和对书本的崇敬。他们更喜欢像素间的动态流动。●文字已经从纸浆里转移到了电脑、手机、游戏机、电视、电子显示屏和平板电脑的像素当中。字母不再白纸黑字地固定在纸上，而是在玻璃平面上以彩虹样的色彩，于眨眼间飞速来去。屏幕占据了我们的口袋、行李箱、仪表盘、客厅墙壁和建筑物的四壁。●屏幕文化是一个不断变动的世界，充满其中的是无穷无尽的新闻素材、剪辑资料和未成熟的理念。这是一条由微博、摘要、随手拍照片、简短文字和漂浮的第一印象构成的河流。●全世界的年轻人每天能用手机写下5亿条段子，无论他们正在求学，还是已经工作。屏幕数量的增长在继续扩展人们的阅读量和写作量。●屏读首先会改变书籍，然后会改变图书馆；之后，它会给电影和视频动手术；再之后，它会瓦解掉游戏和教育；而最终，屏读将会改变每件事。●我们已经有了平板电脑、电子书和手机。其中手机最让人吃惊。长久以来，专家一致保持着这样一种观点：没人想在巴掌大的小屏幕上阅读。但他们错了，而且错的十万八千里。我和很多人都喜欢用这种方式来读书。实际上，我们还不知道可以读书的屏幕到底可以做到多小。●数字图书的直接效果，是可以在任何时间呈现在任何屏幕上。书将会变得呼之即来。在你需要读书之前，就购买和囤积书籍的行为会消失。书不再像是一种人工制品，而更像是映入你视野的信息流。●可以升级与搜索的屏幕和比特比起纸张来具有太多优势。被人批注、标注、标记、收藏、总结、参考、链接、分享、传播，才是这种书籍长久以来真正想要的。数字化能让这些书籍实现“夙愿”，而且做到的还能比“夙愿”更多。 关于使用，凯文·凯利相信使用权将逐步取代所有权。对于日常生活中的大部分事物，对事物的使用将会胜过对其拥有。●在我们向使用权靠拢并远离所有权的长期进程中，有五个深层的科技发展趋势起着推动促进的作用：1.减物质化；2.按需使用的即时性；3.去中心化；4.平台协同；5.云端。●从中心化的组织向更为扁平化的互联网世界转变的后果是，每一个事物——无论是有形的还是无形的，都必须更快地流动起来，以保证整体在一起移动。●在未来的几十年里，云端崛起的下一步就是将不同的云端结合成一个“互联云”（intercloud）。如同互联网是网络的网络，互联云则是云端的云端。●在未来的30年里，减物质化、去中心化、即时性、平台协同和云端的发展将继续强势发展。只要科技进步使得通信成本、计算成本继续下降，这些趋势都是必然。●数字原住民是自由地奔向前方的，他们不会承受拥有事物所带来的负累，可以自由地探索未知的世界。使用而非拥有事物，使我可以保持敏捷和精力充沛，时刻为即将出现的未知事物做好准备。 关于共享，凯文·凯利用维基百科证明了当个体们为实现一个更大目标而共同工作时，群体层面的结果就会涌现出来，这也就是所谓的“合作”。社区的集体影响力已经远远超出了贡献者个人力量之和。这就是社会化机构的全部要义所在——整体优于部分之和。●随着人们协同程度的增加，群体从只需最低程度协同的共享起步，而后进步到合作，再然后是协作，最终则达到集体主义。每一步发展都需要进一步的协同。●分享是数字社会主义中最温和的表现形式，但这样一个动词却是所有高级水平的群体活动的基础。它也是整个网络世界的基本构成成分。●我们从协作中受益越多，就会愈发欢迎政府中的社会化机制。未来的管理模式将会混合了从维基百科和瑞典民主社会主义模式中获取的经验。●任何可以被共享的事物——思想、情绪、金钱、健康、时间，都将在适当的条件和适当的回报下被共享。任何可以被共享的都能以上百万种我们今天尚未实现的方式被更好、更快、更便利、更长久地共享。 关于互动，凯文·凯利预测：●现场感和互动效果是推动当前虚拟现实技术快速发展的两大亮点。“现场感”是虚拟现实技术的主要卖点。●用不了十年，当你体验尖端水平的虚拟现实显示器时，你的眼睛会被蒙蔽，以为自己正在通过一个真实的窗户观看一个真实的世界。那个场景会是明亮的，没有闪光，没有肉眼可见的像素点。你将会十分确定地感觉到那就是绝对的真实世界，只不过它并不是真的。●虚拟现实技术的亮点不仅仅是“存在感”，另一个让它经久不衰的魅力源于它的互动效果。当我们佩戴虚拟现实设备时是否舒适或是会出现不适，这一点还未明确。●现场感会将用户带入虚拟世界，但是虚拟现实设备的互动效果才是维持用户体验的要素。各个层面上的互动效果将会扩散影响到这个技术构造的虚拟世界的其他方面。●在未来的几十年里，我们将继续拓展更多可以与之互动的事物。拓展将遵循三个方向推进：①我们会继续给自己制造的事物添加新的传感器和感官功能；②互动发生的区域将会继续向我们靠近；③最大程度的互动会要求我们跳入到技术本身。●在人类短短几十年的寿命期限中就能“扰乱”社会发展的第一个技术平台是个人电脑。移动电话是第二个平台，它们都是在短短的几十年里引发了社会中一切事物的变革。下一代颠覆性的平台就是虚拟现实，而它已经到来了。●互动的程度在提升，并且将继续提升。想要互动，就需要掌握技能、学会配合、多加体验，并加强学习。我们需要把自己嵌入到技术中，并培养自身能力。 关于提问，凯文·凯利深信维基百科的影响力还没有被完全发掘，它改变人们想法的力量正在潜意识中影响着全世界的千禧年一代，为他们提供了一个有益的蜂巢型心智的实例，并且使人们相信，“不可能的事”也能够做到。●科学作为一种手段，主要增长了我们的无知而不是我们的知识。●提问比回答更有力量。 关于形成，凯文·凯利坦言，过去30年已经开创出了不可思议的起跑线——可以建造真正伟大事物的坚固平台。但即将到来的将会不同，将会超越现在，将会成为他物。而最酷的东西尚未发明出来。●我们会更经常地相信那些不太可能的事情。所有事物都在流动，而新的形式将会是旧事物的融合，这种融合与旧有的那些远远不同。通过努力和想象，我们可以学习更加清晰地辨识前方，不再盲目。●畅想未来30年，网络会变得怎样激动人心时，我们不免首先想到网页2.0——就是更好的网页。但是2050年的网络不会是更好的网页，它会变成别的东西，和今天网络的差距就像是最初的网络和电视的差距一样。●今天确实是一片广袤的处女地，我们都正在“形成”。这在人类的历史上，是绝无仅有的最佳开始时机。●你没迟到。 关于开始，凯文·凯利相信，未来30年，霍洛斯将沿着与过去30年同样的方向挺进，那就是：更多的流动、共享、追踪、使用、互动、屏读、重混、过滤、知化、提问以及形成。我们正站在开始的时刻。●我们的新型超级网络是一股持久变化的浪潮，不断推动着我们的各种新需求和新欲望。我们完全无法预测30年后身边都有哪些产品、品牌和公司。这些完全取决于个人的机遇和命运。●千年之后，当历史学家回溯过往时，会认为第三个千禧年的开端是一个古老的绝妙时代。●已经开始。当然，也仅仅是个开始。 变化是必然的。我们现在承认，一切都是可变的，一切都在变化当中——尽管很多变化并不为人所察觉。我们说水滴石穿，而这颗星球上所有的动植物也在以一种超级慢动作演变成为不同的物种。 怎么样，有所触动吗？这是必然的。在未来，我们所有人都会一次又一次地成为全力避免掉队的菜鸟，永无休止，无一例外。原因在于：首先，未来30年中，大部分可以主导生活的重要科技还没有被发明出来，因此面对这些科技，你自然会成为一个菜鸟；其次，因为新科技需要无穷无尽的升级，你会一直保持菜鸟的状态；第三，因为淘汰的循环正在加速，在新科技被淘汰前，你不会有足够的时间来掌握任何事情，所以你会一直保持菜鸟的身份。永远是菜鸟是所有人的新设定，这与你的年龄，与你的经验，都没有关系。","tags":[{"name":"必然","slug":"必然","permalink":"http://blog.yaodataking.com/tags/必然/"}]},{"title":"使用Keepalived实现将lvs进行高可用","date":"2015-12-06T12:31:46.000Z","path":"2015/12/06/keepalived-lvs/","text":"LVS是Linux Virtual Server的简写，意即Linux虚拟服务器，是一个虚拟的服务器集群系统。目前有三种IP负载均衡技术（VS/NAT、VS/TUN和VS/DR）；十种调度算法（rrr|wrr|lc|wlc|lblc|lblcr|dh|sh|sed|nq）。Keepalived在这里主要用作RealServer的健康状态检查以及LoadBalance主机和BackUP主机之间failover的实现。本文我们将实现下图的实验。环境centos 6.6 服务器功能 IP VIP Lvs MASTER 192.168.199.33 192.168.199.88 Lvs BACKUP 192.168.199.34 192.168.199.88 Web serverA 192.168.199.31 192.168.199.88 Web serverB 192.168.199.32 192.168.199.88准备工作：1)每台服务器关闭SELinux和防火墙 #setenforce 0 #service iptables stop2)每台服务器同步时间tzselectcp /usr/share/zoneinfo/Asia/Shanghai /etc/localtimentpdate -u ntp.api.bz1.配置两台WEB服务器1.1开启httpd服务 #yum install httpd1.2设置访问主页 #cd /var/www/html//192.168.199.31 #vi index.htmlThis is web server 192.168.199.31//192.168.199.32This is web server 192.168.199.32 1.3编写WEB服务器启动脚本 #vi realserver.sh #!/bin/bashVIP=192.168.199.88/etc/rc.d/init.d/functionscase “$1” instart)echo “ start LVS of REALServer”/sbin/ifconfig lo:0 $VIP broadcast $VIP netmask 255.255.255.255 up/sbin/route add -host $VIP dev lo:0echo “1” &gt;/proc/sys/net/ipv4/conf/lo/arp_ignoreecho “2” &gt;/proc/sys/net/ipv4/conf/lo/arp_announceecho “1” &gt;/proc/sys/net/ipv4/conf/all/arp_ignoreecho “2” &gt;/proc/sys/net/ipv4/conf/all/arp_announcesysctl -p &gt;/dev/null 2&gt;&amp;1;;stop)/sbin/ifconfig lo:0 downecho “close LVS Directorserver”echo “0” &gt;/proc/sys/net/ipv4/conf/lo/arp_ignoreecho “0” &gt;/proc/sys/net/ipv4/conf/lo/arp_announceecho “0” &gt;/proc/sys/net/ipv4/conf/all/arp_ignoreecho “0” &gt;/proc/sys/net/ipv4/conf/all/arp_announce;;*)echo “Usage: $0 {start|stop}”exit 1esac //设置可执行 #chmod +x realserver.sh//设置可读写 #chmod 755 /etc/rc.d/init.d/functions 在每台WEB服务器上执行 #./realserver.sh start1.4测试WEB服务器独立可访问此时还不能访问192.168.199.882 安装配置Lvs Master 服务器 #yum install keepalived #cd /etc/keepalived/ #cp keepalived.conf keepalived.conf.bak #vim keepalived.conf ! Configuration File for keepalivedglobal_defs {notification_email {acassen@firewall.locfailover@firewall.locsysadmin@firewall.loc}notification_email_from Alexandre.Cassen@firewall.locsmtp_server 192.168.200.1smtp_connect_timeout 30router_id LVS_DEVEL} vrrp_instance VI_1 {state MASTERinterface eth0virtual_router_id 51priority 100advert_int 1authentication {auth_type PASSauth_pass 1111}track_interface {eth0}virtual_ipaddress {192.168.199.88/24 dev eth0 label eth0:0} } virtual_server 192.168.199.88 80 {delay_loop 6lb_algo wrrlb_kind DRnat_mask 255.255.255.0persistence_timeout 30protocol TCP real_server 192.168.199.31 80 {weight 3TCP_CHECK {connect_timeout 10nb_get_retry 3delay_before_retry 3connect_port 80}}real_server 192.168.199.32 80 {weight 3TCP_CHECK {connect_timeout 10nb_get_retry 3delay_before_retry 3connect_port 80}}} #service keepalived start3. 安装配置Lvs BACKUP服务器 #yum install keepalived #cd /etc/keepalived/ #cp keepalived.conf keepalived.conf.bak #vim keepalived.conf ! Configuration File for keepalivedglobal_defs {notification_email {acassen@firewall.locfailover@firewall.locsysadmin@firewall.loc}notification_email_from Alexandre.Cassen@firewall.locsmtp_server 192.168.200.1smtp_connect_timeout 30router_id LVS_DEVEL} vrrp_instance VI_1 {state BACKUPinterface eth0virtual_router_id 51priority 99advert_int 1authentication {auth_type PASSauth_pass 1111}track_interface {eth0}virtual_ipaddress {192.168.199.88/24 dev eth0 label eth0:0} } virtual_server 192.168.199.88 80 {delay_loop 6lb_algo wrrlb_kind DRnat_mask 255.255.255.0persistence_timeout 30protocol TCP real_server 192.168.199.31 80 {weight 3TCP_CHECK {connect_timeout 10nb_get_retry 3delay_before_retry 3connect_port 80}}real_server 192.168.199.32 80 {weight 3TCP_CHECK {connect_timeout 10nb_get_retry 3delay_before_retry 3connect_port 80}}} #service keepalived start访问192.168.199.88, 访问正常，一段时间会自动切换web服务器4.实验1:将WEB服务器31停止，看访问是否还是正常。 #service httpd stop我们看到还能正常访问，只是切换到WEB 服务器B5.实验2:将负载均衡服务器master停止，看访问是否还是正常。 #service keepalived stop我们可以从master服务器的LOG可以看到,VIP192.168.199.88已释放。我们可以从BACKUP服务器的LOG可以看到，BACKUP服务器已接管VIP192.168.199.88访问192.168.199.88还是正常至此使用Keepalived实现将lvs进行高可用成功部署。","tags":[{"name":"keepalived","slug":"keepalived","permalink":"http://blog.yaodataking.com/tags/keepalived/"},{"name":"lvs","slug":"lvs","permalink":"http://blog.yaodataking.com/tags/lvs/"}]},{"title":"Storm on Yarn平台搭建","date":"2015-11-29T06:12:24.000Z","path":"2015/11/29/storm-on-yarn/","text":"1.简介1)Storm：一个实时计算框架，与Hadoop离线计算框架互补，分别用于解决不同场景下的问题。Storm的官方网站是 http://storm.apache.org，关于Storm，可以参见淘宝搜索的文章：[Storm简介](http://www.searchtb.com/2012/09/introduction-to-storm.html)。2)YARN：YARN是Hadoop 2.0中新引入的资源管理系统，所有应用程序和框架，比如MapReduce、Storm和Spark等，均可运行在YARN之上。3)Storm On YARN：尝试将Storm运行在YARN上，这将带来众多好处。Storm On YARN最有名是Yahoo！的开源实现。 2.环境准备 IP NAME Function 192.168.199.21 hadoop21 namenode, zookeeper,ResourceManager 192.168.199.22 hadoop22 datanode,zookeepaer,NodeManager 192.168.199.23 hadoop23 datanode, zookeepaer，Nodemanagerhadoop2.7.1安装参见hadoop-2-7-1-完全分布式安装部署,zookeeper安装参见hadoop-2-7-1-实现ha集群部署中Zookeeper的安装。 3.Storm on Yarn安装配置3.1下载Storm on Yarn我们用grid安装 #su grid #mkdir storm #cd storm从GitHub上下载Storm on Yarn #wget https://github.com/yahoo/storm-yarn/archive/master.zipunzip master.zip修改hadoop版本vim storm-yarn-master/pom.xml，修改Hadoop的版本号为2.7.1 3.2编译Storm on Yarn下载maven并解压 #wget http://mirrors.aliyun.com/apache/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz #cd storm/storm-yarn-master使用maven编译 #/home/grid/apache-maven-3.3.9/bin/mvn package -DskipTests解压storm.zip至/home/grid/storm下 #unzip /home/grid/storm/storm-yarn-master/lib/storm.zip /home/grid/storm 3.3配置Storm on Yarn#vim ./bashrc #source ./bashrc将编译好后的storm-yarn-master/lib/storm.zip 添加进hdfshadoop fs -mkdir -p /lib/storm/0.9.0-wip21hadoop fs -moveFromLocal storm.zip /lib/storm/0.9.0-wip21 4.Storm on Yarn启动测试4.1Storm on Yarn启动启动hadoop #start-dfs.sh #start-yarn.sh启动zookeeper #~/zookeeper-3.4.6/bin/zkServer.sh start启动storm on yarn #storm-yarn launch $STORM_HOME/storm-0.9.0-wip21/conf/storm.yaml访问http://192.168.199.21:8088/cluster，我们看到storm on yarn已启动。注：因为storm是作为一个yarn程序运行在集群上的，所以会有一个AppId，如下图所示通过以下命令我们可以获取Nimbus host #storm-yarn getStormConfig -appId application_1448767639808_0001 -output ~/storm/storm.yaml #cat ~/storm/storm.yaml | grep nimbus.host访问Nimbus host http://192.168.199.22:7070/可以看到Storm UI 4.1Storm on Yarn测试1)提交Topologystorm jar /home/grid/storm/storm-yarn-master/lib/storm-starter-0.0.1-SNAPSHOT.jar storm.starter.WordCountTopology wordCountTopology -c nimbus.host=192.168.199.222)监控Topology3)关闭Topologystorm kill wordCountTopology4)关闭Storm on yarnstorm-yarn shutdown –appId [applicationId]","tags":[{"name":"Storm","slug":"Storm","permalink":"http://blog.yaodataking.com/tags/Storm/"},{"name":"Yarn","slug":"Yarn","permalink":"http://blog.yaodataking.com/tags/Yarn/"}]},{"title":"Centos 6.5 Postgresql搭建eclipse开发环境","date":"2015-11-28T04:42:12.000Z","path":"2015/11/28/centos-6-5-postgresql-eclipse/","text":"环境Centos 6.5 GNOME 2.28.2 1.安装git#mkdir /home/src #cd src #yum install git 2.eclipse搭建#yum install eclipse #yum install eclipse-cdt #yum install readline-devel 3.安装编译postgresql#su - postgres//使用git下载postgresql源代码 #git clone git://git.postgresql.org/git/postgresql.git//切换到一个发布版本 #cd postgresql #git checkout REL9_3_4 #cd ..//重新copy一个目录，确保服务器上拉下来的代码不受影响 #cp -rf postgresql/ pgsql #yum install bison #yum install flex #cd pgsql #./configure –enable-depend –enable-cassert –enable-debug系统默认安装至/usr/local/pgsql,我们可以使用下面命令安装至其他目录 #./configure –prefix=/usr/local/pgsqldev –enable-depend –enable-cassert –enable-debug #make #make all 4.使用eclipse调试4.1 import源码包//我们postgres用户名打开eclipse,选择workspace选择”C/C++ » Existing Code as Makefile Project”点击Next选择Linux GCC这样我们就建立了一个postgres源码的项目。 4.2调试Initdb//建立data目录 #mkdir /home/data #chown postgres:postgres -R /home/data选择initdb右键，debug as，在C/C++ Application下选择新建，Arguments输入-D /home/data点击Apply按钮，然后点击Debug按钮。在这里我们可以设置断点，启动调试。 4.3调试postgres首选启动postgres #/usr/local/pgsql/bin/pg_ctl -D /home/data -l /home/data/logfile start #/usr/local/pgsql/bin/psql在eclipse选择postgre debug as，在Attach下新建debug我们看到进程5787，5788就是刚才我们运行的psql，双击5788进程，设置断点。命令行输入SQL语句select current_time;断点处我们看到刚才输入的SQL语句 4.4 postgres测试框架测试框架包含在postgres源码目录中src/test主要用例都放在regress目录下,有data、expected、input、output、sql等目录和parallel_schedule、serial_schedule、standby_schedule等设置测试用例并行、串行执行的文件，如下图：postgres的regress test的流程为：逐个执行sql/目录下的sql脚本，将执行的结果重定向到results/目录下。而expected/目录下则是预期的执行结果。将results目录下的文件逐个与expected中文件进行diff。若文件不一致，则判定为对应的sql脚本执行结果为失败，反之为成功。参考：Working_with_Eclipse","tags":[{"name":"postgresql","slug":"postgresql","permalink":"http://blog.yaodataking.com/tags/postgresql/"}]},{"title":"Thrift 入门:安装及示列","date":"2015-11-22T14:00:42.000Z","path":"2015/11/22/thrift/","text":"1.简介Thrift是一个软件框架，用来进行可扩展且跨语言的服务的开发。它结合了功能强大的软件堆栈和代码生成引擎，以构建在 C++, Java, Python, PHP, Ruby, Erlang, Perl, Haskell, C#, Cocoa, JavaScript, Node.js, Smalltalk, and OCaml 这些编程语言间无缝结合的、高效的服务。 2.环境环境：CENTOS6.6, Hadoop 1.2.1, zookeeper 3.4.6, jdk1.8.0_51，Hbase 0.98.15 IPHOST NAMEFunction192.168.199.11Hadoop11Namenode,Zookeeper,Hbase192.168.199.12Hadoop12datanode,Zookeeper,Hbase192.168.199.13Hadoop13datanode,Zookeeper,Hbase 3.安装1)安装开发工具 #yum -y groupinstall “Development Tools”2)更新autoconf #wget http://ftp.gnu.org/gnu/autoconf/autoconf-2.69.tar.gz #tar xvf autoconf-2.69.tar.gz #cd autoconf-2.69 #./configure –prefix=/usr #make #make install #cd ..3)更新automake #wget http://ftp.gnu.org/gnu/automake/automake-1.14.tar.gz #tar xvf automake-1.14.tar.gz #cd automake-1.14 #./configure –prefix=/usr #make #make install #cd ..4)更新bison #wget http://ftp.gnu.org/gnu/bison/bison-2.5.1.tar.gz #tar xvf bison-2.5.1.tar.gz #cd bison-2.5.1 #./configure –prefix=/usr #make #make install #cd ..5)安装C++ lib #yum -y install libevent-devel zlib-devel openssl-devel6)安装boost 1.53.0 #wget http://sourceforge.net/projects/boost/files/boost/1.53.0/boost_1_53_0.tar.gz #tar xvf boost_1_53_0.tar.gz #cd boost_1_53_0 #./bootstrap.sh #./b2 install7)安装Thrift #git clone https://git-wip-us.apache.org/repos/asf/thrift.git #cd thrift #git checkout -b thrift-0.9.1 0.9.1 #./bootstrap.sh #./configure #make #make install 4.示列4.1 Hbase的Thrift接口1)启动Hadoop、Zookeeper、HBase(关于HBase的分布式安装参见另一篇博客Hbase完全分布式安装)//在Hadoop11启动 #start-all.sh//在Hadoop11,12,13分别启动 #~/zookeeper-3.4.6/bin/zkServer.sh start//在Hadoop11启动 #start-hbase.sh2)启动Hbase thrift接口 #hbase thrift -p 9090 start3)生成Hbase.py下载源码 #wget http://mirrors.aliyun.com/apache/hbase/0.98.16/hbase-0.98.16-src.tar.gz #thrift -gen py /home/grid/hbase-0.98.16/hbase-thrift/src/main/resources/org/apache/hadoop/hbase/thrift/Hbase.thrift #cp -r /home/grid/gen-py/hbase/ /usr/lib64/python2.6/site-packages/4)Python代码测试 #! /usr/bin/python #coding=utf-8 import sys sys.path.append(&apos;/usr/lib64/python2.6/site-packages/&apos;) from thrift import Thrift from thrift.transport import TSocket from thrift.transport import TTransport from thrift.protocol import TBinaryProtocol from hbase import Hbase from hbase.ttypes import * transport = TSocket.TSocket(&apos;192.168.199.11&apos;, 9090) transport = TTransport.TBufferedTransport(transport) protocol = TBinaryProtocol.TBinaryProtocol(transport) client = Hbase.Client(protocol) transport.open() print(client.getTableNames()) 我们看到Python代码正确获取了Hbase的表名","tags":[{"name":"Thrift","slug":"Thrift","permalink":"http://blog.yaodataking.com/tags/Thrift/"}]},{"title":"COGNOS BI 10 集群部署示列","date":"2015-11-15T06:09:40.000Z","path":"2015/11/15/cognos-bi-10/","text":"1.Cognos简介Cognos是在BI核心平台之上，以服务为导向进行架构的一种数据模型，是唯一可以通过单一产品和在单一可靠架构上提供完整业务智能功能的解决方案。它可以提供无缝密合的报表、分析、记分卡、仪表盘等解决方案，通过提供所有的系统和资料资源，以简化公司各员工处理资讯的方法。作为一个全面、灵活的产品，Cognos业务智能解决方案可以容易地整合到现有的多系统和数据源架构中。 2.本文目标2.1 将cognos部署到多台电脑上（这里用三台虚拟机）2.2 设置两个以上的cognos使用者权限，一个为管理员（全部权限），另一个为报表开发者（只能进行报表开发）2.3 使用自己的数据生成数据包，并且制作3个以上的报表。 3.准备准备3台机器（或虚拟机）环境：Windows 2008 R2 64Bit；COGNOS 10.2;DB2 10.1 64Bit;Apache HTTP SERVER 2.2;OpenLDAP 2.2.29;安装配置如下图： Name IP Memory/CPU Main function Other Cognos11 192.168.199.7 2G/1CPU Content Manager Server DB2 Server，OpenLDAP Cognos12 192.168.199.8 4G/2CPU BI Server DB2 Client，Framework Manager Cognos13 192.168.199.9 2G/1CPU Gateway Server Apache Server安装顺序DB2 SERVER-&gt;Content Manager-&gt;BI server -&gt; Gateway server-&gt;Apache Server-&gt;OpenLDAP-&gt;DB2 Client -&gt; Framework Manager.注：Framework Manager可安装在任一电脑，前提是安装DB2 client，因为这里DB2 Client本来需要安装在BI SERVER.所以我这里偷懒把Framework Manager也安装在BI server上了。 4.安装部署4.1安装DB2在COGOS11,192.168.199.7电脑上，启动db2 setup程序。 4.2安装Content Manager1)在COGOS11,192.168.199.7电脑上启动cognos setup程序,选择Content Manger,选择安装路径C:\\C10\\CM2)复制C:\\Program Files\\IBM\\SQLLIB\\java下两个文件db2jcc.jar db2jcc_license_cu.jar 到 content manager安装目录的\\webapps\\p2pd\\web-inf\\lib3)启动配置，修改以下localhost为本机IP 192.168.199.7，端口号不变。4)配置content Store输入以上信息，右键点击生成DDL文件在DB2命令行运行DB2 -tvf C:/C10/CM/configuration/schemas/content/db2/createDb.sql5)保存，启动服务,保持启动状态。在WEB浏览器输入http://192.168.199.7:9300/p2pd/servlet 显示成功运行。 4.3安装BI SERVER1)在COGOS12,192.168.199.8电脑上启动cognos setup程序,选择应用程序组件, 选择安装路径C:\\C10\\BI2)同样复制C:\\Program Files\\IBM\\SQLLIB\\java下两个文件db2jcc.jar db2jcc_license_cu.jar 到 content manager安装目录的\\webapps\\p2pd\\web-inf\\lib3)启动配置,Content Manager URI改为Content Manger 服务器IP 192.168.199.7,分派器URI改为本机IP地址192.168.199.8通知存储库设置与content store 一样，注意使用IP地址。4)保存启动（注意保持content manager已启动)在WEB浏览器输入http://192.168.199.8:9300/p2pd/servlet/dispatch 显示以下信息，表示成功。 4.4安装Gateway1)在COGNOS13,192.168.199.9电脑上启动cognos setup程序,选择Gateway, 选择安装路径C:\\C10\\GW2)启动配置,把分派器URI 改成IP192.168.199.8,网关URI 改成本机IP：192.168.199.93)安装Apache http server,在安装路径的httpd.conf文件最后添加如下代码： ScriptAlias /ibmcognos/cgi-bin/ &quot;C:/C10/GW/cgi-bin/&quot; &amp;lt;Directory &quot;C:/C10/GW/cgi-bin/&quot;&amp;gt; AllowOverride None AcceptPathInfo On Options None Order allow,deny Allow from all Alias /ibmcognos/ &quot;C:/C10/GW/webcontent/&quot; &amp;lt;Directory &quot;C:/C10/GW/webcontent/&quot;&amp;gt; Options Indexes MultiViews AllowOverride None AcceptPathInfo On Order allow,deny Allow from all 在WEB浏览器输入http://192.168.199.9/ibmcognos/,出现以下界面表示成功。 4.5安装openLDAP1)在COGOS11,192.168.199.7电脑上，启动openLDAP 安装程序.安装配置过程略，参见文档（为OpenLDAP 配置LDAP命名空间 v2）2)配置结束，重启BI SERVER在WEB浏览器输入http://192.168.199.9/ibmcognos/,右上角出现需要登录。表示成功。 4.6安装DB2 CLIENT在COGNOS12,192.168.199.8电脑上，安装 db2 client.过程略。 4.7安装Framework Manager1)在COGNOS12,192.168.199.8电脑上，安装 Framework Manager,路径 C:\\C10\\FM2)把分派器URI 改成IP192.168.199.8,网关URI 改成IP：192.168.199.9至此，COGNOS三台机器安装部署基本结束。 5.权限配置1)设置openLDAP 的admin为系统管理员,用admin登录，在管理-&gt;安全-&gt;用户,组,角色找到角色系统管理员.添加admin禁止匿名角色2)添加角色报表开发者,加入openLDAP用户cog _user1.在功能区域禁止角色报表开发者其他权限。用cog_user1 登录看到只有报表开发权限。3)修改Content Manger安全认证，把匿名登录项改为false.访问http://192/168.199.9/ibmcognos/将显示需要验证登录 6.导入数据1)因为手上也没有自己的数据，所以还是安装SAMPLE里的GS_DB数据库至DB2 SERVER，并起名为GOSALES,这个过程略。2)配置DB2 CLIENT使用 catalog 将DB2 SERVER上的数据库映射到cognos BI服务器.db2 catalog tcpip node db2db remote 192.168.199.7 server 50000db2 catalog db GOSales at node db2dbdb2 terminate3)制作数据包,并发布。a.启动Framework Manager,新建项目fmPackage3b.设置数据源.c.选择元数据d.制作数据包，为简单起见，只选择销售目标为唯一度量维度，相应选择产品品牌，产品类型为常规维度，具体过程略。业务视图维度视图5)发布数据包新建数据包，首先验证，如果没有问题，就可以发布。在公共文件夹下看到了刚发布的包. 7.制作报表1)制作Query Studio报表用cog_user2登录，选择新建Query Studio，我们将制作一个按品牌显示的销售目标报表。按销售目标降序排序。2)制作Analysis studio报表，如下图:3)制作Report studio报表，带图表显示。","tags":[{"name":"Cognos","slug":"Cognos","permalink":"http://blog.yaodataking.com/tags/Cognos/"}]},{"title":"Flume1.6.0入门:安装、配置及示列","date":"2015-11-14T08:18:06.000Z","path":"2015/11/14/flume1-6-0-e5-85-a5-e9-97-a8-e5-ae-89-e8-a3-85-e3-80-81-e9-85-8d-e7-bd-ae-e5-8f-8a-e7-a4-ba-e5-88-97/","text":"1.简介flume是一个分布式、可靠、和高可用的海量日志采集、聚合和传输的系统。支持在日志系统中定制各类数据发送方，用于收集数据;同时，Flume提供对数据进行简单处理，并写到各种数据接受方(比如文本、HDFS、Hbase等)的能力 。flume的数据流由事件(Event)贯穿始终。事件是flume的基本数据单位，它携带日志数据(字节数组形式)并且携带有头信息，这些Event由Agent外部的Source生成，当Source捕获事件后会进行特定的格式化，然后Source会把事件推入(单个或多个)Channel中。你可以把Channel看作是一个缓冲区，它将保存事件直到Sink处理完该事件。Sink负责持久化日志或者把事件推向另一个Source。flume的一些核心概念：Agent 使用JVM 运行flume。每台机器运行一个agent，但是可以在一个agent中包含多个sources和sinks。Client 生产数据，运行在一个独立的线程。Source 从Client收集数据，传递给Channel。Sink 从Channel收集数据，运行在一个独立线程。Channel 连接 sources 和 sinks ，这个有点像一个队列。Events 可以是日志记录、 avro 对象等。基本的数据流模型 2.环境Hadoop 1.2.1 ,配置见下表，Hadoop 1.2.1 的安装参见 hadoop-1-2-1-完全分布式安装部署 ，这里略。 IPNAME192.168.199.11Hadoop11Namenode,flume192.168.199.12Hadoop12Datanode192.168.199.13Hadoop13Datanode 3.安装配置从镜像服务器下载并解压 #wget http://mirrors.aliyun.com/apache/flume/1.6.0/apache-flume-1.6.0-bin.tar.gz #tar -xzvf apache-flume-1.6.0-bin.tar.gz #vim /etc/profileexport FLUME_HOME=/home/grid/apache-flume-1.6.0-binexport FLUME_CONF_DIR=$FLUME_HOME/confexport PATH=$FLUME_HOME/bin:$PATH #source /etc/profile #cd conf #cp flume-env-template.sh flume-env.sh #vim flume-env.sh 4.测试#flume-ng version出现上面的信息表示安装成功了。 5.示列5.1 一个简单sample#cp conf/flume-conf.properties.template conf/flume-sample #vim conf/flume-samplea1.sources = r1a1.sinks = k1a1.channels = c1 Describe/configure the sourcea1.sources.r1.type = netcata1.sources.r1.bind = localhosta1.sources.r1.port = 44444 Describe the sinka1.sinks.k1.type = logger Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100 Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1//运行，并显示log在终端 #bin/flume-ng agent –conf conf –conf-file conf/flume-sample –name a1 -Dflume.root.logger=INFO,console 再打开一个终端,输入telnet命令 #telnet localhost 44444然后输入hello word回到原来的终端，我们看到hello world被flume的agent收集了。 5.2收集日志到HDFS示列#cp conf/flume-conf.properties.template conf/flume-exec #vim conf/flume-execa1.sources = r1a1.sinks = k1a1.channels = c1a1.sources.r1.type = execa1.sources.r1.channels = c1a1.sources.r1.command = tail -F /home/grid/hadoop-1.2.1/logs/hadoop-grid-namenode-hadoop11.loga1.sinks.k1.type = hdfsa1.sinks.k1.channel = c1a1.sinks.k1.hdfs.path = hdfs://hadoop11:9000/outputsa1.sinks.k1.hdfs.filePrefix = events-a1.sinks.k1.hdfs.round = truea1.sinks.k1.hdfs.roundValue = 10a1.sinks.k1.hdfs.roundUnit = minutea1.sinks.k1.hdfs.rollSize = 4000000a1.sinks.k1.hdfs.rollCount = 0a1.sinks.k1.hdfs.writeFormat = Texta1.sinks.k1.hdfs.fileType = DataStreama1.sinks.k1.hdfs.batchSize = 10a1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100//运行，并显示log在终端 #bin/flume-ng agent –conf conf –conf-file conf/flume-exec –name a1 -Dflume.root.logger=INFO,console我们看到flume不断收集hadoop-grid-namenode-hadoop11.log的数据写入HDFS中。以上只是简单演示了flume的功能，进一步研究可以查看官方的用户指南","tags":[{"name":"flume","slug":"flume","permalink":"http://blog.yaodataking.com/tags/flume/"}]},{"title":"使用Sqoop实现Mysql与HDFS及HIVE互转","date":"2015-11-14T05:22:15.000Z","path":"2015/11/14/sqoop-mysql-hdfs-hive/","text":"1.简介Sqoop(发音：skup)是一款开源的工具，主要用于在Hadoop(Hive)与传统的数据库(mysql、postgresql…)间进行数据的传递，可以将一个关系型数据库（例如 ： MySQL ,Oracle ,Postgres等）中的数据导进到Hadoop的HDFS中，也可以将HDFS的数据导进到关系型数据库中。Sqoop官方网站：http://sqoop.apache.org/ 2.环境Hadoop 1.2.1 ,Hive 1.2.1,Mysql 5.1.73,配置见下表，Hadoop/Hive/Mysql的安装见其他文章，这里略。 IPNAME192.168.199.11Hadoop11Namenode,Hive,Sqoop192.168.199.12Hadoop12Datanode192.168.199.13Hadoop13Datanode,Mysql server 3.安装及启动3.1下载在Hadoop11上从镜像地址下载Sqoophttp://mirrors.aliyun.com/apache/sqoop/1.4.6/ #wget http://mirrors.aliyun.com/apache/sqoop/1.4.6/sqoop-1.4.6.bin__hadoop-1.0.0.tar.gz #tar -xzvf sqoop-1.4.6.bin__hadoop-1.0.0.tar.gz #mv sqoop-1.4.6.bin__hadoop-1.0.0 sqoop-1.4.6 在Hadoop11上下载mysql java connector #wget http://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.37.tar.gz #tar –xzvf mysql-connector-java-5.1.12.tar.gz//复制mysql connector至sqoop lib目录下 #cp mysql-connector-java-5.1.12-bin.jar /home/grid/sqoop-1.4.6/lib 3.2配置#vim /etc/profileexport SQOOP_HOME=/home/hadoop/sqoop-1.4.6export PATH=$SQOOP_HOME/bin:$PATH #source /etc/profile #cd conf #cp sqoop-env-template.sh sqoop-env.sh #vim sqoop-env.sh 3.3启动//在hadoop11上启动hadoop #start-all.sh //在hadoop13上启动mysql #service mysqld start #mysql//创建演示数据库mysql&gt;create database sqoop;mysql&gt;use sqoop;//创建表并导入sample数据mysql&gt;create table tab1 as select table_schema,table_name,table_type from information_schema.TABLES;mysql&gt;show tables;//创建空白表用于从hdfs导入数据mysql&gt;create table tab2 as select from tab1 limit 0;mysql&gt;select from tab2;//同样创建空白表用于从hive导入数据mysql&gt;create table tab3 as select from tab1 limit 0;mysql&gt;select from tab3; 4.演示数据互转4.1 mysql与HDFS互转#sqoop list-databases –connect jdbc:mysql://hadoop13:3306/ –username hive –password hive@Hadoop #sqoop list-tables –connect jdbc:mysql://hadoop13:3306/sqoop –username hive –password hive@Hadoop //导入tab1数据至HDFS #sqoop import –connect jdbc:mysql://hadoop13:3306/sqoop –username hive –password hive@Hadoop –table tab1 -m 1 #hadoop fs -ls /user/grid/tab1 #hadoop fs -cat /user/grid/tab1/part-m-00000 //导出HDFS数据至mysql tab2 #sqoop export –connect jdbc:mysql://hadoop13:3306/sqoop –table tab2 –username hive –password hive@Hadoop –export-dir hdfs://hadoop11:9000/user/grid/tab1/part-m-00000 -m 1//我们看到mysql tab2表已经有数据了 4.2 mysql与hive互转启动hive #hive –service metastore &amp; #hive –service hiveserver2 &amp; //导入mysql tab2数据至hive #sqoop import –connect jdbc:mysql://hadoop13:3306/sqoop –username hive –password hive@Hadoop –table tab2 –hive-table tab2 –hive-import -m 1//进入hive我们看到hive里多了tab2表//进入warehouse我们看到多了一个目录 //导出hive数据至mysql tab3 #sqoop export –connect jdbc:mysql://hadoop13:3306/sqoop –table tab3 –username hive –password hive@Hadoop –export-dir /user/hive/warehouse/tab2/part-m-00000 –input-fields-terminated-by ‘\\001’//到mysql可以看到tab3表有数据了.至此演示数据结束。","tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://blog.yaodataking.com/tags/hadoop/"},{"name":"Mysql","slug":"Mysql","permalink":"http://blog.yaodataking.com/tags/Mysql/"},{"name":"Hive","slug":"Hive","permalink":"http://blog.yaodataking.com/tags/Hive/"},{"name":"sqoop","slug":"sqoop","permalink":"http://blog.yaodataking.com/tags/sqoop/"}]},{"title":"BigInSights安装KERBEROS","date":"2015-11-10T14:04:09.000Z","path":"2015/11/10/biginsights-kerberos/","text":"本文安装KERBEROS主要基于IBM BigInSights 环境，虚拟机环境可从以下网址下载。 http://www.ibm.com/analytics/us/en/technology/hadoop/ 1.安装krb5 #yum -y install krb5-server krb5-libs krb5-auth-dialog krb5-workstation 2.配置 #vi /var/lib/ambari-server/resources/scripts/krb5.conf 3.创建数据库 #kdb5_util create –s 4.启动 #/etc/rc.d/init.d/krb5kdc start #/etc/rc.d/init.d/kadmin start 5.创建admin用户的principal #kadmin.local -q “addprinc admin/admin “ 6.确认步骤5成功 #cat /var/kerberos/krb5kdc/kadm5.acl 7.重启 #/etc/rc.d/init.d/krb5kdc restart #/etc/rc.d/init.d/kadmin restart 8.ambari管理界面配置","tags":[{"name":"BigInSights","slug":"BigInSights","permalink":"http://blog.yaodataking.com/tags/BigInSights/"},{"name":"KERBEROS","slug":"KERBEROS","permalink":"http://blog.yaodataking.com/tags/KERBEROS/"}]},{"title":"Hbase完全分布式安装","date":"2015-11-06T03:17:16.000Z","path":"2015/11/06/hbase/","text":"环境：CENTOS6.6, Hadoop 1.2.1, zookeeper 3.4.6, jdk1.8.0_51 IP HOST NAME 192.168.199.11 Hadoop11 Namenode 192.168.199.12 Hadoop12 datanode 192.168.199.13 Hadoop13 datanode&nbsp; 1.安装配置Habse 1.1从镜像站点下载HBase Hadoop 1.2.1目前对应的Hbase版本是0.98.15 1.2 修改/etc/profile export HBASE_HOME=/home/grid/hbase-0.98.15-hadoop1 export PATH=$PATH:$HBASE_HOME/bin #source /etc/profile 复制/etc/profile到hadoop12和hadoop13，并在hadoop12、hadoop13上执行 #source /etc/profile &nbsp; 1.3修改文件$HBASE_HOME/conf/hbase-env.sh export JAVA_HOME=/usr/java/jdk1.8.0_51 Zookeepr我们用自己安装的，所以改为false export HBASE_MANAGES_ZK=false &nbsp; 1.4修改文件$HBASE_HOME/conf/hbase-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://hadoop11:9000/hbase&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;hadoop11,hadoop12,hadoop13&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 1.5 修改regionservers 1.6复制hbase 至其他两节点 #scp -r hbase-0.98.15-hadoop1 grid@hadoop12:/home/grid #scp -r hbase-0.98.15-hadoop1 grid@hadoop13:/home/grid 1.7启动集群 首先在hadoop11上启动hadoop #start-all.sh 然后启动zookeeper，分别在三台机器上执行。 #~/zookeeper-3.4.6/bin/zkServer.sh start 最后在hadoop11上启动hbase集群 #start-hbase.sh 至此Hbase集群安装成功。 2.验证测试hbase 2.1 jps 运行jps命令，我们看到hbase服务起来了。 2.2 Hbase shell操作 2.3 web监控","tags":[{"name":"Hbase","slug":"Hbase","permalink":"http://blog.yaodataking.com/tags/Hbase/"}]},{"title":"Hadoop 2.7.1 实现HA集群部署","date":"2015-10-27T03:14:52.000Z","path":"2015/10/27/hadoop-2-7-1-ha/","text":"1.0 简述Hadoop 2.0中的HDFS增加了两个重大特性，HA和Federation，HA即为High Availability，用于解决NameNode单点故障问题，该特性通过热备的方式为主NameNode提供一个备用者，一旦主NameNode出现故障，可以迅速切换至备NameNode，从而实现不间断对外提供服务。Federation即为“联邦”，该特性允许一个HDFS集群中存在多个NameNode同时对外提供服务，这些NameNode分管一部分目录（水平切分），彼此之间相互隔离，但共享底层的DataNode存储资源。本文就讲如何安装Hadoop 2.7.1 实现HA集群。联邦的安装部署我们将在下次讲。 1.1 主机列表IPNAMEFunction192.168.199.24hadoop24.hadoop.com主namenode, zookeeper,journalnode,zkfc, ResourceManager192.168.199.25hadoop25.hadoop.com备namenode,zookeepaer,journalnode,zkfc192.168.199.26hadoop26.hadoop.comdatanode, ,zookeepaer,journalnode, NodeManager192.168.199.27hadoop27.hadoop.comDatanode, NodeManager192.168.199.28hadoop28.hadoop.comDatanode, NodeManager 1.2 下载Zookeeper并安装配置#tar -xzvf zookeeper-3.4.6.tar.gz #cd zookeeper-3.4.6/conf #cp zoo_sample.cfg zoo.cfg #vi zoo.cfg dataDir=/home/grid/zookeeper-3.4.6/data server.1=hadoop24.hadoop.com:2888:3888 server.2=hadoop25.hadoop.com:2888:3888 server.3=hadoop26.hadoop.com:2888:3888 #cd .. #mkdir data #cd ..`&lt;/pre&gt; 生成分发脚本，并执行 &lt;pre&gt;`#cat zookeeper-3.4.6/conf/zoo.cfg | grep server |sed &apos;s/[^*]*=//g&apos; |sed &apos;s/:[^*]*//g&apos; |awk &apos;{print &quot;scp -rp ./zookeeper-3.4.6 grid@&quot;$1&quot;:/home/grid&quot;}&apos; | sed &apos;1d&apos; &amp;gt; zoocopy #./zoocopy `&lt;/pre&gt; [![20151027001](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/10/20151027001.png)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/10/20151027001.png) 在hadoop24上 &lt;pre&gt;`#echo &quot;1&quot; &amp;gt; ~/zookeeper-3.4.6/data/myid`&lt;/pre&gt; 在hadoop25 &lt;pre&gt;`#echo &quot;2&quot; &amp;gt; ~/zookeeper-3.4.6/data/myid`&lt;/pre&gt; 在hadoop26 &lt;pre&gt;`#echo &quot;3&quot; &amp;gt; ~/zookeeper-3.4.6/data/myid`&lt;/pre&gt; 在每台上执行启动 &lt;pre&gt;`#~/zookeeper-3.4.6/bin/zkServer.sh start`&lt;/pre&gt; [![20151027002](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/10/20151027002.png)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/10/20151027002.png) 查看状态 &lt;pre&gt;`#~/zookeeper-3.4.6/bin/zkServer.sh status`&lt;/pre&gt; [![20151027003](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/10/20151027003.png)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/10/20151027003.png) 可以看到一台leader，两台follower ## 1.3 安装配置hadoop 2.7.1 &lt;pre&gt;`#tar -xzvf hadoop-2.7.1.tar.gz #cd hadoop-2.7.1 #mkdir name data tmp journal #cd etc/Hadoop `&lt;/pre&gt; 设置JAVA_HOME路径 &lt;pre&gt;`#vi hadoop-env.sh export JAVA_HOME=/usr/java/jdk1.8.0_51 #vi yarn-env.sh export JAVA_HOME=/usr/java/jdk1.8.0_51`&lt;/pre&gt; 编辑slaves文件 &lt;pre&gt;`#vi slaves hadoop26.hadoop.com hadoop27.hadoop.com hadoop28.hadoop.com `&lt;/pre&gt; &lt;pre&gt;`#vi core-site.xml &amp;lt;configuration&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;fs.defaultFS&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;hdfs://mycluster&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;hadoop.tmp.dir&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;file:/home/grid/hadoop-2.7.1/tmp&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;io.file.buffer.size&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;131702&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;hadoop.proxyuser.hadoop.hosts&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;*&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;hadoop.proxyuser.hadoop.groups&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;*&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;ha.zookeeper.quorum&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;192.168.199.24:2181,192.168.199.25:2181,192.168.199.26:2181&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;ha.zookeeper.session-timeout.ms&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;1000&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;/configuration&amp;gt; #vi mapred-site.xml &amp;lt;configuration&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;mapreduce.framework.name&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;yarn&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;/configuration&amp;gt; #vi yarn-site.xml &amp;lt;configuration&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;yarn.nodemanager.aux-services&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;mapreduce_shuffle&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;yarn.nodemanager.auxservices.mapreduce.shuffle.class&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;org.apache.hadoop.mapred.ShuffleHandler&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;yarn.resourcemanager.address&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;hadoop24.hadoop.com:8032&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;yarn.resourcemanager.scheduler.address&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;hadoop24.hadoop.com:8030&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;yarn.resourcemanager.resource-tracker.address&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;hadoop24.hadoop.com:8031&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;yarn.resourcemanager.admin.address&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;hadoop24.hadoop.com:8033&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;yarn.resourcemanager.webapp.address&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;hadoop24.hadoop.com:8088&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;/configuration&amp;gt; #vi hdfs-site.xml &amp;lt;configuration&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;dfs.namenode.name.dir&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;file:/home/grid/hadoop-2.7.1/name&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;dfs.datanode.data.dir&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;file:/home/grid/hadoop-2.7.1/data&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;dfs.replication&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;3&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;dfs.webhdfs.enabled&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;true&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;dfs.permissions&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;false&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;dfs.permissions.enabled&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;false&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;dfs.nameservices&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;mycluster&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;dfs.ha.namenodes.mycluster&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;nn1,nn2&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;dfs.namenode.rpc-address.mycluster.nn1&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;hadoop24.hadoop.com:9000&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;dfs.namenode.rpc-address.mycluster.nn2&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;hadoop25.hadoop.com:9000&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;dfs.namenode.servicerpc-address.mycluster.nn1&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;hadoop24.hadoop.com:53310&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;dfs.namenode.servicerpc-address.mycluster.nn2&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;hadoop25.hadoop.com:53310&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;dfs.namenode.http-address.mycluster.nn1&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;hadoop24.hadoop.com:50070&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;dfs.namenode.http-address.mycluster.nn2&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;hadoop25.hadoop.com:50070&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;dfs.namenode.shared.edits.dir&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;qjournal://hadoop24.hadoop.com:8485;hadoop25.hadoop.com:8485;hadoop26.hadoop.com:8485/mycluster&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;dfs.client.failover.proxy.provider.mycluster&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;dfs.ha.fencing.methods&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;sshfence&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;dfs.ha.fencing.ssh.private-key-files&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;/home/grid/.ssh/id_rsa&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;dfs.ha.fencing.ssh.connect-timeout&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;30000&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;dfs.journalnode.edits.dir&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;/home/grid/hadoop-2.7.1/journal&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;dfs.ha.automatic-failover.enabled&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;true&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;ha.failover-controller.cli-check.rpc-timeout.ms&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;60000&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;ipc.client.connect.timeout&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;60000&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;dfs.image.transfer.bandwidthPerSec&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;4194304&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;/configuration&amp;gt; `&lt;/pre&gt; 生成分发脚本 &lt;pre&gt;`#cat zoocopy|sed &apos;s/zookeeper-3.4.6/hadoop-2.7.1/g&apos; &amp;gt; tmphadoop #cat ./hadoop-2.7.1/etc/hadoop/slaves |awk &apos;{print &quot;scp -rp ./hadoop-2.7.1 grid@&quot;$1&quot;:/home/grid&quot;}&apos; &amp;gt;&amp;gt; tmphadoop #sort -n tmphadoop | uniq &amp;gt; hadoopcopy #rm -r tmphadoop #cat hadoopcopy #./hadoopcopy `&lt;/pre&gt; [![20151027004](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/10/20151027004.png)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/10/20151027004.png) 创建znode &lt;pre&gt;`#~/zookeeper-3.4.6/bin/zkServer.sh start #~/hadoop-2.7.1/bin/hdfs zkfc -formatZK `&lt;/pre&gt; 验证 &lt;pre&gt;`#~/zookeeper-3.4.6/bin/zkCli.sh`&lt;/pre&gt; [![20151027005](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/10/20151027005.png)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/10/20151027005.png) 在hadoop24,hadoop25,hadoop26启动journalnode &lt;pre&gt;`#~/hadoop-2.7.1/sbin/hadoop-daemon.sh start journalnode`&lt;/pre&gt; [![20151027006](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/10/20151027006.png)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/10/20151027006.png) 在主namenode节点(Hadoop24.hadoop.com)上格式化Namenode &lt;pre&gt;`#~/hadoop-2.7.1/bin/hadoop namenode –format`&lt;/pre&gt; [![20151027007](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/10/20151027007.png)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/10/20151027007.png) 在主namenode节点(Hadoop24.hadoop.com)启动namenode进程 &lt;pre&gt;`#~/hadoop-2.7.1/sbin/hadoop-daemon.sh start namenode`&lt;/pre&gt; [![20151027008](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/10/20151027008.png)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/10/20151027008.png) 在备namenode节点(Hadoop25.hadoop.com)同步元数据 &lt;pre&gt;`#~/hadoop-2.7.1/bin/hdfs namenode -bootstrapStandby`&lt;/pre&gt; 启动备NameNode节点(Hadoop25.hadoop.com) &lt;pre&gt;`#~/hadoop-2.7.1/sbin/hadoop-daemon.sh start namenode`&lt;/pre&gt; 在两个namenode节点都执行以下命令来配置自动故障转移： 在NameNode节点上安装和运行ZKFC &lt;pre&gt;`#~/hadoop-2.7.1/sbin/hadoop-daemon.sh start zkfc`&lt;/pre&gt; [![20151027008](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/10/20151027008.png)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/10/20151027008.png) [![20151027009](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/10/20151027009.png)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/10/20151027009.png) 启动datanode(hadoop26,hadoop27,hadoop28) &lt;pre&gt;`#~/hadoop-2.7.1/sbin/hadoop-daemons.sh start datanode`&lt;/pre&gt; [![20151027010](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/10/20151027010.png)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/10/20151027010.png) 启动yarn &lt;pre&gt;`#~/hadoop-2.7.1/sbin/start-yarn.sh`&lt;/pre&gt; [![20151027011](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/10/20151027011.png)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/10/20151027011.png) 主namenode节点上active [![20151027012](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/10/20151027012.png)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/10/20151027012.png) 备namenode节点上standby [![20151027013](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/10/20151027013.png)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/10/20151027013.png) ## 1.4 测试主备namenode节点是否工作 在主namenode机器上通过jps命令查找到namenode的进程号，然后通过kill -9的方式杀掉进程，观察另一个namenode节点是否会从状态standby变成active状态 [![20151027015](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/10/20151027015.png)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/10/20151027015.png) Kill进程后,主节点不能访问 [![20151027016](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/10/20151027016.png)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/10/20151027016.png) 备节点已变为active [![20151027017](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/10/20151027017.png)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/10/20151027017.png) 测试备节点能否继续工作 在备节点上测试 &lt;pre&gt;`#~/hadoop-2.7.1/bin/hadoop fs -ls / #~/hadoop-2.7.1/bin/hadoop fs -put /home/grid/test.txt / #~/hadoop-2.7.1/bin/hadoop fs -cat /test.txt #~/hadoop-2.7.1/bin/hadoop jar ~/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar wordcount /test.txt /out #~/hadoop-2.7.1/bin/hadoop fs -ls /out #~/hadoop-2.7.1/bin/hadoop fs -cat /out/part-r-00000 至此在Hadoop2.7.1集群上成功实现HA","tags":[{"name":"HA","slug":"HA","permalink":"http://blog.yaodataking.com/tags/HA/"},{"name":"hadoop","slug":"hadoop","permalink":"http://blog.yaodataking.com/tags/hadoop/"},{"name":"zookeeper","slug":"zookeeper","permalink":"http://blog.yaodataking.com/tags/zookeeper/"}]},{"title":"Hadoop 2.7.1 完全分布式安装部署","date":"2015-10-27T00:01:51.000Z","path":"2015/10/27/hadoop2-7-1/","text":"1.安装前，准备三台CENTOS 6.6系统的主机或虚机,并且关闭防火墙及selinux.2.按如下表格配置IP地址，修改hosts文件及本机名 IP 主机名 节点 192.168.199.21 hadoop21 Master 102.168.199.22 hadoop22 Slave1 192.168.199.23 hadoop23 Slave2 同理修改Slave1，Slave2的IP地址，hosts文件及本机名。 3.安装ORACLE JDK先卸载本机openJDK,使用rpm -qa|grep java查看，然后用rpm -e 卸载 从oracle网站找到最新JDK,我这选择了JDK8http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html下载好以后解压，并移至/usr/java,如果没有可以mkdir 建立。 #tar -xzvf jdk-8u51-linux-x64.gz #mv jdk1.8.0_51 /usr/java #vi /etc/profile export JAVA_HOME=/usr/java/jdk1.8.0_51 export CLASSPATH=.:$JAVA_HOME/lib/rt.jar:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar export PATH=$PATH:$JAVA_HOME/bin #source /etc/profile `&lt;/pre&gt; ## 4.在各节点分别建立Hadoop运行帐号grid,并设置密码 ## 5.配置SSH免密码登陆。 在各节点分别以grid用户名生成两个密钥文件，一个是私钥id_rsa,另一个是公钥id_rsa.pub &lt;pre&gt;`#ssh-keygen -t rsa -f ~/.ssh/id_rsa `&lt;/pre&gt; 然后在hadoop21上 &lt;pre&gt;`#cp /home/grid/.ssh/id_rsa.pub /home/grid/.ssh/authorized_keys #scp hadoop22:/home/grid/.ssh/id_rsa.pub pubkeys22 #scp hadoop23:/home/grid/.ssh/id_rsa.pub pubkeys23 #cat pubkeys22 &amp;gt;&amp;gt; /home/grid/.ssh/authorized_keys #cat pubkeys23 &amp;gt;&amp;gt; /home/grid/.ssh/authorized_keys #rm pubkeys22 #rm pubkeys23 `&lt;/pre&gt; 最后分发authorized_keys 到各节点 &lt;pre&gt;`scp /home/grid/.ssh/authorized_keys hadoop22:/home/grid/.ssh scp /home/grid/.ssh/authorized_keys hadoop23:/home/grid/.ssh `&lt;/pre&gt; ## 6.在Master机下载并解压Hadoop2.7.1（使用grid用户名) 找到最近的hadoop镜像，使用wget下载2.7.1 &lt;pre&gt;`#wget http://mirror.bit.edu.cn/apache/hadoop/common/hadoop-2.7.1/hadoop-2.7.1.tar.gz `&lt;/pre&gt; 解压hadoop-2.7.1.tar.gz &lt;pre&gt;`#tar -xzvf hadoop-2.7.1.tar.gz #cd hadoop-2.7.1 `&lt;/pre&gt; 建立tmp,dfs,dfs/data,dfs/name ## 7.修改配置文件 修改hadoop-env.sh &lt;pre&gt;`export JAVA_HOME=/usr/java/jdk1.8.0_51 `&lt;/pre&gt; [![002nJwOegy6UlVcJVkK62&amp;amp;690](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/10/002nJwOegy6UlVcJVkK62690.png)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/10/002nJwOegy6UlVcJVkK62690.png) [![002nJwOegy6UlVhJ5xJ07&amp;amp;690](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/10/002nJwOegy6UlVhJ5xJ07690.png)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/10/002nJwOegy6UlVhJ5xJ07690.png) [![002nJwOegy6UlVcQwkOca&amp;amp;690](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/10/002nJwOegy6UlVcQwkOca690.png)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/10/002nJwOegy6UlVcQwkOca690.png) [![002nJwOegy6UlVcV90844&amp;amp;690](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/10/002nJwOegy6UlVcV90844690.png)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/10/002nJwOegy6UlVcV90844690.png) [![002nJwOegy6UlVcXuiM0c&amp;amp;690](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/10/002nJwOegy6UlVcXuiM0c690.png)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/10/002nJwOegy6UlVcXuiM0c690.png) ## 8\\. 分发至各Salve节点 &lt;pre&gt;`#scp -r /home/grid/hadoop-2.7.1 hadoop22:/home/grid #scp -r /home/grid/hadoop-2.7.1 hadoop23:/home/grid `&lt;/pre&gt; ## 9.Master机格式化namenode &lt;pre&gt;`#cd /home/grid/hadoop-2.7.1 #./bin/hdfs namenode -format 10.启动Hadoop#./sbin/start-dfs.sh #./sbin/start-yarn.sh 11.验证是否成功Master机应该启动NameNode,SecondaryNameNode,ResourceManagerSlave机应该启动DataNode,NodeManager","tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://blog.yaodataking.com/tags/hadoop/"}]},{"title":"Hadoop 1.2.1 完全分布式安装部署","date":"2015-10-26T08:50:15.000Z","path":"2015/10/26/hadoop1-2-1/","text":"1.安装前，准备三台CENTOS 6.6系统的主机或虚机,并且关闭防火墙及selinux.2.按如下表格配置IP地址，修改hosts文件及本机名 IP 主机名 节点 192.168.199.11 hadoop11 Master 102.168.199.12 hadoop12 Slave1 192.168.199.13 hadoop13 Slave2修改master节点配置同理修改Slave1，Slave2的IP地址，hosts文件及本机名。 3.安装ORACLE JDK先卸载本机openJDK,使用rpm -qa|grep java查看，然后用rpm -e 卸载从oracle网站找到最新JDK,我这选择了JDK8http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html下载好以后解压，并移至/usr/java,如果没有可以mkdir 建立。 #tar -xzvf jdk-8u51-linux-x64.gz #mv jdk1.8.0_51 /usr/java #vi /etc/profile export JAVA_HOME=/usr/java/jdk1.8.0_51 export CLASSPATH=.:$JAVA_HOME/lib/rt.jar:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar export PATH=$PATH:$JAVA_HOME/bin #source /etc/profile`&lt;/pre&gt; ## 4.在各节点分别建立Hadoop运行帐号grid,并设置密码 ## 5.配置SSH各节点免密码登陆。 在各节点分别以grid用户名生成两个密钥文件，一个是私钥id_rsa,另一个是公钥id_rsa.pub [![002nJwOegy6UlO7nj194b&amp;amp;690](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/10/002nJwOegy6UlO7nj194b690.png)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/10/002nJwOegy6UlO7nj194b690.png) [![002nJwOegy6UlMtCeGJ71&amp;amp;690](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/10/002nJwOegy6UlMtCeGJ71690.png)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/10/002nJwOegy6UlMtCeGJ71690.png) 在master机上把三个节点的公钥都添加至文件authorized_keys，然后分发至各节点。 在hadoop11上 &lt;pre&gt;`#cp /home/grid/.ssh/id_rsa.pub /home/grid/.ssh/authorized_keys #scp hadoop12:/home/grid/.ssh/id_rsa.pub pubkeys12 #scp hadoop13:/home/grid/.ssh/id_rsa.pub pubkeys13 #cat pubkeys12 &amp;gt;&amp;gt; /home/grid/.ssh/authorized_keys #cat pubkeys13 &amp;gt;&amp;gt; /home/grid/.ssh/authorized_keys #rm pubkeys12 #rm pubkeys13 `&lt;/pre&gt; 最后分发authorized_keys 到各节点 &lt;pre&gt;`#scp /home/grid/.ssh/authorized_keys hadoop12:/home/grid/.ssh #scp /home/grid/.ssh/authorized_keys hadoop13:/home/grid/.ssh `&lt;/pre&gt; ## 6.在Master机下载并解压Hadoop1.2.1（使用grid用户名) 找到最近的hadoop镜像，使用wget下载1.2.1 &lt;pre&gt;`#wget http://mirror.bit.edu.cn/apache/hadoop/common/hadoop-1.2.1/hadoop-1.2.1.tar.gz`&lt;/pre&gt; 解压hadoop-1.2.1.tar.gz &lt;pre&gt;`#tar -xzvf hadoop-1.2.1.tar.gz #cd hadoop-1.2.1 #mkdir tmp `&lt;/pre&gt; ## 7.修改配置文件 &lt;pre&gt;`#cd hadoop-1.2.1 #cd conf`&lt;/pre&gt; 修改hadoop-env.sh &lt;pre&gt;`export JAVA_HOME=/usr/java/jdk1.8.0_51`&lt;/pre&gt; 修改core-site.xml [![002nJwOegy6UlSzzXzs1a&amp;amp;690](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/10/002nJwOegy6UlSzzXzs1a690.png)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/10/002nJwOegy6UlSzzXzs1a690.png) 修改hdfs-site.xml [![002nJwOegy6UlSBcLQ7cb&amp;amp;690](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/10/002nJwOegy6UlSBcLQ7cb690.png)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/10/002nJwOegy6UlSBcLQ7cb690.png) 修改mapred-site.xml [![002nJwOegy6UlSDlQIMf0&amp;amp;690](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/10/002nJwOegy6UlSDlQIMf0690.png)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/10/002nJwOegy6UlSDlQIMf0690.png) 修改masters,slaves [![002nJwOegy6UlRpkmdP41&amp;amp;690](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/10/002nJwOegy6UlRpkmdP41690.png)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/10/002nJwOegy6UlRpkmdP41690.png) ## 8\\. 分发至各Salve节点 &lt;pre&gt;`#scp -r /home/grid/hadoop-1.2.1 hadoop12:/home/grid #scp -r /home/grid/hadoop-1.2.1 hadoop13:/home/grid `&lt;/pre&gt; ## 9.Master机格式化namenode &lt;pre&gt;`#bin/hadoop namenode -format`&lt;/pre&gt; [![002nJwOegy6UlRN6Cc101&amp;amp;690](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/10/002nJwOegy6UlRN6Cc101690.png)](http://orufryv17.bkt.clouddn.com/wp-content/uploads/2015/10/002nJwOegy6UlRN6Cc101690.png) ## 10.启动Hadoop &lt;pre&gt;`#bin/start-all.sh 11.验证是否成功Master机应该启动NameNode,SecondaryNameNode,JobTrackerSlave机应该启动DataNode,TaskTracker","tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://blog.yaodataking.com/tags/hadoop/"}]}]